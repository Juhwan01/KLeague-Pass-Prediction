{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d00b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import os\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "BATCH_SIZE = 64\n",
    "LR_BASE = 1e-3\n",
    "LR_FINE = 1e-4\n",
    "EPOCHS_BASE = 15\n",
    "EPOCHS_FINE = 5\n",
    "D_MODEL = 128\n",
    "NHEAD = 4\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "MAX_SEQ_LEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. ê²½ë¡œ ì„¤ì • ë° Train ë¡œë“œ\n",
    "# ======================================================\n",
    "BASE_DIR = \"./open_track1\" # ë°ì´í„°ê°€ ìˆëŠ” í´ë”ëª… (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)\n",
    "if not os.path.exists(BASE_DIR): BASE_DIR = \".\" # í´ë” ì—†ìœ¼ë©´ í˜„ì¬ ê²½ë¡œ\n",
    "\n",
    "TRAIN_PATH = os.path.join(BASE_DIR, \"train.csv\")\n",
    "TEST_META_PATH = os.path.join(BASE_DIR, \"test.csv\") # ì‚¬ìš©ìë‹˜ì´ ì˜¬ë¦¬ì‹  íŒŒì¼ëª…\n",
    "MATCH_PATH = os.path.join(BASE_DIR, \"match_info.csv\")\n",
    "\n",
    "# 1. Train ë¡œë“œ\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "print(f\"âœ… Train Loaded: {train_df.shape}\")\n",
    "\n",
    "# 2. Test í†µí•© ë¡œë“œ (í•µì‹¬ ìˆ˜ì • ì‚¬í•­)\n",
    "if os.path.exists(TEST_META_PATH):\n",
    "    test_meta = pd.read_csv(TEST_META_PATH)\n",
    "    print(f\"â„¹ï¸ Reading {len(test_meta)} test files...\")\n",
    "    \n",
    "    test_dfs = []\n",
    "    # tqdmìœ¼ë¡œ ì§„í–‰ìƒí™© í™•ì¸\n",
    "    for _, row in tqdm(test_meta.iterrows(), total=len(test_meta), desc=\"Loading Test CSVs\"):\n",
    "        rel_path = row['path']\n",
    "        \n",
    "        # ê²½ë¡œ ë³´ì •: 1) ê·¸ëŒ€ë¡œ, 2) í´ë” ë¶™ì—¬ì„œ, 3) íŒŒì¼ëª…ë§Œìœ¼ë¡œ ì°¾ê¸°\n",
    "        paths_to_try = [\n",
    "            rel_path,\n",
    "            os.path.join(BASE_DIR, rel_path.lstrip(\"./\")),\n",
    "            os.path.join(BASE_DIR, \"test\", str(row['game_id']), os.path.basename(rel_path))\n",
    "        ]\n",
    "        \n",
    "        for p in paths_to_try:\n",
    "            if os.path.exists(p):\n",
    "                test_dfs.append(pd.read_csv(p))\n",
    "                break\n",
    "        \n",
    "    if test_dfs:\n",
    "        test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "        print(f\"âœ… Test Data Merged: {test_df.shape}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Test íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"test.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 3. Match Info ë³‘í•©\n",
    "if os.path.exists(MATCH_PATH):\n",
    "    match_info = pd.read_csv(MATCH_PATH)\n",
    "    match_subset = match_info[['game_id', 'home_team_id', 'venue']]\n",
    "    train_df = pd.merge(train_df, match_subset, on='game_id', how='left')\n",
    "    test_df = pd.merge(test_df, match_subset, on='game_id', how='left')\n",
    "\n",
    "# 4. ì „ì²˜ë¦¬ (ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°)\n",
    "def preprocess(df):\n",
    "    if 'home_team_id' in df.columns:\n",
    "        df['is_home'] = (df['team_id'] == df['home_team_id']).astype(float)\n",
    "    else:\n",
    "        df['is_home'] = 0.5\n",
    "        \n",
    "    # V2 ë¡œì§ ì‘ë™ì„ ìœ„í•´ end_xê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€\n",
    "    if 'end_x' not in df.columns:\n",
    "        df['end_x'] = 0.0\n",
    "        df['end_y'] = 0.0\n",
    "    else:\n",
    "        df['end_x'] = df['end_x'].fillna(0.0)\n",
    "        df['end_y'] = df['end_y'].fillna(0.0)\n",
    "    return df\n",
    "\n",
    "train_df = preprocess(train_df)\n",
    "test_df = preprocess(test_df)\n",
    "\n",
    "ID_COL = 'game_episode' if 'game_episode' in train_df.columns else 'episode_id'\n",
    "print(f\"Data Ready. ID Column: {ID_COL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5662aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ë²”ì£¼í˜• ë³€ìˆ˜ ì •ì˜ (Train ë°ì´í„° ë¶„ì„ ê¸°ë°˜)\n",
    "TOP_TYPES = ['Pass', 'Carry', 'Recovery', 'Interception', 'Duel', 'Tackle', \n",
    "             'Throw-In', 'Clearance', 'Intervention', 'Block', 'Pass_Freekick', \n",
    "             'Cross', 'Goal Kick', 'Error', 'Shot']\n",
    "ALL_RESULTS = ['Successful', 'Unsuccessful', 'On Target', 'Yellow_Card', \n",
    "               'Blocked', 'Keeper Rush-Out', 'Low Quality Shot', 'Off Target']\n",
    "\n",
    "def make_features(group):\n",
    "    n = len(group)\n",
    "    \n",
    "    # --- 1. ê¸°ë³¸ ì¢Œí‘œ & Context ---\n",
    "    sx = group['start_x'].values / 105.0\n",
    "    sy = group['start_y'].values / 68.0\n",
    "    ex = group['end_x'].values / 105.0\n",
    "    ey = group['end_y'].values / 68.0\n",
    "    is_home = group['is_home'].values\n",
    "    \n",
    "    # --- 2. ì‹œê°„ & ì†ë„ (New!) ---\n",
    "    # time_secondsê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ 0 ì²˜ë¦¬\n",
    "    if 'time_seconds' in group.columns:\n",
    "        times = group['time_seconds'].values\n",
    "        # ì‹œê°„ ê°„ê²© (Delta Time): í˜„ì¬ - ì§ì „\n",
    "        # 0ë²ˆì§¸ëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬ (í˜¹ì€ 1ì´ˆ ë“± ê¸°ë³¸ê°’)\n",
    "        dt = np.zeros(n, dtype=np.float32)\n",
    "        dt[1:] = times[1:] - times[:-1]\n",
    "        dt = np.maximum(dt, 0.1) # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€ (ìµœì†Œ 0.1ì´ˆ)\n",
    "    else:\n",
    "        dt = np.ones(n, dtype=np.float32) # ì‹œê°„ ì •ë³´ ì—†ìœ¼ë©´ 1ì´ˆë¡œ ê°€ì •\n",
    "\n",
    "    # --- 3. ì´ë™ëŸ‰ & ê±°ë¦¬ ê³„ì‚° ---\n",
    "    dx = ex - sx\n",
    "    dy = ey - sy\n",
    "    # ì •ê·œí™”ëœ ê±°ë¦¬ ë§ê³ , ì‹¤ì œ ë¯¸í„° ê±°ë¦¬ ì‚¬ìš© (ì†ë„ ê³„ì‚°ìš©)\n",
    "    dist_meter = np.sqrt((dx*105)**2 + (dy*68)**2)\n",
    "    \n",
    "    # --- 4. í†µê³„ëŸ‰ (Lagged Features for Leakage Prevention) ---\n",
    "    # (1) ëˆ„ì  ì´ë™ëŸ‰ (Flow)\n",
    "    cumsum_dx = np.cumsum(dx) / 105.0\n",
    "    cumsum_dy = np.cumsum(dy) / 68.0\n",
    "    \n",
    "    # (2) ì§ì „ ì´ë™ ê±°ë¦¬ & ëˆ„ì ê°’ (Lagged)\n",
    "    # lag_val[i] = val[i-1] (ì§ì „ í–‰ë™ì˜ ê²°ê³¼)\n",
    "    lag_dist_m = np.roll(dist_meter, 1); lag_dist_m[0] = 0\n",
    "    lag_cumsum_dx = np.roll(cumsum_dx, 1); lag_cumsum_dx[0] = 0\n",
    "    lag_cumsum_dy = np.roll(cumsum_dy, 1); lag_cumsum_dy[0] = 0\n",
    "    \n",
    "    # (3) ì§ì „ ì†ë„ (Speed) - New!\n",
    "    # ì§ì „ ê±°ë¦¬ / ì§ì „ ì‹œê°„ê°„ê²© (m/s)\n",
    "    lag_dt = np.roll(dt, 1); lag_dt[0] = 1.0 # 0 ë‚˜ëˆ„ê¸° ë°©ì§€\n",
    "    lag_speed = lag_dist_m / np.maximum(lag_dt, 0.1)\n",
    "    \n",
    "    # --- 5. ì„ ìˆ˜ ì •ë³´ (New!) ---\n",
    "    # ê°™ì€ ì„ ìˆ˜ê°€ ì—°ì†ìœ¼ë¡œ ê³µì„ ì¡ì•˜ëŠ”ê°€? (ë“œë¦¬ë¸”/í‚¤í•‘ ì—¬ë¶€)\n",
    "    if 'player_id' in group.columns:\n",
    "        p_ids = group['player_id'].values\n",
    "        # [i]ì™€ [i-1]ì´ ê°™ìœ¼ë©´ 1, ë‹¤ë¥´ë©´ 0\n",
    "        is_same = np.zeros(n, dtype=np.float32)\n",
    "        is_same[1:] = (p_ids[1:] == p_ids[:-1]).astype(np.float32)\n",
    "    else:\n",
    "        is_same = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "    # --- 6. ì§„í–‰ë¥  & ê¸°íƒ€ ---\n",
    "    progress = np.arange(n) / max(n-1, 1)\n",
    "    # í›„ë°˜ì „ ì—¬ë¶€ (period_idê°€ 2 ì´ìƒì´ë©´ 1)\n",
    "    is_second_half = (group['period_id'].values > 1).astype(np.float32) if 'period_id' in group.columns else np.zeros(n)\n",
    "    \n",
    "    # --- 7. ë²”ì£¼í˜• ë³€ìˆ˜ One-hot Encoding (New!) ---\n",
    "    # (1) Type Name (Event ì¢…ë¥˜)\n",
    "    # Test ë§ˆì§€ë§‰ í–‰ì—ë„ 'Pass', 'Carry' ë“±ì€ ì£¼ì–´ì§€ë¯€ë¡œ ì‚¬ìš© ê°€ëŠ¥\n",
    "    # ë¯¸ë¦¬ ì •ì˜ëœ TOP_TYPESì— ëŒ€í•´ 1/0 ë§¤í•‘\n",
    "    types_onehot = np.zeros((n, len(TOP_TYPES) + 1), dtype=np.float32) # +1 for Other\n",
    "    curr_types = group['type_name'].values\n",
    "    for i, t in enumerate(curr_types):\n",
    "        if t in TOP_TYPES:\n",
    "            idx = TOP_TYPES.index(t)\n",
    "            types_onehot[i, idx] = 1.0\n",
    "        else:\n",
    "            types_onehot[i, -1] = 1.0 # Other\n",
    "\n",
    "    # (2) Result Name (ì„±ê³µ ì—¬ë¶€)\n",
    "    # Test ë§ˆì§€ë§‰ í–‰ì´ ë§Œì•½ Pass(Successful)ì´ë¼ë©´ \"ì„±ê³µí•œ íŒ¨ìŠ¤ì˜ ë„ì°©ì \"ì„ ë¬»ëŠ” ê²ƒì´ë¯€ë¡œ ì‚¬ìš© ê°€ëŠ¥.\n",
    "    # ë§Œì•½ ë¹„ì–´ìˆë‹¤ë©´(nan) 0ìœ¼ë¡œ ì²˜ë¦¬ë¨.\n",
    "    results_onehot = np.zeros((n, len(ALL_RESULTS) + 1), dtype=np.float32) # +1 for Nan/Other\n",
    "    curr_results = group['result_name'].values\n",
    "    for i, r in enumerate(curr_results):\n",
    "        if r in ALL_RESULTS:\n",
    "            idx = ALL_RESULTS.index(r)\n",
    "            results_onehot[i, idx] = 1.0\n",
    "        else:\n",
    "            results_onehot[i, -1] = 1.0 # Nan or Other\n",
    "\n",
    "    features = []\n",
    "    \n",
    "    # --- 8. ì‹œí€€ìŠ¤ ê²°í•© ---\n",
    "    for i in range(n):\n",
    "        # Scalar Features (Vector Concatenation)\n",
    "        scalars = [\n",
    "            sx[i], sy[i],             # 1. ìœ„ì¹˜\n",
    "            lag_cumsum_dx[i], lag_cumsum_dy[i], # 2. ëˆ„ì  íë¦„ (Safe)\n",
    "            lag_dist_m[i] / 100.0,    # 3. ì§ì „ ê±°ë¦¬ (ìŠ¤ì¼€ì¼ë§: ëŒ€ëµ 100më¡œ ë‚˜ëˆ”)\n",
    "            lag_speed[i] / 10.0,      # 4. ì§ì „ ì†ë„ (ìŠ¤ì¼€ì¼ë§: ëŒ€ëµ 10m/së¡œ ë‚˜ëˆ”)\n",
    "            dt[i] / 10.0,             # 5. í˜„ì¬ ì‹œê°„ ê²½ê³¼ (ìŠ¤ì¼€ì¼ë§)\n",
    "            progress[i],              # 6. ì§„í–‰ë¥ \n",
    "            is_home[i],               # 7. í™ˆ ì—¬ë¶€\n",
    "            is_same[i],               # 8. ì„ ìˆ˜ ì—°ì†ì„±\n",
    "            is_second_half[i]         # 9. í›„ë°˜ì „ ì—¬ë¶€\n",
    "        ]\n",
    "        \n",
    "        # Combine: Scalar + Type Onehot + Result Onehot\n",
    "        feat_vec = np.concatenate([scalars, types_onehot[i], results_onehot[i]])\n",
    "        \n",
    "        # (1) Start Node\n",
    "        features.append(feat_vec)\n",
    "        \n",
    "        # (2) End Node (ë§ˆì§€ë§‰ íƒ€ê²Ÿ ì œì™¸) -> ì—¬ê¸°ë„ ì°¨ì› ë§ì¶°ì¤˜ì•¼ í•¨\n",
    "        if i < n - 1:\n",
    "            # End NodeëŠ” 'ìœ„ì¹˜'ë§Œ ë°”ë€Œê³  ë‚˜ë¨¸ì§€ëŠ” ê·¸ ì‹œì  ìƒíƒœ ìœ ì§€\n",
    "            # ë‹¨, End ì‹œì ì˜ ëˆ„ì /ê±°ë¦¬ëŠ” 'í˜„ì¬(i)' ê°’ì„ ì¨ì•¼ í•¨ (ì´ë¯¸ ì´ë™í–ˆìœ¼ë¯€ë¡œ)\n",
    "            scalars_end = scalars.copy()\n",
    "            scalars_end[0] = ex[i] # End X\n",
    "            scalars_end[1] = ey[i] # End Y\n",
    "            # End Nodeì—ëŠ” 'ì´ë²ˆ ì´ë™'ì˜ ê²°ê³¼ê°€ ë°˜ì˜ëœ í†µê³„ ì‚¬ìš© ê°€ëŠ¥\n",
    "            scalars_end[2] = cumsum_dx[i]\n",
    "            scalars_end[3] = cumsum_dy[i]\n",
    "            \n",
    "            feat_vec_end = np.concatenate([scalars_end, types_onehot[i], results_onehot[i]])\n",
    "            features.append(feat_vec_end)\n",
    "            \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "# ì…ë ¥ ì°¨ì› ìë™ ê³„ì‚°\n",
    "# Scalar(11) + Type(16) + Result(9) = 36\n",
    "dummy_group = train_df.iloc[:5].copy()\n",
    "dummy_feat = make_features(dummy_group)\n",
    "INPUT_DIM = dummy_feat.shape[1]\n",
    "\n",
    "print(f\"âœ… Advanced Feature Logic Ready.\")\n",
    "print(f\"   Input Dimension: {INPUT_DIM}\")\n",
    "print(f\"   (Included: Speed, Time Delta, Player Continuity, Event Type, Result)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d92c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoccerDataset(Dataset):\n",
    "    def __init__(self, df, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.episodes = []\n",
    "        self.targets = []\n",
    "        self.team_ids = []\n",
    "        self.episode_ids = []\n",
    "        \n",
    "        # ìˆœì„œ ì„ì„ ë°©ì§€\n",
    "        grouped = df.groupby(ID_COL, sort=False)\n",
    "        \n",
    "        for name, group in tqdm(grouped, desc=f\"Dataset ({mode})\"):\n",
    "            if mode == 'train' and len(group) < 2: continue\n",
    "            \n",
    "            # Feature Engineering\n",
    "            if 'time_seconds' in group.columns:\n",
    "                 seq = make_features(group)\n",
    "            else:\n",
    "                 seq = make_v2_features_corrected(group)\n",
    "\n",
    "            team_id = group.iloc[0]['team_id']\n",
    "            \n",
    "            if mode == 'train':\n",
    "                last = group.iloc[-1]\n",
    "                self.targets.append([last['end_x']/105.0, last['end_y']/68.0])\n",
    "                self.episodes.append(seq)\n",
    "                self.team_ids.append(team_id)\n",
    "            else:\n",
    "                self.episodes.append(seq)\n",
    "                self.team_ids.append(team_id)\n",
    "                self.episode_ids.append(str(name))\n",
    "\n",
    "    def __len__(self): return len(self.episodes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = torch.FloatTensor(self.episodes[idx])\n",
    "        if len(seq) > MAX_SEQ_LEN: seq = seq[-MAX_SEQ_LEN:]\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            # Train ë°˜í™˜: (seq, target, team_id)\n",
    "            return seq, torch.FloatTensor(self.targets[idx]), self.team_ids[idx]\n",
    "        else:\n",
    "            # Test ë°˜í™˜: (seq, team_id, episode_id)\n",
    "            return seq, self.team_ids[idx], self.episode_ids[idx]\n",
    "\n",
    "# [í•µì‹¬ ìˆ˜ì •] collate_fn ë¡œì§ ë³€ê²½\n",
    "def collate_fn(batch):\n",
    "    seqs = [b[0] for b in batch]\n",
    "    lens = torch.LongTensor([len(s) for s in seqs])\n",
    "    padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    mask = torch.arange(padded.size(1))[None, :] >= lens[:, None]\n",
    "    \n",
    "    # ë°°ì¹˜ ì²« ë²ˆì§¸ ìš”ì†Œë¡œ ëª¨ë“œë¥¼ íŒë‹¨\n",
    "    elem = batch[0]\n",
    "    \n",
    "    # Test Mode í™•ì¸ (3ë²ˆì§¸ ìš”ì†Œê°€ ë¬¸ìì—´ IDì¸ ê²½ìš°)\n",
    "    if isinstance(elem[2], str):\n",
    "        # Test: b[1]ì´ Team ID\n",
    "        team_ids = torch.LongTensor([b[1] for b in batch])\n",
    "        episode_ids = [b[2] for b in batch]\n",
    "        return padded, mask, team_ids, episode_ids\n",
    "        \n",
    "    else:\n",
    "        # Train Mode: b[1]ì€ Target(ì¢Œí‘œ), b[2]ê°€ Team ID\n",
    "        targets = torch.stack([b[1] for b in batch])\n",
    "        team_ids_train = torch.LongTensor([b[2] for b in batch]) \n",
    "        return padded, targets, mask, team_ids_train\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "full_dataset = SoccerDataset(train_df, mode='train')\n",
    "test_dataset = SoccerDataset(test_df, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model*4,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model // 2, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        output = self.transformer(x, src_key_padding_mask=mask)\n",
    "        \n",
    "        # Valid Token Mean Pooling\n",
    "        valid_mask = (~mask).unsqueeze(-1).float()\n",
    "        mean_out = (output * valid_mask).sum(dim=1) / valid_mask.sum(dim=1).clamp(min=1e-9)\n",
    "        return self.fc(mean_out)\n",
    "\n",
    "model = TransformerPredictor(INPUT_DIM, D_MODEL, NHEAD, NUM_LAYERS, DROPOUT).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63608a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# [Cell 6 ìµœì¢…] Base Model ì•™ìƒë¸” í•™ìŠµ (3 Models)\n",
    "# ======================================================\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# ì—í­ì€ ì¶©ë¶„íˆ ì¤˜ì„œ ê¸°ë³¸ê¸°ë¥¼ í™•ì‹¤íˆ ë‹¤ì§€ê²Œ í•¨\n",
    "EPOCHS_BASE = 30 \n",
    "SEEDS = [42, 2024, 777] # ì„œë¡œ ë‹¤ë¥¸ ì‹œë“œ 3ê°œ\n",
    "\n",
    "print(f\">>> Ensemble Training Start (Training 3 Models)\")\n",
    "\n",
    "for i, seed in enumerate(SEEDS):\n",
    "    print(f\"\\nğŸš€ [Model {i+1}/3] Training with Seed {seed}...\")\n",
    "    seed_everything(seed) # ì‹œë“œ ë³€ê²½ (ë°ì´í„° ì„ì´ëŠ” ìˆœì„œê°€ ë°”ë€œ)\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ë¶„í• \n",
    "    train_idx, val_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=seed)\n",
    "    train_loader = DataLoader(torch.utils.data.Subset(full_dataset, train_idx), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(torch.utils.data.Subset(full_dataset, val_idx), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    model = TransformerPredictor(INPUT_DIM, D_MODEL, NHEAD, NUM_LAYERS, DROPOUT).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR_BASE)\n",
    "    # ìŠ¤ì¼€ì¤„ëŸ¬: ì„±ëŠ¥ ì •ì²´ì‹œ í•™ìŠµë¥  ë‚®ì¶¤\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    best_dist = float('inf')\n",
    "    \n",
    "    for epoch in range(EPOCHS_BASE):\n",
    "        model.train()\n",
    "        for seqs, targets, mask, _ in train_loader:\n",
    "            seqs, targets, mask = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(seqs, mask), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # ê²€ì¦ (Meter ê±°ë¦¬ ê¸°ì¤€)\n",
    "        model.eval()\n",
    "        dists = []\n",
    "        with torch.no_grad():\n",
    "            for seqs, targets, mask, _ in val_loader:\n",
    "                seqs, targets, mask = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE)\n",
    "                pred = model(seqs, mask)\n",
    "                p_real = pred.cpu().numpy() * np.array([105.0, 68.0])\n",
    "                t_real = targets.cpu().numpy() * np.array([105.0, 68.0])\n",
    "                dists.extend(np.sqrt(np.sum((p_real - t_real)**2, axis=1)))\n",
    "        \n",
    "        avg_dist = np.mean(dists)\n",
    "        scheduler.step(avg_dist)\n",
    "        \n",
    "        # ì‹œë“œë³„ë¡œ ìµœê³  ëª¨ë¸ ì €ì¥\n",
    "        if avg_dist < best_dist:\n",
    "            best_dist = avg_dist\n",
    "            torch.save(model.state_dict(), f'base_model_{i}.pth')\n",
    "            \n",
    "    print(f\"  --> Model {i} Finished. Best Val Dist: {best_dist:.4f}m\")\n",
    "\n",
    "print(\"âœ… All 3 Models Trained Successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5911a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# [Cell 8 ìˆ˜ì •] ID ë§¤í•‘ì„ í†µí•œ ì•ˆì „í•œ ì œì¶œ (Merge ë°©ì‹)\n",
    "# ======================================================\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (3ê°œ ì•™ìƒë¸”)\n",
    "models = []\n",
    "for i in range(3):\n",
    "    m = TransformerPredictor(INPUT_DIM, D_MODEL, NHEAD, NUM_LAYERS, DROPOUT).to(DEVICE)\n",
    "    m.load_state_dict(torch.load(f'base_model_{i}.pth'))\n",
    "    m.eval()\n",
    "    models.append(m)\n",
    "\n",
    "print(f\">>> Weighted Ensemble Inference with ID Tracking...\")\n",
    "results = [] # [ID, Pred_X, Pred_Y] ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "weights = [0.25, 0.25, 0.5]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # ë¡œë”ì—ì„œ episode_idsë¥¼ ê°™ì´ ë°›ìŒ\n",
    "    for seqs, mask, team_ids, episode_ids in tqdm(test_loader):\n",
    "        seqs, mask = seqs.to(DEVICE), mask.to(DEVICE)\n",
    "        \n",
    "        m1 = models[0](seqs, mask).cpu().numpy()\n",
    "        m2 = models[1](seqs, mask).cpu().numpy()\n",
    "        m3 = models[2](seqs, mask).cpu().numpy()\n",
    "        \n",
    "        avg_pred = (m1 * weights[0]) + (m2 * weights[1]) + (m3 * weights[2])\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥ (IDì™€ ì˜ˆì¸¡ê°’ì„ ì§ì§€ìŒ)\n",
    "        for i, eid in enumerate(episode_ids):\n",
    "            px = avg_pred[i, 0] * 105.0\n",
    "            py = avg_pred[i, 1] * 68.0\n",
    "            results.append({'game_episode': eid, 'pred_x': px, 'pred_y': py})\n",
    "\n",
    "# 1. ì˜ˆì¸¡ ê²°ê³¼ DataFrame ìƒì„±\n",
    "pred_df = pd.DataFrame(results)\n",
    "\n",
    "# 2. ì œì¶œ ì–‘ì‹ ë¡œë“œ\n",
    "SUBMISSION_PATH = \"open_track1/sample_submission.csv\"\n",
    "if os.path.exists(SUBMISSION_PATH):\n",
    "    sub = pd.read_csv(SUBMISSION_PATH)\n",
    "else:\n",
    "    sub = pd.read_csv(TEST_META_PATH)\n",
    "    # ì»¬ëŸ¼ëª… í†µì¼\n",
    "    col_map = {'episode_id': 'game_episode'}\n",
    "    sub = sub.rename(columns=col_map)\n",
    "    sub = sub[['game_episode']]\n",
    "\n",
    "print(f\"Submission Shape: {sub.shape}, Prediction Shape: {pred_df.shape}\")\n",
    "\n",
    "# 3. [í•µì‹¬] ID ê¸°ì¤€ìœ¼ë¡œ í•©ì¹˜ê¸° (Left Join)\n",
    "# ì œì¶œ íŒŒì¼ì˜ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ, ì˜ˆì¸¡ê°’ì„ ì˜†ì— ë¶™ì„\n",
    "final_sub = pd.merge(sub[['game_episode']], pred_df, on='game_episode', how='left')\n",
    "\n",
    "# 4. ì»¬ëŸ¼ëª… ë³€ê²½ ë° ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "final_sub = final_sub.rename(columns={'pred_x': 'end_x', 'pred_y': 'end_y'})\n",
    "\n",
    "# í˜¹ì‹œ ë§¤ì¹­ ì•ˆ ëœ ê²Œ ìˆëŠ”ì§€ í™•ì¸\n",
    "nan_count = final_sub.isnull().sum().sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"âš ï¸ ê²½ê³ : {nan_count}ê°œì˜ ê²°ì¸¡ì¹˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ID ë¶ˆì¼ì¹˜)\")\n",
    "    final_sub = final_sub.fillna(50.0) # ì¤‘ì•™ê°’ìœ¼ë¡œ ë•œë¹µ\n",
    "\n",
    "# 5. ì €ì¥\n",
    "filename = \"v3_submit.csv\"\n",
    "final_sub.to_csv(filename, index=False)\n",
    "print(f\"âœ… Submission Saved: {filename}\")\n",
    "print(final_sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eec295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daycon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
