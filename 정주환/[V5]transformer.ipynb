{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d00b8",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\nimport random\nimport os\nimport math\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nSEED = 42\nseed_everything(SEED)\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {DEVICE}\")\n\n# í•˜ì´í¼íŒŒë¼ë¯¸í„° [V4 Advanced - í•™ìŠµ ì „ëµ ê°œì„ ]\nBATCH_SIZE = 64\nLR_BASE = 1e-3\nWARMUP_EPOCHS = 3      # Warmup ì—í¬í¬\nEPOCHS_BASE = 40       # 30 -> 40 (ì¶©ë¶„í•œ í•™ìŠµ)\nD_MODEL = 128          # ì›ë˜ëŒ€ë¡œ\nNHEAD = 4              # ì›ë˜ëŒ€ë¡œ\nNUM_LAYERS = 2         # ì›ë˜ëŒ€ë¡œ\nDROPOUT = 0.2          # ì›ë˜ëŒ€ë¡œ\nMAX_SEQ_LEN = 30       # ì›ë˜ëŒ€ë¡œ\nGRAD_CLIP = 1.0        # Gradient Clipping"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70de82db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train Loaded: (356721, 15)\n",
      "â„¹ï¸ Reading 2414 test files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Test CSVs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2414/2414 [00:04<00:00, 570.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test Data Merged: (53110, 15)\n",
      "Data Ready. ID Column: game_episode\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# 1. ê²½ë¡œ ì„¤ì • ë° Train ë¡œë“œ\n",
    "# ======================================================\n",
    "BASE_DIR = \"./open_track1\" # ë°ì´í„°ê°€ ìˆëŠ” í´ë”ëª… (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)\n",
    "if not os.path.exists(BASE_DIR): BASE_DIR = \".\" # í´ë” ì—†ìœ¼ë©´ í˜„ì¬ ê²½ë¡œ\n",
    "\n",
    "TRAIN_PATH = os.path.join(BASE_DIR, \"train.csv\")\n",
    "TEST_META_PATH = os.path.join(BASE_DIR, \"test.csv\") # ì‚¬ìš©ìë‹˜ì´ ì˜¬ë¦¬ì‹  íŒŒì¼ëª…\n",
    "MATCH_PATH = os.path.join(BASE_DIR, \"match_info.csv\")\n",
    "\n",
    "# 1. Train ë¡œë“œ\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "print(f\"âœ… Train Loaded: {train_df.shape}\")\n",
    "\n",
    "# 2. Test í†µí•© ë¡œë“œ (í•µì‹¬ ìˆ˜ì • ì‚¬í•­)\n",
    "if os.path.exists(TEST_META_PATH):\n",
    "    test_meta = pd.read_csv(TEST_META_PATH)\n",
    "    print(f\"â„¹ï¸ Reading {len(test_meta)} test files...\")\n",
    "    \n",
    "    test_dfs = []\n",
    "    # tqdmìœ¼ë¡œ ì§„í–‰ìƒí™© í™•ì¸\n",
    "    for _, row in tqdm(test_meta.iterrows(), total=len(test_meta), desc=\"Loading Test CSVs\"):\n",
    "        rel_path = row['path']\n",
    "        \n",
    "        # ê²½ë¡œ ë³´ì •: 1) ê·¸ëŒ€ë¡œ, 2) í´ë” ë¶™ì—¬ì„œ, 3) íŒŒì¼ëª…ë§Œìœ¼ë¡œ ì°¾ê¸°\n",
    "        paths_to_try = [\n",
    "            rel_path,\n",
    "            os.path.join(BASE_DIR, rel_path.lstrip(\"./\")),\n",
    "            os.path.join(BASE_DIR, \"test\", str(row['game_id']), os.path.basename(rel_path))\n",
    "        ]\n",
    "        \n",
    "        for p in paths_to_try:\n",
    "            if os.path.exists(p):\n",
    "                test_dfs.append(pd.read_csv(p))\n",
    "                break\n",
    "        \n",
    "    if test_dfs:\n",
    "        test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "        print(f\"âœ… Test Data Merged: {test_df.shape}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Test íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"test.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 3. Match Info ë³‘í•©\n",
    "if os.path.exists(MATCH_PATH):\n",
    "    match_info = pd.read_csv(MATCH_PATH)\n",
    "    match_subset = match_info[['game_id', 'home_team_id', 'venue']]\n",
    "    train_df = pd.merge(train_df, match_subset, on='game_id', how='left')\n",
    "    test_df = pd.merge(test_df, match_subset, on='game_id', how='left')\n",
    "\n",
    "# 4. ì „ì²˜ë¦¬ (ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°)\n",
    "def preprocess(df):\n",
    "    if 'home_team_id' in df.columns:\n",
    "        df['is_home'] = (df['team_id'] == df['home_team_id']).astype(float)\n",
    "    else:\n",
    "        df['is_home'] = 0.5\n",
    "        \n",
    "    # V2 ë¡œì§ ì‘ë™ì„ ìœ„í•´ end_xê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€\n",
    "    if 'end_x' not in df.columns:\n",
    "        df['end_x'] = 0.0\n",
    "        df['end_y'] = 0.0\n",
    "    else:\n",
    "        df['end_x'] = df['end_x'].fillna(0.0)\n",
    "        df['end_y'] = df['end_y'].fillna(0.0)\n",
    "    return df\n",
    "\n",
    "train_df = preprocess(train_df)\n",
    "test_df = preprocess(test_df)\n",
    "\n",
    "ID_COL = 'game_episode' if 'game_episode' in train_df.columns else 'episode_id'\n",
    "print(f\"Data Ready. ID Column: {ID_COL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5662aa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Advanced Feature Logic Ready.\n",
      "   Input Dimension: 36\n",
      "   (Included: Speed, Time Delta, Player Continuity, Event Type, Result)\n"
     ]
    }
   ],
   "source": [
    "# 1. ë²”ì£¼í˜• ë³€ìˆ˜ ì •ì˜ (Train ë°ì´í„° ë¶„ì„ ê¸°ë°˜)\n",
    "TOP_TYPES = ['Pass', 'Carry', 'Recovery', 'Interception', 'Duel', 'Tackle', \n",
    "             'Throw-In', 'Clearance', 'Intervention', 'Block', 'Pass_Freekick', \n",
    "             'Cross', 'Goal Kick', 'Error', 'Shot']\n",
    "ALL_RESULTS = ['Successful', 'Unsuccessful', 'On Target', 'Yellow_Card', \n",
    "               'Blocked', 'Keeper Rush-Out', 'Low Quality Shot', 'Off Target']\n",
    "\n",
    "def make_features(group):\n",
    "    n = len(group)\n",
    "    \n",
    "    # --- 1. ê¸°ë³¸ ì¢Œí‘œ & Context ---\n",
    "    sx = group['start_x'].values / 105.0\n",
    "    sy = group['start_y'].values / 68.0\n",
    "    ex = group['end_x'].values / 105.0\n",
    "    ey = group['end_y'].values / 68.0\n",
    "    is_home = group['is_home'].values\n",
    "    \n",
    "    # --- 2. ì‹œê°„ & ì†ë„ (New!) ---\n",
    "    # time_secondsê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ 0 ì²˜ë¦¬\n",
    "    if 'time_seconds' in group.columns:\n",
    "        times = group['time_seconds'].values\n",
    "        # ì‹œê°„ ê°„ê²© (Delta Time): í˜„ì¬ - ì§ì „\n",
    "        # 0ë²ˆì§¸ëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬ (í˜¹ì€ 1ì´ˆ ë“± ê¸°ë³¸ê°’)\n",
    "        dt = np.zeros(n, dtype=np.float32)\n",
    "        dt[1:] = times[1:] - times[:-1]\n",
    "        dt = np.maximum(dt, 0.1) # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€ (ìµœì†Œ 0.1ì´ˆ)\n",
    "    else:\n",
    "        dt = np.ones(n, dtype=np.float32) # ì‹œê°„ ì •ë³´ ì—†ìœ¼ë©´ 1ì´ˆë¡œ ê°€ì •\n",
    "\n",
    "    # --- 3. ì´ë™ëŸ‰ & ê±°ë¦¬ ê³„ì‚° ---\n",
    "    dx = ex - sx\n",
    "    dy = ey - sy\n",
    "    # ì •ê·œí™”ëœ ê±°ë¦¬ ë§ê³ , ì‹¤ì œ ë¯¸í„° ê±°ë¦¬ ì‚¬ìš© (ì†ë„ ê³„ì‚°ìš©)\n",
    "    dist_meter = np.sqrt((dx*105)**2 + (dy*68)**2)\n",
    "    \n",
    "    # --- 4. í†µê³„ëŸ‰ (Lagged Features for Leakage Prevention) ---\n",
    "    # (1) ëˆ„ì  ì´ë™ëŸ‰ (Flow)\n",
    "    cumsum_dx = np.cumsum(dx) / 105.0\n",
    "    cumsum_dy = np.cumsum(dy) / 68.0\n",
    "    \n",
    "    # (2) ì§ì „ ì´ë™ ê±°ë¦¬ & ëˆ„ì ê°’ (Lagged)\n",
    "    # lag_val[i] = val[i-1] (ì§ì „ í–‰ë™ì˜ ê²°ê³¼)\n",
    "    lag_dist_m = np.roll(dist_meter, 1); lag_dist_m[0] = 0\n",
    "    lag_cumsum_dx = np.roll(cumsum_dx, 1); lag_cumsum_dx[0] = 0\n",
    "    lag_cumsum_dy = np.roll(cumsum_dy, 1); lag_cumsum_dy[0] = 0\n",
    "    \n",
    "    # (3) ì§ì „ ì†ë„ (Speed) - New!\n",
    "    # ì§ì „ ê±°ë¦¬ / ì§ì „ ì‹œê°„ê°„ê²© (m/s)\n",
    "    lag_dt = np.roll(dt, 1); lag_dt[0] = 1.0 # 0 ë‚˜ëˆ„ê¸° ë°©ì§€\n",
    "    lag_speed = lag_dist_m / np.maximum(lag_dt, 0.1)\n",
    "    \n",
    "    # --- 5. ì„ ìˆ˜ ì •ë³´ (New!) ---\n",
    "    # ê°™ì€ ì„ ìˆ˜ê°€ ì—°ì†ìœ¼ë¡œ ê³µì„ ì¡ì•˜ëŠ”ê°€? (ë“œë¦¬ë¸”/í‚¤í•‘ ì—¬ë¶€)\n",
    "    if 'player_id' in group.columns:\n",
    "        p_ids = group['player_id'].values\n",
    "        # [i]ì™€ [i-1]ì´ ê°™ìœ¼ë©´ 1, ë‹¤ë¥´ë©´ 0\n",
    "        is_same = np.zeros(n, dtype=np.float32)\n",
    "        is_same[1:] = (p_ids[1:] == p_ids[:-1]).astype(np.float32)\n",
    "    else:\n",
    "        is_same = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "    # --- 6. ì§„í–‰ë¥  & ê¸°íƒ€ ---\n",
    "    progress = np.arange(n) / max(n-1, 1)\n",
    "    # í›„ë°˜ì „ ì—¬ë¶€ (period_idê°€ 2 ì´ìƒì´ë©´ 1)\n",
    "    is_second_half = (group['period_id'].values > 1).astype(np.float32) if 'period_id' in group.columns else np.zeros(n)\n",
    "    \n",
    "    # --- 7. ë²”ì£¼í˜• ë³€ìˆ˜ One-hot Encoding (New!) ---\n",
    "    # (1) Type Name (Event ì¢…ë¥˜)\n",
    "    # Test ë§ˆì§€ë§‰ í–‰ì—ë„ 'Pass', 'Carry' ë“±ì€ ì£¼ì–´ì§€ë¯€ë¡œ ì‚¬ìš© ê°€ëŠ¥\n",
    "    # ë¯¸ë¦¬ ì •ì˜ëœ TOP_TYPESì— ëŒ€í•´ 1/0 ë§¤í•‘\n",
    "    types_onehot = np.zeros((n, len(TOP_TYPES) + 1), dtype=np.float32) # +1 for Other\n",
    "    curr_types = group['type_name'].values\n",
    "    for i, t in enumerate(curr_types):\n",
    "        if t in TOP_TYPES:\n",
    "            idx = TOP_TYPES.index(t)\n",
    "            types_onehot[i, idx] = 1.0\n",
    "        else:\n",
    "            types_onehot[i, -1] = 1.0 # Other\n",
    "\n",
    "    # (2) Result Name (ì„±ê³µ ì—¬ë¶€)\n",
    "    # Test ë§ˆì§€ë§‰ í–‰ì´ ë§Œì•½ Pass(Successful)ì´ë¼ë©´ \"ì„±ê³µí•œ íŒ¨ìŠ¤ì˜ ë„ì°©ì \"ì„ ë¬»ëŠ” ê²ƒì´ë¯€ë¡œ ì‚¬ìš© ê°€ëŠ¥.\n",
    "    # ë§Œì•½ ë¹„ì–´ìˆë‹¤ë©´(nan) 0ìœ¼ë¡œ ì²˜ë¦¬ë¨.\n",
    "    results_onehot = np.zeros((n, len(ALL_RESULTS) + 1), dtype=np.float32) # +1 for Nan/Other\n",
    "    curr_results = group['result_name'].values\n",
    "    for i, r in enumerate(curr_results):\n",
    "        if r in ALL_RESULTS:\n",
    "            idx = ALL_RESULTS.index(r)\n",
    "            results_onehot[i, idx] = 1.0\n",
    "        else:\n",
    "            results_onehot[i, -1] = 1.0 # Nan or Other\n",
    "\n",
    "    features = []\n",
    "    \n",
    "    # --- 8. ì‹œí€€ìŠ¤ ê²°í•© ---\n",
    "    for i in range(n):\n",
    "        # Scalar Features (Vector Concatenation)\n",
    "        scalars = [\n",
    "            sx[i], sy[i],             # 1. ìœ„ì¹˜\n",
    "            lag_cumsum_dx[i], lag_cumsum_dy[i], # 2. ëˆ„ì  íë¦„ (Safe)\n",
    "            lag_dist_m[i] / 100.0,    # 3. ì§ì „ ê±°ë¦¬ (ìŠ¤ì¼€ì¼ë§: ëŒ€ëµ 100më¡œ ë‚˜ëˆ”)\n",
    "            lag_speed[i] / 10.0,      # 4. ì§ì „ ì†ë„ (ìŠ¤ì¼€ì¼ë§: ëŒ€ëµ 10m/së¡œ ë‚˜ëˆ”)\n",
    "            dt[i] / 10.0,             # 5. í˜„ì¬ ì‹œê°„ ê²½ê³¼ (ìŠ¤ì¼€ì¼ë§)\n",
    "            progress[i],              # 6. ì§„í–‰ë¥ \n",
    "            is_home[i],               # 7. í™ˆ ì—¬ë¶€\n",
    "            is_same[i],               # 8. ì„ ìˆ˜ ì—°ì†ì„±\n",
    "            is_second_half[i]         # 9. í›„ë°˜ì „ ì—¬ë¶€\n",
    "        ]\n",
    "        \n",
    "        # Combine: Scalar + Type Onehot + Result Onehot\n",
    "        feat_vec = np.concatenate([scalars, types_onehot[i], results_onehot[i]])\n",
    "        \n",
    "        # (1) Start Node\n",
    "        features.append(feat_vec)\n",
    "        \n",
    "        # (2) End Node (ë§ˆì§€ë§‰ íƒ€ê²Ÿ ì œì™¸) -> ì—¬ê¸°ë„ ì°¨ì› ë§ì¶°ì¤˜ì•¼ í•¨\n",
    "        if i < n - 1:\n",
    "            # End NodeëŠ” 'ìœ„ì¹˜'ë§Œ ë°”ë€Œê³  ë‚˜ë¨¸ì§€ëŠ” ê·¸ ì‹œì  ìƒíƒœ ìœ ì§€\n",
    "            # ë‹¨, End ì‹œì ì˜ ëˆ„ì /ê±°ë¦¬ëŠ” 'í˜„ì¬(i)' ê°’ì„ ì¨ì•¼ í•¨ (ì´ë¯¸ ì´ë™í–ˆìœ¼ë¯€ë¡œ)\n",
    "            scalars_end = scalars.copy()\n",
    "            scalars_end[0] = ex[i] # End X\n",
    "            scalars_end[1] = ey[i] # End Y\n",
    "            # End Nodeì—ëŠ” 'ì´ë²ˆ ì´ë™'ì˜ ê²°ê³¼ê°€ ë°˜ì˜ëœ í†µê³„ ì‚¬ìš© ê°€ëŠ¥\n",
    "            scalars_end[2] = cumsum_dx[i]\n",
    "            scalars_end[3] = cumsum_dy[i]\n",
    "            \n",
    "            feat_vec_end = np.concatenate([scalars_end, types_onehot[i], results_onehot[i]])\n",
    "            features.append(feat_vec_end)\n",
    "            \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "# ì…ë ¥ ì°¨ì› ìë™ ê³„ì‚°\n",
    "# Scalar(11) + Type(16) + Result(9) = 36\n",
    "dummy_group = train_df.iloc[:5].copy()\n",
    "dummy_feat = make_features(dummy_group)\n",
    "INPUT_DIM = dummy_feat.shape[1]\n",
    "\n",
    "print(f\"âœ… Advanced Feature Logic Ready.\")\n",
    "print(f\"   Input Dimension: {INPUT_DIM}\")\n",
    "print(f\"   (Included: Speed, Time Delta, Player Continuity, Event Type, Result)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d92c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15435/15435 [00:12<00:00, 1263.30it/s]\n",
      "Dataset (test): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2414/2414 [00:01<00:00, 1423.57it/s]\n"
     ]
    }
   ],
   "source": [
    "class SoccerDataset(Dataset):\n",
    "    def __init__(self, df, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.episodes = []\n",
    "        self.targets = []\n",
    "        self.team_ids = []\n",
    "        self.episode_ids = []\n",
    "        \n",
    "        # ìˆœì„œ ì„ì„ ë°©ì§€\n",
    "        grouped = df.groupby(ID_COL, sort=False)\n",
    "        \n",
    "        for name, group in tqdm(grouped, desc=f\"Dataset ({mode})\"):\n",
    "            if mode == 'train' and len(group) < 2: continue\n",
    "            \n",
    "            # Feature Engineering\n",
    "            if 'time_seconds' in group.columns:\n",
    "                 seq = make_features(group)\n",
    "            else:\n",
    "                 seq = make_v2_features_corrected(group)\n",
    "\n",
    "            team_id = group.iloc[0]['team_id']\n",
    "            \n",
    "            if mode == 'train':\n",
    "                last = group.iloc[-1]\n",
    "                self.targets.append([last['end_x']/105.0, last['end_y']/68.0])\n",
    "                self.episodes.append(seq)\n",
    "                self.team_ids.append(team_id)\n",
    "            else:\n",
    "                self.episodes.append(seq)\n",
    "                self.team_ids.append(team_id)\n",
    "                self.episode_ids.append(str(name))\n",
    "\n",
    "    def __len__(self): return len(self.episodes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = torch.FloatTensor(self.episodes[idx])\n",
    "        if len(seq) > MAX_SEQ_LEN: seq = seq[-MAX_SEQ_LEN:]\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            # Train ë°˜í™˜: (seq, target, team_id)\n",
    "            return seq, torch.FloatTensor(self.targets[idx]), self.team_ids[idx]\n",
    "        else:\n",
    "            # Test ë°˜í™˜: (seq, team_id, episode_id)\n",
    "            return seq, self.team_ids[idx], self.episode_ids[idx]\n",
    "\n",
    "# [í•µì‹¬ ìˆ˜ì •] collate_fn ë¡œì§ ë³€ê²½\n",
    "def collate_fn(batch):\n",
    "    seqs = [b[0] for b in batch]\n",
    "    lens = torch.LongTensor([len(s) for s in seqs])\n",
    "    padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    mask = torch.arange(padded.size(1))[None, :] >= lens[:, None]\n",
    "    \n",
    "    # ë°°ì¹˜ ì²« ë²ˆì§¸ ìš”ì†Œë¡œ ëª¨ë“œë¥¼ íŒë‹¨\n",
    "    elem = batch[0]\n",
    "    \n",
    "    # Test Mode í™•ì¸ (3ë²ˆì§¸ ìš”ì†Œê°€ ë¬¸ìì—´ IDì¸ ê²½ìš°)\n",
    "    if isinstance(elem[2], str):\n",
    "        # Test: b[1]ì´ Team ID\n",
    "        team_ids = torch.LongTensor([b[1] for b in batch])\n",
    "        episode_ids = [b[2] for b in batch]\n",
    "        return padded, mask, team_ids, episode_ids\n",
    "        \n",
    "    else:\n",
    "        # Train Mode: b[1]ì€ Target(ì¢Œí‘œ), b[2]ê°€ Team ID\n",
    "        targets = torch.stack([b[1] for b in batch])\n",
    "        team_ids_train = torch.LongTensor([b[2] for b in batch]) \n",
    "        return padded, targets, mask, team_ids_train\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "full_dataset = SoccerDataset(train_df, mode='train')\n",
    "test_dataset = SoccerDataset(test_df, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca77e0",
   "metadata": {},
   "outputs": [],
   "source": "# ======================================================\n# [V5] Sinusoidal Positional Encoding + GELU Activation\n# ======================================================\nclass SinusoidalPositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=100):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1), :]\n\nclass TransformerPredictor(nn.Module):\n    def __init__(self, input_dim, d_model, nhead, num_layers, dropout, max_seq_len=MAX_SEQ_LEN):\n        super().__init__()\n        self.embedding = nn.Linear(input_dim, d_model)\n\n        # [V5] Sinusoidal Positional Encoding (ë” ì•ˆì •ì ì¸ ì„±ëŠ¥)\n        self.pos_encoder = SinusoidalPositionalEncoding(d_model, max_len=max_seq_len)\n        self.dropout = nn.Dropout(dropout)\n\n        # [V5] GELU í™œì„±í™” í•¨ìˆ˜ ì ìš©\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=d_model*4,\n            dropout=dropout,\n            activation='gelu',  # ReLU â†’ GELU\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        self.fc = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model // 2, 2)\n        )\n\n    def forward(self, x, mask=None):\n        # Embedding\n        x = self.embedding(x)\n\n        # Sinusoidal Positional Encoding ì¶”ê°€\n        x = self.pos_encoder(x)\n\n        x = self.dropout(x)\n        output = self.transformer(x, src_key_padding_mask=mask)\n\n        # Valid Token Mean Pooling\n        valid_mask = (~mask).unsqueeze(-1).float()\n        mean_out = (output * valid_mask).sum(dim=1) / valid_mask.sum(dim=1).clamp(min=1e-9)\n        return self.fc(mean_out)\n\nmodel = TransformerPredictor(INPUT_DIM, D_MODEL, NHEAD, NUM_LAYERS, DROPOUT).to(DEVICE)\nprint(\"âœ… [V5] Sinusoidal PE + GELU ì ìš© ì™„ë£Œ (Learnable PEë³´ë‹¤ ì•ˆì •ì )\")"
  },
  {
   "cell_type": "code",
   "id": "1tk1izm28w8",
   "source": "# ======================================================\n# Sinusoidal Positional Encoding ì‹œê°í™”\n# ======================================================\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import font_manager, rc\n\n# í•œê¸€ í°íŠ¸ ì„¤ì •\nplt.rcParams['font.family'] = 'Malgun Gothic'  # Windows\nplt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€\n\ndef visualize_positional_encoding(max_len=30, d_model=128):\n    \"\"\"\n    Sinusoidal Positional Encodingì˜ íŒ¨í„´ì„ ì‹œê°í™”\n    \n    ê° ìœ„ì¹˜(position)ë§ˆë‹¤ ê³ ìœ í•œ sin/cos ì¡°í•©ì´ ìƒì„±ë˜ì–´,\n    ëª¨ë¸ì´ \"3ë²ˆì§¸ íŒ¨ìŠ¤\"ì™€ \"10ë²ˆì§¸ íŒ¨ìŠ¤\"ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n    \"\"\"\n    # Positional Encoding ìƒì„±\n    pe = torch.zeros(max_len, d_model)\n    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    \n    pe_array = pe.numpy()\n    \n    # Figure ìƒì„±\n    fig = plt.figure(figsize=(18, 10))\n    gs = fig.add_gridspec(3, 2, hspace=0.35, wspace=0.3)\n    \n    # ========================================\n    # 1. Positional Encoding ì „ì²´ íˆíŠ¸ë§µ\n    # ========================================\n    ax1 = fig.add_subplot(gs[0, :])\n    im = ax1.imshow(pe_array.T, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n    ax1.set_xlabel('Position (íŒ¨ìŠ¤ ìˆœì„œ)', fontsize=12, fontweight='bold')\n    ax1.set_ylabel('Embedding Dimension', fontsize=12, fontweight='bold')\n    ax1.set_title('Sinusoidal Positional Encoding Matrix (30 positions Ã— 128 dimensions)', \n                  fontsize=14, fontweight='bold', pad=15)\n    \n    # íŠ¹ì • ìœ„ì¹˜ ê°•ì¡° (3, 10, 20ë²ˆì§¸)\n    for pos in [3, 10, 20]:\n        ax1.axvline(x=pos, color='yellow', linewidth=2, linestyle='--', alpha=0.7)\n        ax1.text(pos, d_model + 5, f'Pos {pos}', ha='center', fontsize=10, \n                color='yellow', fontweight='bold')\n    \n    plt.colorbar(im, ax=ax1, label='Encoding Value')\n    ax1.grid(False)\n    \n    # ========================================\n    # 2. ê° ì°¨ì›ì˜ ì£¼íŒŒìˆ˜ íŒ¨í„´ (ì„ íƒëœ ì°¨ì›ë“¤)\n    # ========================================\n    ax2 = fig.add_subplot(gs[1, 0])\n    \n    # ë‹¤ì–‘í•œ ì£¼íŒŒìˆ˜ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì—¬ëŸ¬ ì°¨ì› ì„ íƒ\n    dims_to_plot = [0, 2, 8, 16, 32, 64]\n    colors_line = plt.cm.viridis(np.linspace(0, 1, len(dims_to_plot)))\n    \n    for dim, color in zip(dims_to_plot, colors_line):\n        ax2.plot(range(max_len), pe_array[:, dim], \n                label=f'Dim {dim}', linewidth=2, color=color, alpha=0.8)\n    \n    ax2.set_xlabel('Position (íŒ¨ìŠ¤ ìˆœì„œ)', fontsize=11, fontweight='bold')\n    ax2.set_ylabel('Encoding Value', fontsize=11, fontweight='bold')\n    ax2.set_title('ê° ì°¨ì›ì˜ Sinusoidal íŒ¨í„´ (ë‹¤ì–‘í•œ ì£¼íŒŒìˆ˜)', fontsize=12, fontweight='bold')\n    ax2.legend(loc='upper right', fontsize=9)\n    ax2.grid(True, alpha=0.3)\n    ax2.axhline(y=0, color='black', linewidth=0.8, linestyle='-', alpha=0.3)\n    \n    # ========================================\n    # 3. íŠ¹ì • ìœ„ì¹˜ë“¤ì˜ ì¸ì½”ë”© ë²¡í„° ë¹„êµ\n    # ========================================\n    ax3 = fig.add_subplot(gs[1, 1])\n    \n    positions_compare = [3, 10, 20]\n    colors_bar = ['#FF6B6B', '#4ECDC4', '#FFE66D']\n    \n    x_offset = np.arange(len(positions_compare))\n    bar_width = 0.25\n    \n    # ì²˜ìŒ 12ê°œ ì°¨ì›ë§Œ ë³´ì—¬ì£¼ê¸° (ê°€ë…ì„±)\n    dims_show = 12\n    for i, pos in enumerate(positions_compare):\n        ax3.bar(np.arange(dims_show) + i * bar_width, \n               pe_array[pos, :dims_show], \n               width=bar_width, \n               label=f'Position {pos}',\n               color=colors_bar[i],\n               alpha=0.8)\n    \n    ax3.set_xlabel('Embedding Dimension (ì²˜ìŒ 12ê°œ)', fontsize=11, fontweight='bold')\n    ax3.set_ylabel('Encoding Value', fontsize=11, fontweight='bold')\n    ax3.set_title('ë‹¤ë¥¸ ìœ„ì¹˜ë“¤ì˜ ì¸ì½”ë”© ë²¡í„° ë¹„êµ', fontsize=12, fontweight='bold')\n    ax3.legend(fontsize=10)\n    ax3.grid(True, alpha=0.3, axis='y')\n    ax3.set_xticks(np.arange(dims_show) + bar_width)\n    ax3.set_xticklabels([f'{i}' for i in range(dims_show)])\n    \n    # ========================================\n    # 4. ìœ„ì¹˜ ê°„ ê±°ë¦¬ (ìœ ì‚¬ë„) íˆíŠ¸ë§µ\n    # ========================================\n    ax4 = fig.add_subplot(gs[2, 0])\n    \n    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n    from sklearn.metrics.pairwise import cosine_similarity\n    similarity = cosine_similarity(pe_array[:max_len])\n    \n    im4 = ax4.imshow(similarity, cmap='YlGnBu', aspect='auto')\n    ax4.set_xlabel('Position', fontsize=11, fontweight='bold')\n    ax4.set_ylabel('Position', fontsize=11, fontweight='bold')\n    ax4.set_title('ìœ„ì¹˜ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ê° ìœ„ì¹˜ì˜ ê³ ìœ ì„±)', fontsize=12, fontweight='bold')\n    plt.colorbar(im4, ax=ax4, label='Cosine Similarity')\n    \n    # ëŒ€ê°ì„  (ìê¸° ìì‹ )ì€ í•­ìƒ 1ì´ê³ , ë©€ì–´ì§ˆìˆ˜ë¡ ìœ ì‚¬ë„ê°€ ë‚®ì•„ì§ì„ ë³´ì—¬ì¤Œ\n    ax4.text(max_len//2, max_len - 2, \n            'ëŒ€ê°ì„ : ìê¸° ìì‹  (ìœ ì‚¬ë„=1)\\në©€ìˆ˜ë¡ ë‚®ì€ ìœ ì‚¬ë„',\n            ha='center', va='top', \n            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n            fontsize=9)\n    \n    # ========================================\n    # 5. ì£¼íŒŒìˆ˜ ë¶„ì„ (FFT)\n    # ========================================\n    ax5 = fig.add_subplot(gs[2, 1])\n    \n    # ëª‡ ê°œ ì°¨ì›ì˜ FFT ê³„ì‚°\n    dims_fft = [0, 4, 16, 64]\n    \n    for dim in dims_fft:\n        signal = pe_array[:, dim]\n        fft = np.fft.fft(signal)\n        freqs = np.fft.fftfreq(len(signal))\n        \n        # ì–‘ìˆ˜ ì£¼íŒŒìˆ˜ë§Œ\n        pos_mask = freqs > 0\n        ax5.plot(freqs[pos_mask], np.abs(fft[pos_mask]), \n                label=f'Dim {dim}', linewidth=2, alpha=0.7)\n    \n    ax5.set_xlabel('ì£¼íŒŒìˆ˜ (Frequency)', fontsize=11, fontweight='bold')\n    ax5.set_ylabel('ê°•ë„ (Magnitude)', fontsize=11, fontweight='bold')\n    ax5.set_title('ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ë¶„ì„ (ê° ì°¨ì›ì˜ ì£¼íŒŒìˆ˜ ì„±ë¶„)', fontsize=12, fontweight='bold')\n    ax5.legend(fontsize=9)\n    ax5.grid(True, alpha=0.3)\n    ax5.set_xlim([0, 0.5])\n    \n    # ì „ì²´ íƒ€ì´í‹€\n    fig.suptitle('ğŸ¯ Sinusoidal Positional Encoding ì‹œê°í™”\\n\"ê° ìœ„ì¹˜(íŒ¨ìŠ¤)ë§ˆë‹¤ ê³ ìœ í•œ sin/cos ì§€ë¬¸ì„ ê°€ì§‘ë‹ˆë‹¤\"', \n                 fontsize=16, fontweight='bold', y=0.995)\n    \n    plt.savefig('positional_encoding_visualization.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # ========================================\n    # ìš”ì•½ í†µê³„ ì¶œë ¥\n    # ========================================\n    print(\"\\n\" + \"=\"*60)\n    print(\"ğŸ“Š Positional Encoding ìš”ì•½ í†µê³„\")\n    print(\"=\"*60)\n    print(f\"Max Length (ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´): {max_len}\")\n    print(f\"D_Model (ì„ë² ë”© ì°¨ì›): {d_model}\")\n    print(f\"Encoding ë²”ìœ„: [{pe_array.min():.4f}, {pe_array.max():.4f}]\")\n    print(f\"\\nìœ„ì¹˜ ê°„ í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„:\")\n    for i, pos1 in enumerate([0, 5, 10, 15, 20, 25]):\n        if pos1 >= max_len: continue\n        avg_sim = np.mean([similarity[pos1, pos2] for pos2 in range(max_len) if pos2 != pos1])\n        print(f\"  Position {pos1:2d}: {avg_sim:.4f} (ë‹¤ë¥¸ ìœ„ì¹˜ë“¤ê³¼ì˜ í‰ê·  ìœ ì‚¬ë„)\")\n    \n    print(f\"\\nâœ… ê° ìœ„ì¹˜ëŠ” ê³ ìœ í•œ sin/cos íŒ¨í„´ì„ ê°€ì§€ë¯€ë¡œ\")\n    print(f\"   íŠ¸ëœìŠ¤í¬ë¨¸ê°€ '3ë²ˆì§¸ íŒ¨ìŠ¤'ì™€ '10ë²ˆì§¸ íŒ¨ìŠ¤'ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")\n    print(\"=\"*60 + \"\\n\")\n\n# ì‹¤í–‰\nvisualize_positional_encoding(max_len=MAX_SEQ_LEN, d_model=D_MODEL)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63608a68",
   "metadata": {},
   "outputs": [],
   "source": "# ======================================================\n# [V5 GELU] ê³ ê¸‰ í•™ìŠµ ì „ëµ ì ìš©\n# - Cosine Annealing Scheduler\n# - Warmup Learning Rate\n# - Gradient Clipping\n# - Huber Loss\n# ======================================================\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nSEEDS = [42, 2024, 777]\nprint(f\">>> [V5 GELU] Advanced Training Start (3 Models)\")\n\n# íˆìŠ¤í† ë¦¬ ì €ì¥ìš©\nall_histories = []\n\nfor i, seed in enumerate(SEEDS):\n    print(f\"\\nğŸš€ [Model {i+1}/3] Training with Seed {seed}...\")\n    seed_everything(seed)\n    \n    # ë°ì´í„°ì…‹ ë¶„í• \n    train_idx, val_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=seed)\n    train_loader = DataLoader(torch.utils.data.Subset(full_dataset, train_idx), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(torch.utils.data.Subset(full_dataset, val_idx), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n    # ëª¨ë¸ ì´ˆê¸°í™”\n    model = TransformerPredictor(INPUT_DIM, D_MODEL, NHEAD, NUM_LAYERS, DROPOUT).to(DEVICE)\n    optimizer = optim.AdamW(model.parameters(), lr=LR_BASE, weight_decay=1e-4)\n    \n    # Cosine Annealing Scheduler (Warmup ì´í›„ ì ìš©)\n    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS_BASE - WARMUP_EPOCHS, eta_min=1e-6)\n    \n    # Huber Loss (MSEë³´ë‹¤ ì´ìƒì¹˜ì— ê°•í•¨)\n    criterion = nn.HuberLoss(delta=1.0)\n    \n    best_dist = float('inf')\n    \n    # íˆìŠ¤í† ë¦¬ ì €ì¥\n    history = {\n        'train_loss': [],\n        'val_dist': [],\n        'lr': []\n    }\n    \n    for epoch in range(EPOCHS_BASE):\n        # === Warmup Learning Rate ===\n        if epoch < WARMUP_EPOCHS:\n            lr = LR_BASE * (epoch + 1) / WARMUP_EPOCHS\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr\n        \n        # === Training ===\n        model.train()\n        train_loss = 0.0\n        for seqs, targets, mask, _ in train_loader:\n            seqs, targets, mask = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE)\n            \n            optimizer.zero_grad()\n            pred = model(seqs, mask)\n            loss = criterion(pred, targets)\n            loss.backward()\n            \n            # Gradient Clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n            \n            optimizer.step()\n            train_loss += loss.item()\n        \n        # === Validation ===\n        model.eval()\n        dists = []\n        with torch.no_grad():\n            for seqs, targets, mask, _ in val_loader:\n                seqs, targets, mask = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE)\n                pred = model(seqs, mask)\n                p_real = pred.cpu().numpy() * np.array([105.0, 68.0])\n                t_real = targets.cpu().numpy() * np.array([105.0, 68.0])\n                dists.extend(np.sqrt(np.sum((p_real - t_real)**2, axis=1)))\n        \n        avg_dist = np.mean(dists)\n        avg_train_loss = train_loss / len(train_loader)\n        \n        # Scheduler Step (Warmup ì´í›„)\n        if epoch >= WARMUP_EPOCHS:\n            scheduler.step()\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # íˆìŠ¤í† ë¦¬ ì €ì¥\n        history['train_loss'].append(avg_train_loss)\n        history['val_dist'].append(avg_dist)\n        history['lr'].append(current_lr)\n        \n        # ë¡œê·¸ ì¶œë ¥\n        if (epoch + 1) % 5 == 0 or epoch < 5:\n            print(f\"  [Epoch {epoch+1:2d}/{EPOCHS_BASE}] \"\n                  f\"Loss: {avg_train_loss:.4f} | \"\n                  f\"Val Dist: {avg_dist:.4f}m | \"\n                  f\"LR: {current_lr:.6f}\")\n        \n        # Best Model ì €ì¥\n        if avg_dist < best_dist:\n            best_dist = avg_dist\n            torch.save(model.state_dict(), f'base_model_{i}.pth')\n    \n    history['best_dist'] = best_dist\n    history['seed'] = seed\n    all_histories.append(history)\n    \n    print(f\"  âœ… Model {i} Finished. Best Val Dist: {best_dist:.4f}m\")\n\nprint(\"\\nâœ… All 3 Models Trained Successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5911a718",
   "metadata": {},
   "outputs": [],
   "source": "# ======================================================\n# [V5 Sinusoidal PE + GELU] ID ë§¤í•‘ì„ í†µí•œ ì•ˆì „í•œ ì œì¶œ (Merge ë°©ì‹)\n# ======================================================\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n# ëª¨ë¸ ë¡œë“œ (3ê°œ ì•™ìƒë¸”)\nmodels = []\nfor i in range(3):\n    m = TransformerPredictor(INPUT_DIM, D_MODEL, NHEAD, NUM_LAYERS, DROPOUT).to(DEVICE)\n    m.load_state_dict(torch.load(f'base_model_{i}.pth'))\n    m.eval()\n    models.append(m)\n\nprint(f\">>> [V5 Sinusoidal PE + GELU] Weighted Ensemble Inference...\")\nresults = [] # [ID, Pred_X, Pred_Y] ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\nweights = [0.25, 0.25, 0.5]\n\nwith torch.no_grad():\n    # ë¡œë”ì—ì„œ episode_idsë¥¼ ê°™ì´ ë°›ìŒ\n    for seqs, mask, team_ids, episode_ids in tqdm(test_loader):\n        seqs, mask = seqs.to(DEVICE), mask.to(DEVICE)\n        \n        m1 = models[0](seqs, mask).cpu().numpy()\n        m2 = models[1](seqs, mask).cpu().numpy()\n        m3 = models[2](seqs, mask).cpu().numpy()\n        \n        avg_pred = (m1 * weights[0]) + (m2 * weights[1]) + (m3 * weights[2])\n        \n        # ê²°ê³¼ ì €ì¥ (IDì™€ ì˜ˆì¸¡ê°’ì„ ì§ì§€ìŒ)\n        for i, eid in enumerate(episode_ids):\n            px = avg_pred[i, 0] * 105.0\n            py = avg_pred[i, 1] * 68.0\n            results.append({'game_episode': eid, 'pred_x': px, 'pred_y': py})\n\n# 1. ì˜ˆì¸¡ ê²°ê³¼ DataFrame ìƒì„±\npred_df = pd.DataFrame(results)\n\n# 2. ì œì¶œ ì–‘ì‹ ë¡œë“œ\nSUBMISSION_PATH = \"open_track1/sample_submission.csv\"\nif os.path.exists(SUBMISSION_PATH):\n    sub = pd.read_csv(SUBMISSION_PATH)\nelse:\n    sub = pd.read_csv(TEST_META_PATH)\n    # ì»¬ëŸ¼ëª… í†µì¼\n    col_map = {'episode_id': 'game_episode'}\n    sub = sub.rename(columns=col_map)\n    sub = sub[['game_episode']]\n\nprint(f\"Submission Shape: {sub.shape}, Prediction Shape: {pred_df.shape}\")\n\n# 3. [í•µì‹¬] ID ê¸°ì¤€ìœ¼ë¡œ í•©ì¹˜ê¸° (Left Join)\n# ì œì¶œ íŒŒì¼ì˜ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ, ì˜ˆì¸¡ê°’ì„ ì˜†ì— ë¶™ì„\nfinal_sub = pd.merge(sub[['game_episode']], pred_df, on='game_episode', how='left')\n\n# 4. ì»¬ëŸ¼ëª… ë³€ê²½ ë° ê²°ì¸¡ì¹˜ í™•ì¸\nfinal_sub = final_sub.rename(columns={'pred_x': 'end_x', 'pred_y': 'end_y'})\n\n# í˜¹ì‹œ ë§¤ì¹­ ì•ˆ ëœ ê²Œ ìˆëŠ”ì§€ í™•ì¸\nnan_count = final_sub.isnull().sum().sum()\nif nan_count > 0:\n    print(f\"âš ï¸ ê²½ê³ : {nan_count}ê°œì˜ ê²°ì¸¡ì¹˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ID ë¶ˆì¼ì¹˜)\")\n    final_sub = final_sub.fillna(50.0) # ì¤‘ì•™ê°’ìœ¼ë¡œ ë•œë¹µ\n\n# 5. ì €ì¥\nfilename = \"v5_sinusoidal_gelu_submit.csv\"\nfinal_sub.to_csv(filename, index=False)\nprint(f\"âœ… Submission Saved: {filename}\")\nprint(f\"âœ… [V5] Sinusoidal PE + GELU ì ìš© ì™„ë£Œ\")\nprint(final_sub.head())"
  },
  {
   "cell_type": "code",
   "id": "ai7ldiiy9o",
   "source": "# ======================================================\n# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n# ======================================================\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle('[V5 GELU] Training History', fontsize=16, fontweight='bold')\n\n# ëª¨ë¸ë³„ ìƒ‰ìƒ\ncolors = ['#1f77b4', '#ff7f0e', '#2ca02c']\nlabels = [f'Model {i+1} (Seed {h[\"seed\"]})' for i, h in enumerate(all_histories)]\n\n# 1. Training Loss\nax1 = axes[0, 0]\nfor i, history in enumerate(all_histories):\n    epochs = range(1, len(history['train_loss']) + 1)\n    ax1.plot(epochs, history['train_loss'], label=labels[i], color=colors[i], linewidth=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Huber Loss')\nax1.set_title('Training Loss')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 2. Validation Distance\nax2 = axes[0, 1]\nfor i, history in enumerate(all_histories):\n    epochs = range(1, len(history['val_dist']) + 1)\n    ax2.plot(epochs, history['val_dist'], label=labels[i], color=colors[i], linewidth=2)\n    # Best ì§€ì  í‘œì‹œ\n    best_idx = np.argmin(history['val_dist'])\n    best_val = history['val_dist'][best_idx]\n    ax2.scatter(best_idx + 1, best_val, color=colors[i], s=100, zorder=5, marker='*')\n    ax2.annotate(f'{best_val:.2f}m', \n                 xy=(best_idx + 1, best_val), \n                 xytext=(5, 5), \n                 textcoords='offset points',\n                 fontsize=9,\n                 color=colors[i])\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Distance (m)')\nax2.set_title('Validation Distance')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# 3. Learning Rate Schedule\nax3 = axes[1, 0]\nhistory = all_histories[0]  # LRì€ ëª¨ë‘ ë™ì¼í•˜ë¯€ë¡œ ì²« ë²ˆì§¸ ê²ƒë§Œ\nepochs = range(1, len(history['lr']) + 1)\nax3.plot(epochs, history['lr'], color='purple', linewidth=2)\nax3.axvline(x=WARMUP_EPOCHS, color='red', linestyle='--', label=f'Warmup End (Epoch {WARMUP_EPOCHS})')\nax3.set_xlabel('Epoch')\nax3.set_ylabel('Learning Rate')\nax3.set_title('Learning Rate Schedule (Warmup + Cosine Annealing)')\nax3.legend()\nax3.grid(True, alpha=0.3)\nax3.set_yscale('log')\n\n# 4. Best Distances ë¹„êµ\nax4 = axes[1, 1]\nbest_dists = [h['best_dist'] for h in all_histories]\nseeds = [h['seed'] for h in all_histories]\nbars = ax4.bar(range(len(best_dists)), best_dists, color=colors)\nax4.set_xticks(range(len(best_dists)))\nax4.set_xticklabels([f'Model {i+1}\\n(Seed {s})' for i, s in enumerate(seeds)])\nax4.set_ylabel('Best Validation Distance (m)')\nax4.set_title('Best Performance per Model')\nax4.grid(True, alpha=0.3, axis='y')\n\n# ê° ë°” ìœ„ì— ê°’ í‘œì‹œ\nfor i, (bar, dist) in enumerate(zip(bars, best_dists)):\n    height = bar.get_height()\n    ax4.text(bar.get_x() + bar.get_width()/2., height,\n             f'{dist:.4f}m',\n             ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('v5_gelu_training_history.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nğŸ“Š í•™ìŠµ ê³¡ì„  ì €ì¥ ì™„ë£Œ: v5_gelu_training_history.png\")\nprint(f\"\\nğŸ“ˆ ìµœì¢… ê²°ê³¼:\")\nfor i, h in enumerate(all_histories):\n    print(f\"  Model {i+1} (Seed {h['seed']:4d}): {h['best_dist']:.4f}m\")\nprint(f\"  í‰ê· : {np.mean([h['best_dist'] for h in all_histories]):.4f}m\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eec295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daycon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}