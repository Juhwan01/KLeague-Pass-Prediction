{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d00b8",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\nimport random\nimport os\nimport math\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nSEED = 42\nseed_everything(SEED)\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {DEVICE}\")\n\n# ======================================================\n# [V12-Lite] Type/Result Embedding (í¬ì†Œì„± ë¬¸ì œ í•´ê²°)\n# ======================================================\nBATCH_SIZE = 64\nLR_BASE = 1e-3\nWARMUP_EPOCHS = 3\nEPOCHS_BASE = 50\nDROPOUT = 0.2\nMAX_SEQ_LEN = 30\nGRAD_CLIP = 1.0\n\nHIDDEN_DIM = 256\nLSTM_LAYERS = 3\nBIDIRECTIONAL = True\n\n# MDN íŒŒë¼ë¯¸í„°\nNUM_GAUSSIANS = 2\nMIN_SIGMA = 0.01\nMAX_SIGMA = 0.3\nHYBRID_LOSS_WEIGHT = 0.3\n\nprint(f\"[V12-Lite] Type/Result Embedding (ê°œì„  ë²„ì „)\")\nprint(f\"  ì£¼ìš” ë³€ê²½ì‚¬í•­:\")\nprint(f\"  1. Type/Resultë§Œ Embedding (í¬ì†Œì„± ë¬¸ì œ í•´ê²°)\")\nprint(f\"  2. Player â†’ is_same í”¼ì²˜ í™œìš©\")\nprint(f\"  3. Team â†’ is_home í”¼ì²˜ í™œìš©\")\nprint(f\"  4. ì‘ì€ ì´ˆê¸°í™” (std=0.01)\")\nprint(f\"  5. Embedding Dropout ì¶”ê°€\")\nprint(f\"  6. ì˜ˆìƒ: V10 ëŒ€ë¹„ 1-2m ê°œì„ \")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de82db",
   "metadata": {},
   "outputs": [],
   "source": "# ë°ì´í„° ë¡œë“œ (V11ê³¼ ë™ì¼)\nBASE_DIR = \"./open_track1\"\nif not os.path.exists(BASE_DIR): BASE_DIR = \".\"\n\nTRAIN_PATH = os.path.join(BASE_DIR, \"train.csv\")\nTEST_META_PATH = os.path.join(BASE_DIR, \"test.csv\")\nMATCH_PATH = os.path.join(BASE_DIR, \"match_info.csv\")\n\ntrain_df = pd.read_csv(TRAIN_PATH)\nprint(f\"âœ… Train Loaded: {train_df.shape}\")\n\nif os.path.exists(TEST_META_PATH):\n    test_meta = pd.read_csv(TEST_META_PATH)\n    print(f\"â„¹ï¸ Reading {len(test_meta)} test files...\")\n    test_dfs = []\n    for _, row in tqdm(test_meta.iterrows(), total=len(test_meta), desc=\"Loading Test CSVs\"):\n        rel_path = row['path']\n        paths_to_try = [\n            rel_path,\n            os.path.join(BASE_DIR, rel_path.lstrip(\"./\")),\n            os.path.join(BASE_DIR, \"test\", str(row['game_id']), os.path.basename(rel_path))\n        ]\n        for p in paths_to_try:\n            if os.path.exists(p):\n                test_dfs.append(pd.read_csv(p))\n                break\n    if test_dfs:\n        test_df = pd.concat(test_dfs, ignore_index=True)\n        print(f\"âœ… Test Data Merged: {test_df.shape}\")\nelse:\n    raise FileNotFoundError(\"test.csv not found\")\n\nif os.path.exists(MATCH_PATH):\n    match_info = pd.read_csv(MATCH_PATH)\n    match_subset = match_info[['game_id', 'home_team_id', 'venue']]\n    train_df = pd.merge(train_df, match_subset, on='game_id', how='left')\n    test_df = pd.merge(test_df, match_subset, on='game_id', how='left')\n\ndef preprocess(df):\n    if 'home_team_id' in df.columns:\n        df['is_home'] = (df['team_id'] == df['home_team_id']).astype(float)\n    else:\n        df['is_home'] = 0.5\n    if 'end_x' not in df.columns:\n        df['end_x'] = 0.0\n        df['end_y'] = 0.0\n    else:\n        df['end_x'] = df['end_x'].fillna(0.0)\n        df['end_y'] = df['end_y'].fillna(0.0)\n    return df\n\ntrain_df = preprocess(train_df)\ntest_df = preprocess(test_df)\n\nID_COL = 'game_episode' if 'game_episode' in train_df.columns else 'episode_id'\nprint(f\"Data Ready. ID Column: {ID_COL}\")\n\n# ======================================================\n# [Entity Embedding] ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n# ======================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ”§ [Entity Embedding] ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±\")\nprint(\"=\"*70)\n\n# Player ID ë§¤í•‘ (0ì€ padding/unknown)\nall_player_ids = pd.concat([train_df['player_id'], test_df['player_id']]).dropna().unique()\nplayer_to_idx = {pid: i+1 for i, pid in enumerate(sorted(all_player_ids))}\nplayer_to_idx[0] = 0  # Unknown/padding\nNUM_PLAYERS = len(player_to_idx)\n\n# Team ID ë§¤í•‘ (0ì€ unknown)\nall_team_ids = pd.concat([train_df['team_id'], test_df['team_id']]).dropna().unique()\nteam_to_idx = {tid: i+1 for i, tid in enumerate(sorted(all_team_ids))}\nteam_to_idx[0] = 0\nNUM_TEAMS = len(team_to_idx)\n\n# Type Name ë§¤í•‘ (ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ëŠ” unknown)\ntype_to_idx = {t: i for i, t in enumerate(TOP_TYPES)}\ntype_to_idx['unknown'] = len(TOP_TYPES)\nNUM_TYPES = len(TOP_TYPES) + 1\n\n# Result Name ë§¤í•‘ (ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ëŠ” unknown)\nresult_to_idx = {r: i for i, r in enumerate(ALL_RESULTS)}\nresult_to_idx['unknown'] = len(ALL_RESULTS)\nNUM_RESULTS = len(ALL_RESULTS) + 1\n\nprint(f\"âœ… Players: {NUM_PLAYERS} (unique: {len(all_player_ids)})\")\nprint(f\"âœ… Teams: {NUM_TEAMS} (unique: {len(all_team_ids)})\")\nprint(f\"âœ… Types: {NUM_TYPES} (categories: {len(TOP_TYPES)})\")\nprint(f\"âœ… Results: {NUM_RESULTS} (categories: {len(ALL_RESULTS)})\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "features",
   "metadata": {},
   "outputs": [],
   "source": "# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (Entity Embedding ë²„ì „ - Type/Resultë§Œ)\nTOP_TYPES = ['Pass', 'Carry', 'Recovery', 'Interception', 'Duel', 'Tackle', \n             'Throw-In', 'Clearance', 'Intervention', 'Block', 'Pass_Freekick', \n             'Cross', 'Goal Kick', 'Error', 'Shot']\nALL_RESULTS = ['Successful', 'Unsuccessful', 'On Target', 'Yellow_Card', \n               'Blocked', 'Keeper Rush-Out', 'Low Quality Shot', 'Off Target']\n\ndef make_features_embedding(group):\n    \"\"\"\n    [Entity Embedding] Type/Resultë§Œ Embedding, Player/Teamì€ ê¸°ì¡´ í”¼ì²˜ í™œìš©\n    Returns:\n        continuous_features: (seq_len, num_continuous)\n        categorical_indices: (seq_len, 2) - [type_idx, result_idx]\n    \"\"\"\n    n = len(group)\n    sx = group['start_x'].values / 105.0\n    sy = group['start_y'].values / 68.0\n    ex = group['end_x'].values / 105.0\n    ey = group['end_y'].values / 68.0\n    is_home = group['is_home'].values\n    \n    if 'time_seconds' in group.columns:\n        times = group['time_seconds'].values\n        dt = np.zeros(n, dtype=np.float32)\n        dt[1:] = times[1:] - times[:-1]\n        dt = np.maximum(dt, 0.1)\n    else:\n        dt = np.ones(n, dtype=np.float32)\n\n    dx = ex - sx\n    dy = ey - sy\n    dist_meter = np.sqrt((dx*105)**2 + (dy*68)**2)\n    cumsum_dx = np.cumsum(dx) / 105.0\n    cumsum_dy = np.cumsum(dy) / 68.0\n    lag_dist_m = np.roll(dist_meter, 1); lag_dist_m[0] = 0\n    lag_cumsum_dx = np.roll(cumsum_dx, 1); lag_cumsum_dx[0] = 0\n    lag_cumsum_dy = np.roll(cumsum_dy, 1); lag_cumsum_dy[0] = 0\n    lag_dt = np.roll(dt, 1); lag_dt[0] = 1.0\n    lag_speed = lag_dist_m / np.maximum(lag_dt, 0.1)\n    \n    if 'player_id' in group.columns:\n        p_ids = group['player_id'].values\n        is_same = np.zeros(n, dtype=np.float32)\n        is_same[1:] = (p_ids[1:] == p_ids[:-1]).astype(np.float32)\n    else:\n        is_same = np.zeros(n, dtype=np.float32)\n\n    progress = np.arange(n) / max(n-1, 1)\n    is_second_half = (group['period_id'].values > 1).astype(np.float32) if 'period_id' in group.columns else np.zeros(n)\n    \n    GOAL_X, GOAL_Y = 105.0, 34.0\n    sx_real, sy_real = sx * 105.0, sy * 68.0\n    dist_to_goal = np.sqrt((sx_real - GOAL_X)**2 + (sy_real - GOAL_Y)**2) / 105.0\n    angle_to_goal = np.arctan2(GOAL_Y - sy_real, GOAL_X - sx_real)\n    angle_sin, angle_cos = np.sin(angle_to_goal), np.cos(angle_to_goal)\n    dist_to_sideline = np.minimum(sy_real, 68.0 - sy_real) / 68.0\n    dist_to_endline = np.minimum(sx_real, 105.0 - sx_real) / 105.0\n    \n    def get_zone(x_norm):\n        if x_norm < 35.0/105.0: return 0\n        elif x_norm < 70.0/105.0: return 1\n        else: return 2\n    \n    zones = np.array([get_zone(x) for x in sx])\n    zone_onehot = np.zeros((n, 3), dtype=np.float32)\n    for i, z in enumerate(zones): zone_onehot[i, z] = 1.0\n    \n    # âœ… Categorical Indices (Type/Resultë§Œ)\n    type_indices = np.array([type_to_idx.get(t, type_to_idx['unknown']) for t in group['type_name'].values], dtype=np.int64)\n    result_indices = np.array([result_to_idx.get(r, result_to_idx['unknown']) for r in group['result_name'].values], dtype=np.int64)\n\n    continuous_features = []\n    categorical_features = []\n    \n    for i in range(n):\n        # Continuous features\n        scalars = [sx[i], sy[i], lag_cumsum_dx[i], lag_cumsum_dy[i], lag_dist_m[i]/100.0,\n                   lag_speed[i]/10.0, dt[i]/10.0, progress[i], is_home[i], is_same[i],\n                   is_second_half[i], dist_to_goal[i], angle_sin[i], angle_cos[i],\n                   dist_to_sideline[i], dist_to_endline[i]]\n        continuous_vec = np.concatenate([scalars, zone_onehot[i]])  # 16 + 3 = 19\n        continuous_features.append(continuous_vec)\n        \n        # Categorical indices (Type/Resultë§Œ)\n        categorical_features.append([type_indices[i], result_indices[i]])\n        \n        # End state features\n        if i < n - 1:\n            ex_real, ey_real = ex[i] * 105.0, ey[i] * 68.0\n            end_dist_to_goal = np.sqrt((ex_real - GOAL_X)**2 + (ey_real - GOAL_Y)**2) / 105.0\n            end_angle = np.arctan2(GOAL_Y - ey_real, GOAL_X - ex_real)\n            scalars_end = scalars.copy()\n            scalars_end[0:2] = [ex[i], ey[i]]\n            scalars_end[2:4] = [cumsum_dx[i], cumsum_dy[i]]\n            scalars_end[11:16] = [end_dist_to_goal, np.sin(end_angle), np.cos(end_angle),\n                                   min(ey_real, 68.0 - ey_real) / 68.0,\n                                   min(ex_real, 105.0 - ex_real) / 105.0]\n            end_zone_onehot = np.zeros(3, dtype=np.float32)\n            end_zone_onehot[get_zone(ex[i])] = 1.0\n            continuous_vec_end = np.concatenate([scalars_end, end_zone_onehot])\n            continuous_features.append(continuous_vec_end)\n            \n            # Same categorical for end state\n            categorical_features.append([type_indices[i], result_indices[i]])\n            \n    return np.array(continuous_features, dtype=np.float32), np.array(categorical_features, dtype=np.int64)\n\n# Test\ndummy_group = train_df.iloc[:5].copy()\ncont_feat, cat_feat = make_features_embedding(dummy_group)\nCONTINUOUS_DIM = cont_feat.shape[1]\nprint(f\"âœ… Continuous Dim: {CONTINUOUS_DIM}\")\nprint(f\"âœ… Categorical Dim: {cat_feat.shape[1]} (type, result only)\")\nprint(f\"âœ… Sample continuous: {cont_feat[0, :5]}\")\nprint(f\"âœ… Sample categorical: {cat_feat[0]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset",
   "metadata": {},
   "outputs": [],
   "source": "# ë°ì´í„°ì…‹ (Entity Embedding ë²„ì „)\nclass SoccerDatasetEmbedding(Dataset):\n    def __init__(self, df, mode='train'):\n        self.mode = mode\n        self.continuous_seqs, self.categorical_seqs = [], []\n        self.targets, self.team_ids, self.episode_ids = [], [], []\n        \n        for name, group in tqdm(df.groupby(ID_COL, sort=False), desc=f\"Dataset ({mode})\"):\n            if mode == 'train' and len(group) < 2: continue\n            \n            cont_seq, cat_seq = make_features_embedding(group)\n            team_id = group.iloc[0]['team_id']\n            \n            if mode == 'train':\n                last = group.iloc[-1]\n                self.targets.append([last['end_x']/105.0, last['end_y']/68.0])\n                self.continuous_seqs.append(cont_seq)\n                self.categorical_seqs.append(cat_seq)\n                self.team_ids.append(team_id)\n            else:\n                self.continuous_seqs.append(cont_seq)\n                self.categorical_seqs.append(cat_seq)\n                self.team_ids.append(team_id)\n                self.episode_ids.append(str(name))\n\n    def __len__(self): \n        return len(self.continuous_seqs)\n    \n    def __getitem__(self, idx):\n        cont = torch.FloatTensor(self.continuous_seqs[idx])\n        cat = torch.LongTensor(self.categorical_seqs[idx])\n        \n        if len(cont) > MAX_SEQ_LEN:\n            cont = cont[-MAX_SEQ_LEN:]\n            cat = cat[-MAX_SEQ_LEN:]\n        \n        if self.mode == 'train':\n            return cont, cat, torch.FloatTensor(self.targets[idx]), self.team_ids[idx]\n        return cont, cat, self.team_ids[idx], self.episode_ids[idx]\n\ndef collate_fn_embedding(batch):\n    \"\"\"\n    Collate function for embedding dataset\n    Returns:\n        - continuous: padded (batch, max_len, continuous_dim)\n        - categorical: padded (batch, max_len, 4)\n        - targets (train only)\n        - mask\n        - lengths\n        - team_ids\n        - episode_ids (test only)\n    \"\"\"\n    continuous_seqs = [b[0] for b in batch]\n    categorical_seqs = [b[1] for b in batch]\n    lens = torch.LongTensor([len(s) for s in continuous_seqs])\n    \n    # Padding\n    continuous_padded = pad_sequence(continuous_seqs, batch_first=True, padding_value=0)\n    categorical_padded = pad_sequence(categorical_seqs, batch_first=True, padding_value=0)\n    \n    mask = torch.arange(continuous_padded.size(1))[None, :] >= lens[:, None]\n    \n    if isinstance(batch[0][2], str):  # Test mode\n        return continuous_padded, categorical_padded, mask, lens, torch.LongTensor([b[2] for b in batch]), [b[3] for b in batch]\n    else:  # Train mode\n        return continuous_padded, categorical_padded, torch.stack([b[2] for b in batch]), mask, lens, torch.LongTensor([b[3] for b in batch])\n\nfull_dataset = SoccerDatasetEmbedding(train_df, mode='train')\ntest_dataset = SoccerDatasetEmbedding(test_df, mode='test')\nprint(f\"âœ… Dataset: {len(full_dataset)} train, {len(test_dataset)} test\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": "# ======================================================\n# [V12-Entity Embedding-Lite] Type/Resultë§Œ Embedding\n# ======================================================\n\n# Embedding Dimensions (Type/Resultë§Œ)\nTYPE_EMBED_DIM = 12\nRESULT_EMBED_DIM = 6\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.goal_attn = nn.Sequential(nn.Linear(3, 16), nn.ReLU(), nn.Linear(16, 1))\n        self.zone_attn = nn.Sequential(nn.Linear(3, 8), nn.ReLU(), nn.Linear(8, 1))\n        self.pos_attn = nn.Sequential(nn.Linear(2, 8), nn.ReLU(), nn.Linear(8, 1))\n        self.fusion = nn.Linear(3, 1)\n    def forward(self, x):\n        return self.fusion(torch.cat([self.pos_attn(x[..., 0:2]), \n                                       self.goal_attn(x[..., 11:14]), \n                                       self.zone_attn(x[..., 16:19])], dim=-1))\n\nclass TemporalAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.pos_encoding = nn.Parameter(torch.randn(1, 100, hidden_dim) * 0.02)\n        self.temporal_attn = nn.Sequential(nn.Linear(hidden_dim, hidden_dim // 2), \n                                           nn.Tanh(), nn.Dropout(0.1), \n                                           nn.Linear(hidden_dim // 2, 1))\n    def forward(self, lstm_out):\n        return self.temporal_attn(lstm_out + self.pos_encoding[:, :lstm_out.size(1), :])\n\nclass SpatialTemporalFusion(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.spatial_weight = nn.Parameter(torch.tensor(0.5))\n        self.temporal_weight = nn.Parameter(torch.tensor(0.5))\n        self.combine = nn.Sequential(nn.Linear(2, 8), nn.ReLU(), nn.Linear(8, 1), nn.Sigmoid())\n    def forward(self, s, t):\n        return (torch.sigmoid(self.spatial_weight) * s + \n                torch.sigmoid(self.temporal_weight) * t) * self.combine(torch.cat([s, t], -1))\n\nclass MDNPredictorWithEmbeddingLite(nn.Module):\n    \"\"\"\n    [V12-Lite] Type/Resultë§Œ Embedding (Player/Team ì œê±°)\n    - í¬ì†Œì„± ë¬¸ì œ í•´ê²°\n    - ì•ˆì •ì ì¸ í•™ìŠµ\n    \"\"\"\n    def __init__(self, continuous_dim, hidden_dim, num_layers, dropout, num_gaussians=2, bidirectional=True):\n        super().__init__()\n        self.num_gaussians = num_gaussians\n        \n        # âœ… Type/Result Embeddingë§Œ ì‚¬ìš©\n        self.type_embedding = nn.Embedding(NUM_TYPES, TYPE_EMBED_DIM)\n        self.result_embedding = nn.Embedding(NUM_RESULTS, RESULT_EMBED_DIM)\n        \n        # âœ… ì‘ì€ ê°’ìœ¼ë¡œ ì´ˆê¸°í™” (Xavier ëŒ€ì‹ )\n        nn.init.normal_(self.type_embedding.weight, mean=0, std=0.01)\n        nn.init.normal_(self.result_embedding.weight, mean=0, std=0.01)\n        \n        # âœ… Embedding Dropout ì¶”ê°€\n        self.embedding_dropout = nn.Dropout(dropout * 0.5)\n        \n        # Total input dimension\n        total_input_dim = continuous_dim + TYPE_EMBED_DIM + RESULT_EMBED_DIM\n        \n        self.lstm = nn.LSTM(total_input_dim, hidden_dim, num_layers, batch_first=True,\n                           dropout=dropout if num_layers > 1 else 0, bidirectional=bidirectional)\n        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n        \n        self.spatial_attn = SpatialAttention(lstm_output_dim)\n        self.temporal_attn = TemporalAttention(lstm_output_dim)\n        self.fusion = SpatialTemporalFusion(lstm_output_dim)\n        \n        # MDN Heads\n        self.pi_head = nn.Sequential(\n            nn.Linear(lstm_output_dim, lstm_output_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(lstm_output_dim // 2, num_gaussians)\n        )\n        self.mu_head = nn.Sequential(\n            nn.Linear(lstm_output_dim, lstm_output_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(lstm_output_dim // 2, num_gaussians * 2)\n        )\n        self.sigma_head = nn.Sequential(\n            nn.Linear(lstm_output_dim, lstm_output_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(lstm_output_dim // 2, num_gaussians * 2)\n        )\n        \n        # Mu ì´ˆê¸°í™”\n        nn.init.constant_(self.mu_head[-1].bias, 0.5)\n        nn.init.xavier_uniform_(self.mu_head[-1].weight, gain=0.1)\n\n    def forward(self, continuous, categorical, mask=None, lengths=None):\n        \"\"\"\n        Args:\n            continuous: (batch, seq_len, continuous_dim)\n            categorical: (batch, seq_len, 2) - [type_idx, result_idx]\n        \"\"\"\n        batch_size, seq_len = continuous.size(0), continuous.size(1)\n        \n        # âœ… Embedding lookup (Type/Resultë§Œ)\n        type_idx = categorical[:, :, 0]\n        result_idx = categorical[:, :, 1]\n        \n        type_emb = self.type_embedding(type_idx)\n        result_emb = self.result_embedding(result_idx)\n        \n        # âœ… Embedding Dropout ì ìš©\n        type_emb = self.embedding_dropout(type_emb)\n        result_emb = self.embedding_dropout(result_emb)\n        \n        # âœ… Concatenate continuous + embeddings\n        x = torch.cat([continuous, type_emb, result_emb], dim=-1)\n        \n        # LSTM + Attention\n        if lengths is not None:\n            packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n            lstm_out, _ = self.lstm(packed)\n            lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True, total_length=seq_len)\n        else:\n            lstm_out, _ = self.lstm(x)\n        \n        fused_attn = self.fusion(self.spatial_attn(continuous), self.temporal_attn(lstm_out))\n        if mask is not None:\n            fused_attn = fused_attn.masked_fill(mask.unsqueeze(-1), float('-inf'))\n        final_attn = torch.softmax(fused_attn, dim=1)\n        context = torch.sum(lstm_out * final_attn, dim=1)\n        \n        # MDN ì¶œë ¥\n        pi = torch.softmax(self.pi_head(context), dim=1)\n        mu = torch.sigmoid(self.mu_head(context)).view(batch_size, self.num_gaussians, 2)\n        \n        sigma_raw = self.sigma_head(context).view(batch_size, self.num_gaussians, 2)\n        sigma = torch.sigmoid(sigma_raw) * (MAX_SIGMA - MIN_SIGMA) + MIN_SIGMA\n        \n        return pi, mu, sigma\n\n\ndef hybrid_mdn_loss(pi, mu, sigma, target, mse_weight=HYBRID_LOSS_WEIGHT):\n    \"\"\"Hybrid Loss: NLL + MSE\"\"\"\n    batch_size = target.size(0)\n    target_expanded = target.unsqueeze(1).expand_as(mu)\n    \n    # NLL Loss\n    diff = target_expanded - mu\n    log_prob_components = (\n        -0.5 * math.log(2 * math.pi) - torch.log(sigma) - 0.5 * (diff / sigma) ** 2\n    )\n    log_prob = log_prob_components.sum(dim=2)\n    weighted_log_prob = log_prob + torch.log(pi + 1e-8)\n    nll_loss = -torch.logsumexp(weighted_log_prob, dim=1).mean()\n    \n    # MSE Loss\n    pred_mean = (pi.unsqueeze(-1) * mu).sum(dim=1)\n    mse_loss = nn.functional.mse_loss(pred_mean, target)\n    \n    total_loss = nll_loss + mse_weight * mse_loss\n    \n    return total_loss, nll_loss, mse_loss\n\n\ndef mdn_predict_improved(pi, mu, sigma, strategy='mean'):\n    \"\"\"ê°œì„ ëœ ì˜ˆì¸¡\"\"\"\n    if strategy == 'mode':\n        max_idx = torch.argmax(pi, dim=1)\n        pred = mu[torch.arange(len(mu)), max_idx]\n    elif strategy == 'mean':\n        pred = (pi.unsqueeze(-1) * mu).sum(dim=1)\n    else:\n        raise ValueError(f\"Unknown strategy: {strategy}\")\n    \n    return torch.clamp(pred, 0.0, 1.0)\n\n\nmodel = MDNPredictorWithEmbeddingLite(CONTINUOUS_DIM, HIDDEN_DIM, LSTM_LAYERS, DROPOUT, NUM_GAUSSIANS, BIDIRECTIONAL).to(DEVICE)\n\nprint(\"=\"*70)\nprint(\"âœ… [V12-Lite] Type/Result Embedding ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ\")\nprint(\"=\"*70)\nprint(f\"Embedding êµ¬ì„±:\")\nprint(f\"  - Type: {NUM_TYPES} â†’ {TYPE_EMBED_DIM}dim\")\nprint(f\"  - Result: {NUM_RESULTS} â†’ {RESULT_EMBED_DIM}dim\")\nprint(f\"  - Player: is_same í”¼ì²˜ë¡œ ëŒ€ì²´ (Embedding ì œê±°)\")\nprint(f\"  - Team: is_home í”¼ì²˜ë¡œ ëŒ€ì²´ (Embedding ì œê±°)\")\nprint(f\"  - Total Embedding: {TYPE_EMBED_DIM + RESULT_EMBED_DIM}dim\")\nprint(f\"  - Continuous: {CONTINUOUS_DIM}dim\")\nprint(f\"  - LSTM Input: {CONTINUOUS_DIM + TYPE_EMBED_DIM + RESULT_EMBED_DIM}dim\")\nprint(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"\\nê°œì„ ì‚¬í•­:\")\nprint(f\"  1. í¬ì†Œì„± ë¬¸ì œ í•´ê²° (Player/Team ì œê±°)\")\nprint(f\"  2. ì‘ì€ ì´ˆê¸°í™” (std=0.01)\")\nprint(f\"  3. Embedding Dropout ì¶”ê°€\")\nprint(f\"  4. ì•ˆì •ì ì¸ í•™ìŠµ ë³´ì¥\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": "# ======================================================\n# [V12-Lite] í•™ìŠµ\n# ======================================================\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nSEEDS = [42, 2024, 777]\nPRED_STRATEGY = 'mean'\n\nprint(f\"\\n{'='*70}\")\nprint(f\"ğŸš€ [V12-Lite] Type/Result Embedding í•™ìŠµ ì‹œì‘\")\nprint(f\"{'='*70}\\n\")\n\nall_histories = []\n\nfor i, seed in enumerate(SEEDS):\n    print(f\"\\n{'='*70}\")\n    print(f\"ğŸ“¦ [ëª¨ë¸ {i+1}/3] Seed {seed}\")\n    print(f\"{'='*70}\")\n    seed_everything(seed)\n    \n    train_idx, val_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=seed)\n    train_loader = DataLoader(torch.utils.data.Subset(full_dataset, train_idx), \n                              batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_embedding)\n    val_loader = DataLoader(torch.utils.data.Subset(full_dataset, val_idx), \n                           batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_embedding)\n    \n    model = MDNPredictorWithEmbeddingLite(CONTINUOUS_DIM, HIDDEN_DIM, LSTM_LAYERS, DROPOUT, \n                                          NUM_GAUSSIANS, BIDIRECTIONAL).to(DEVICE)\n    optimizer = optim.AdamW(model.parameters(), lr=LR_BASE, weight_decay=1e-4)\n    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS_BASE - WARMUP_EPOCHS, eta_min=1e-6)\n    \n    best_dist = float('inf')\n    history = {'train_loss': [], 'train_nll': [], 'train_mse': [], 'val_dist': [], 'lr': []}\n    \n    for epoch in range(EPOCHS_BASE):\n        # Warmup\n        if epoch < WARMUP_EPOCHS:\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = LR_BASE * (epoch + 1) / WARMUP_EPOCHS\n        \n        # Training\n        model.train()\n        train_loss, train_nll, train_mse = 0.0, 0.0, 0.0\n        for continuous, categorical, targets, mask, lens, _ in train_loader:\n            continuous = continuous.to(DEVICE)\n            categorical = categorical.to(DEVICE)\n            targets = targets.to(DEVICE)\n            mask = mask.to(DEVICE)\n            lens = lens.to(DEVICE)\n            \n            optimizer.zero_grad()\n            pi, mu, sigma = model(continuous, categorical, mask, lens)\n            loss, nll, mse = hybrid_mdn_loss(pi, mu, sigma, targets)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n            optimizer.step()\n            \n            train_loss += loss.item()\n            train_nll += nll.item()\n            train_mse += mse.item()\n        \n        # Validation\n        model.eval()\n        dists = []\n        with torch.no_grad():\n            for continuous, categorical, targets, mask, lens, _ in val_loader:\n                continuous = continuous.to(DEVICE)\n                categorical = categorical.to(DEVICE)\n                targets = targets.to(DEVICE)\n                mask = mask.to(DEVICE)\n                lens = lens.to(DEVICE)\n                \n                pi, mu, sigma = model(continuous, categorical, mask, lens)\n                pred = mdn_predict_improved(pi, mu, sigma, strategy=PRED_STRATEGY)\n                \n                p_real = pred.cpu().numpy() * np.array([105.0, 68.0])\n                t_real = targets.cpu().numpy() * np.array([105.0, 68.0])\n                dists.extend(np.sqrt(np.sum((p_real - t_real)**2, axis=1)))\n        \n        avg_dist = np.mean(dists)\n        avg_loss = train_loss / len(train_loader)\n        avg_nll = train_nll / len(train_loader)\n        avg_mse = train_mse / len(train_loader)\n        \n        if epoch >= WARMUP_EPOCHS:\n            scheduler.step()\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        history['train_loss'].append(avg_loss)\n        history['train_nll'].append(avg_nll)\n        history['train_mse'].append(avg_mse)\n        history['val_dist'].append(avg_dist)\n        history['lr'].append(current_lr)\n        \n        if (epoch + 1) % 5 == 0 or epoch < 5:\n            print(f\"  [Epoch {epoch+1:2d}/{EPOCHS_BASE}] \"\n                  f\"Loss: {avg_loss:.4f} (NLL: {avg_nll:.4f}, MSE: {avg_mse:.4f}) | \"\n                  f\"Val: {avg_dist:.4f}m | LR: {current_lr:.6f}\")\n        \n        if avg_dist < best_dist:\n            best_dist = avg_dist\n            torch.save(model.state_dict(), f'v12_lite_mdn_{i}.pth')\n    \n    history['best_dist'] = best_dist\n    history['seed'] = seed\n    all_histories.append(history)\n    print(f\"  âœ… Best: {best_dist:.4f}m\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"âœ… í•™ìŠµ ì™„ë£Œ!\")\nprint(f\"{'='*70}\")\nprint(f\"í‰ê·  Best: {np.mean([h['best_dist'] for h in all_histories]):.4f}m\")\nprint(f\"ìµœê³  ì„±ëŠ¥: {min([h['best_dist'] for h in all_histories]):.4f}m\")\nprint(f\"{'='*70}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference",
   "metadata": {},
   "outputs": [],
   "source": "# ì¶”ë¡ \ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_embedding)\n\nmodels = []\nfor i in range(3):\n    model = MDNPredictorWithEmbeddingLite(CONTINUOUS_DIM, HIDDEN_DIM, LSTM_LAYERS, DROPOUT, \n                                          NUM_GAUSSIANS, BIDIRECTIONAL).to(DEVICE)\n    model.load_state_dict(torch.load(f'v12_lite_mdn_{i}.pth'))\n    model.eval()\n    models.append(model)\n\nresults = []\nwith torch.no_grad():\n    for continuous, categorical, mask, lens, team_ids, episode_ids in tqdm(test_loader, desc=\"Inference\"):\n        continuous = continuous.to(DEVICE)\n        categorical = categorical.to(DEVICE)\n        mask = mask.to(DEVICE)\n        lens = lens.to(DEVICE)\n        \n        preds = []\n        for model in models:\n            pi, mu, sigma = model(continuous, categorical, mask, lens)\n            pred = mdn_predict_improved(pi, mu, sigma, strategy=PRED_STRATEGY)\n            preds.append(pred.cpu().numpy())\n        \n        avg_pred = np.mean(preds, axis=0)\n        for i, eid in enumerate(episode_ids):\n            results.append({'game_episode': eid, 'pred_x': avg_pred[i, 0] * 105.0, \n                          'pred_y': avg_pred[i, 1] * 68.0})\n\npred_df = pd.DataFrame(results)\nSUBMISSION_PATH = \"open_track1/sample_submission.csv\"\nif os.path.exists(SUBMISSION_PATH):\n    sub = pd.read_csv(SUBMISSION_PATH)\nelse:\n    sub = pd.read_csv(TEST_META_PATH).rename(columns={'episode_id': 'game_episode'})[['game_episode']]\n\nfinal_sub = pd.merge(sub[['game_episode']], pred_df, on='game_episode', how='left')\nfinal_sub = final_sub.rename(columns={'pred_x': 'end_x', 'pred_y': 'end_y'})\nif final_sub.isnull().sum().sum() > 0:\n    final_sub = final_sub.fillna(50.0)\n\nfilename = \"v12_lite_embedding_submit.csv\"\nfinal_sub.to_csv(filename, index=False)\n\nprint(f\"\\nâœ… ì œì¶œ íŒŒì¼: {filename}\")\nprint(final_sub.head())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz",
   "metadata": {},
   "outputs": [],
   "source": "# ì‹œê°í™”\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = 'Malgun Gothic'\nplt.rcParams['axes.unicode_minus'] = False\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle('[V12-Lite] Type/Result Embedding í•™ìŠµ ê³¡ì„ ', fontsize=16, fontweight='bold')\n\ncolors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n\n# 1. Hybrid Loss\nax1 = axes[0, 0]\nfor i, h in enumerate(all_histories):\n    epochs = range(1, len(h['train_loss']) + 1)\n    ax1.plot(epochs, h['train_loss'], label=f\"Model {i+1}\", color=colors[i], linewidth=2)\nax1.set_title('Training Loss (Hybrid: NLL + MSE)')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 2. Validation Distance\nax2 = axes[0, 1]\nfor i, h in enumerate(all_histories):\n    epochs = range(1, len(h['val_dist']) + 1)\n    ax2.plot(epochs, h['val_dist'], label=f\"Model {i+1}\", color=colors[i], linewidth=2)\n    best_idx = np.argmin(h['val_dist'])\n    ax2.scatter(best_idx + 1, h['val_dist'][best_idx], color=colors[i], s=100, marker='*')\nax2.set_title(f'Validation Distance ({PRED_STRATEGY})')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Distance (m)')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# 3. NLL vs MSE\nax3 = axes[1, 0]\nh = all_histories[0]\nepochs = range(1, len(h['train_nll']) + 1)\nax3.plot(epochs, h['train_nll'], label='NLL', color='red', linewidth=2)\nax3_twin = ax3.twinx()\nax3_twin.plot(epochs, h['train_mse'], label='MSE', color='blue', linewidth=2)\nax3.set_xlabel('Epoch')\nax3.set_ylabel('NLL', color='red')\nax3_twin.set_ylabel('MSE', color='blue')\nax3.set_title('Loss Components (Model 1)')\nax3.grid(True, alpha=0.3)\n\n# 4. Best Distances\nax4 = axes[1, 1]\nbest_dists = [h['best_dist'] for h in all_histories]\nbars = ax4.bar(range(len(best_dists)), best_dists, color=colors)\nax4.set_xticks(range(len(best_dists)))\nax4.set_xticklabels([f'Model {i+1}' for i in range(len(best_dists))])\nax4.set_ylabel('Best Distance (m)')\nax4.set_title('Best Performance')\nfor bar, dist in zip(bars, best_dists):\n    ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height(), \n             f'{dist:.4f}m', ha='center', va='bottom', fontweight='bold')\nax4.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig('v12_lite_embedding_history.png', dpi=150)\nplt.show()\n\nprint(f\"\\nìµœì¢… ê²°ê³¼:\")\nfor i, h in enumerate(all_histories):\n    print(f\"  Model {i+1}: {h['best_dist']:.4f}m\")\nprint(f\"  í‰ê· : {np.mean(best_dists):.4f}m\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"âœ… [V12-Lite] ì™„ë£Œ!\")\nprint(f\"{'='*70}\")\nprint(f\"ì£¼ìš” ë³€ê²½ì‚¬í•­:\")\nprint(f\"  1. Type/Resultë§Œ Entity Embedding (í¬ì†Œì„± ë¬¸ì œ í•´ê²°)\")\nprint(f\"  2. Player â†’ is_same í”¼ì²˜ í™œìš©\")\nprint(f\"  3. Team â†’ is_home í”¼ì²˜ í™œìš©\")\nprint(f\"  4. ì‘ì€ ì´ˆê¸°í™” (std=0.01)\")\nprint(f\"  5. Embedding Dropout (ì•ˆì •í™”)\")\nprint(f\"  6. One-Hot ëŒ€ë¹„ ì°¨ì› ì¶•ì†Œ (25â†’18 embedding)\")\nprint(f\"  7. ê´€ê³„ í•™ìŠµ: Passâ†”Cross, Carryâ†”Duel ë“±\")\nprint(f\"{'='*70}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daycon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}