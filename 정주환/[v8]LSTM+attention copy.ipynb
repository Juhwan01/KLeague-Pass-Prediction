{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d00b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# ======================================================\n",
    "# [V8] Bi-LSTM + Attention ë‹¨ì¼ ëª¨ë¸\n",
    "# ======================================================\n",
    "BATCH_SIZE = 64\n",
    "LR_BASE = 1e-3\n",
    "WARMUP_EPOCHS = 3\n",
    "EPOCHS_BASE = 40\n",
    "DROPOUT = 0.2\n",
    "MAX_SEQ_LEN = 30\n",
    "GRAD_CLIP = 1.0\n",
    "\n",
    "# LSTM íŒŒë¼ë¯¸í„°\n",
    "HIDDEN_DIM = 256       # LSTM hidden dimension\n",
    "LSTM_LAYERS = 2        # LSTM ë ˆì´ì–´ ìˆ˜\n",
    "BIDIRECTIONAL = True   # ì–‘ë°©í–¥ LSTM\n",
    "\n",
    "print(f\"[V8] Bi-LSTM + Attention ë‹¨ì¼ ëª¨ë¸\")\n",
    "print(f\"  - Hidden Dim: {HIDDEN_DIM}\")\n",
    "print(f\"  - Layers: {LSTM_LAYERS}\")\n",
    "print(f\"  - Bidirectional: {BIDIRECTIONAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. ê²½ë¡œ ì„¤ì • ë° Train ë¡œë“œ\n",
    "# ======================================================\n",
    "BASE_DIR = \"./open_track1\" # ë°ì´í„°ê°€ ìžˆëŠ” í´ë”ëª… (í™˜ê²½ì— ë§žê²Œ ìˆ˜ì •)\n",
    "if not os.path.exists(BASE_DIR): BASE_DIR = \".\" # í´ë” ì—†ìœ¼ë©´ í˜„ìž¬ ê²½ë¡œ\n",
    "\n",
    "TRAIN_PATH = os.path.join(BASE_DIR, \"train.csv\")\n",
    "TEST_META_PATH = os.path.join(BASE_DIR, \"test.csv\") # ì‚¬ìš©ìžë‹˜ì´ ì˜¬ë¦¬ì‹  íŒŒì¼ëª…\n",
    "MATCH_PATH = os.path.join(BASE_DIR, \"match_info.csv\")\n",
    "\n",
    "# 1. Train ë¡œë“œ\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "print(f\"âœ… Train Loaded: {train_df.shape}\")\n",
    "\n",
    "# 2. Test í†µí•© ë¡œë“œ (í•µì‹¬ ìˆ˜ì • ì‚¬í•­)\n",
    "if os.path.exists(TEST_META_PATH):\n",
    "    test_meta = pd.read_csv(TEST_META_PATH)\n",
    "    print(f\"â„¹ï¸ Reading {len(test_meta)} test files...\")\n",
    "    \n",
    "    test_dfs = []\n",
    "    # tqdmìœ¼ë¡œ ì§„í–‰ìƒí™© í™•ì¸\n",
    "    for _, row in tqdm(test_meta.iterrows(), total=len(test_meta), desc=\"Loading Test CSVs\"):\n",
    "        rel_path = row['path']\n",
    "        \n",
    "        # ê²½ë¡œ ë³´ì •: 1) ê·¸ëŒ€ë¡œ, 2) í´ë” ë¶™ì—¬ì„œ, 3) íŒŒì¼ëª…ë§Œìœ¼ë¡œ ì°¾ê¸°\n",
    "        paths_to_try = [\n",
    "            rel_path,\n",
    "            os.path.join(BASE_DIR, rel_path.lstrip(\"./\")),\n",
    "            os.path.join(BASE_DIR, \"test\", str(row['game_id']), os.path.basename(rel_path))\n",
    "        ]\n",
    "        \n",
    "        for p in paths_to_try:\n",
    "            if os.path.exists(p):\n",
    "                test_dfs.append(pd.read_csv(p))\n",
    "                break\n",
    "        \n",
    "    if test_dfs:\n",
    "        test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "        print(f\"âœ… Test Data Merged: {test_df.shape}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Test íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"test.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 3. Match Info ë³‘í•©\n",
    "if os.path.exists(MATCH_PATH):\n",
    "    match_info = pd.read_csv(MATCH_PATH)\n",
    "    match_subset = match_info[['game_id', 'home_team_id', 'venue']]\n",
    "    train_df = pd.merge(train_df, match_subset, on='game_id', how='left')\n",
    "    test_df = pd.merge(test_df, match_subset, on='game_id', how='left')\n",
    "\n",
    "# 4. ì „ì²˜ë¦¬ (ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°)\n",
    "def preprocess(df):\n",
    "    if 'home_team_id' in df.columns:\n",
    "        df['is_home'] = (df['team_id'] == df['home_team_id']).astype(float)\n",
    "    else:\n",
    "        df['is_home'] = 0.5\n",
    "        \n",
    "    # V2 ë¡œì§ ìž‘ë™ì„ ìœ„í•´ end_xê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€\n",
    "    if 'end_x' not in df.columns:\n",
    "        df['end_x'] = 0.0\n",
    "        df['end_y'] = 0.0\n",
    "    else:\n",
    "        df['end_x'] = df['end_x'].fillna(0.0)\n",
    "        df['end_y'] = df['end_y'].fillna(0.0)\n",
    "    return df\n",
    "\n",
    "train_df = preprocess(train_df)\n",
    "test_df = preprocess(test_df)\n",
    "\n",
    "ID_COL = 'game_episode' if 'game_episode' in train_df.columns else 'episode_id'\n",
    "print(f\"Data Ready. ID Column: {ID_COL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5662aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ë²”ì£¼í˜• ë³€ìˆ˜ ì •ì˜ (Train ë°ì´í„° ë¶„ì„ ê¸°ë°˜)\n",
    "TOP_TYPES = ['Pass', 'Carry', 'Recovery', 'Interception', 'Duel', 'Tackle', \n",
    "             'Throw-In', 'Clearance', 'Intervention', 'Block', 'Pass_Freekick', \n",
    "             'Cross', 'Goal Kick', 'Error', 'Shot']\n",
    "ALL_RESULTS = ['Successful', 'Unsuccessful', 'On Target', 'Yellow_Card', \n",
    "               'Blocked', 'Keeper Rush-Out', 'Low Quality Shot', 'Off Target']\n",
    "\n",
    "def make_features(group):\n",
    "    n = len(group)\n",
    "    \n",
    "    # --- 1. ê¸°ë³¸ ì¢Œí‘œ & Context ---\n",
    "    sx = group['start_x'].values / 105.0\n",
    "    sy = group['start_y'].values / 68.0\n",
    "    ex = group['end_x'].values / 105.0\n",
    "    ey = group['end_y'].values / 68.0\n",
    "    is_home = group['is_home'].values\n",
    "    \n",
    "    # --- 2. ì‹œê°„ & ì†ë„ ---\n",
    "    if 'time_seconds' in group.columns:\n",
    "        times = group['time_seconds'].values\n",
    "        dt = np.zeros(n, dtype=np.float32)\n",
    "        dt[1:] = times[1:] - times[:-1]\n",
    "        dt = np.maximum(dt, 0.1)\n",
    "    else:\n",
    "        dt = np.ones(n, dtype=np.float32)\n",
    "\n",
    "    # --- 3. ì´ë™ëŸ‰ & ê±°ë¦¬ ê³„ì‚° ---\n",
    "    dx = ex - sx\n",
    "    dy = ey - sy\n",
    "    dist_meter = np.sqrt((dx*105)**2 + (dy*68)**2)\n",
    "    \n",
    "    # --- 4. í†µê³„ëŸ‰ (Lagged Features) ---\n",
    "    cumsum_dx = np.cumsum(dx) / 105.0\n",
    "    cumsum_dy = np.cumsum(dy) / 68.0\n",
    "    \n",
    "    lag_dist_m = np.roll(dist_meter, 1); lag_dist_m[0] = 0\n",
    "    lag_cumsum_dx = np.roll(cumsum_dx, 1); lag_cumsum_dx[0] = 0\n",
    "    lag_cumsum_dy = np.roll(cumsum_dy, 1); lag_cumsum_dy[0] = 0\n",
    "    \n",
    "    lag_dt = np.roll(dt, 1); lag_dt[0] = 1.0\n",
    "    lag_speed = lag_dist_m / np.maximum(lag_dt, 0.1)\n",
    "    \n",
    "    # --- 5. ì„ ìˆ˜ ì •ë³´ ---\n",
    "    if 'player_id' in group.columns:\n",
    "        p_ids = group['player_id'].values\n",
    "        is_same = np.zeros(n, dtype=np.float32)\n",
    "        is_same[1:] = (p_ids[1:] == p_ids[:-1]).astype(np.float32)\n",
    "    else:\n",
    "        is_same = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "    # --- 6. ì§„í–‰ë¥  & ê¸°íƒ€ ---\n",
    "    progress = np.arange(n) / max(n-1, 1)\n",
    "    is_second_half = (group['period_id'].values > 1).astype(np.float32) if 'period_id' in group.columns else np.zeros(n)\n",
    "    \n",
    "    # ======================================================\n",
    "    # [V6 Phase 1] ê³µê°„ í”¼ì²˜ ì¶”ê°€\n",
    "    # ======================================================\n",
    "    # 6-1. ê³¨ëŒ€ê¹Œì§€ ê±°ë¦¬ (ìƒëŒ€ ê³¨ëŒ€ = ì˜¤ë¥¸ìª½)\n",
    "    GOAL_X = 105.0\n",
    "    GOAL_Y = 34.0  # ê³¨ëŒ€ ì¤‘ì•™\n",
    "    \n",
    "    sx_real = sx * 105.0\n",
    "    sy_real = sy * 68.0\n",
    "    dist_to_goal = np.sqrt((sx_real - GOAL_X)**2 + (sy_real - GOAL_Y)**2) / 105.0  # ì •ê·œí™”\n",
    "    \n",
    "    # 6-2. ê³¨ëŒ€ ë°©í–¥ ê°ë„ (sin/cos)\n",
    "    dx_to_goal = GOAL_X - sx_real\n",
    "    dy_to_goal = GOAL_Y - sy_real\n",
    "    angle_to_goal = np.arctan2(dy_to_goal, dx_to_goal)\n",
    "    angle_sin = np.sin(angle_to_goal)\n",
    "    angle_cos = np.cos(angle_to_goal)\n",
    "    \n",
    "    # 6-3. ê²½ê³„ì„ ê¹Œì§€ ê±°ë¦¬\n",
    "    dist_to_sideline = np.minimum(sy_real, 68.0 - sy_real) / 68.0  # ìœ„/ì•„ëž˜ í„°ì¹˜ë¼ì¸\n",
    "    dist_to_endline = np.minimum(sx_real, 105.0 - sx_real) / 105.0  # ì¢Œ/ìš° ê³¨ë¼ì¸\n",
    "    \n",
    "    # 6-4. í•„ë“œ êµ¬ì—­ (3êµ¬ì—­: ìˆ˜ë¹„/ì¤‘ì•™/ê³µê²©)\n",
    "    def get_zone(x_norm):\n",
    "        if x_norm < 35.0/105.0:\n",
    "            return 0  # ìˆ˜ë¹„ êµ¬ì—­\n",
    "        elif x_norm < 70.0/105.0:\n",
    "            return 1  # ì¤‘ì•™ êµ¬ì—­\n",
    "        else:\n",
    "            return 2  # ê³µê²© êµ¬ì—­\n",
    "    \n",
    "    zones = np.array([get_zone(x) for x in sx])\n",
    "    zone_onehot = np.zeros((n, 3), dtype=np.float32)\n",
    "    for i, z in enumerate(zones):\n",
    "        zone_onehot[i, z] = 1.0\n",
    "    \n",
    "    # --- 7. ë²”ì£¼í˜• ë³€ìˆ˜ One-hot Encoding ---\n",
    "    types_onehot = np.zeros((n, len(TOP_TYPES) + 1), dtype=np.float32)\n",
    "    curr_types = group['type_name'].values\n",
    "    for i, t in enumerate(curr_types):\n",
    "        if t in TOP_TYPES:\n",
    "            idx = TOP_TYPES.index(t)\n",
    "            types_onehot[i, idx] = 1.0\n",
    "        else:\n",
    "            types_onehot[i, -1] = 1.0\n",
    "\n",
    "    results_onehot = np.zeros((n, len(ALL_RESULTS) + 1), dtype=np.float32)\n",
    "    curr_results = group['result_name'].values\n",
    "    for i, r in enumerate(curr_results):\n",
    "        if r in ALL_RESULTS:\n",
    "            idx = ALL_RESULTS.index(r)\n",
    "            results_onehot[i, idx] = 1.0\n",
    "        else:\n",
    "            results_onehot[i, -1] = 1.0\n",
    "\n",
    "    features = []\n",
    "    \n",
    "    # --- 8. ì‹œí€€ìŠ¤ ê²°í•© ---\n",
    "    for i in range(n):\n",
    "        # ìŠ¤ì¹¼ë¼ í”¼ì²˜ (11ê°œ â†’ 16ê°œ)\n",
    "        scalars = [\n",
    "            sx[i], sy[i],                    # 0-1: ìœ„ì¹˜\n",
    "            lag_cumsum_dx[i], lag_cumsum_dy[i],  # 2-3: ëˆ„ì  íë¦„\n",
    "            lag_dist_m[i] / 100.0,           # 4: ì§ì „ ê±°ë¦¬\n",
    "            lag_speed[i] / 10.0,             # 5: ì§ì „ ì†ë„\n",
    "            dt[i] / 10.0,                    # 6: ì‹œê°„ ê²½ê³¼\n",
    "            progress[i],                     # 7: ì§„í–‰ë¥ \n",
    "            is_home[i],                      # 8: í™ˆ ì—¬ë¶€\n",
    "            is_same[i],                      # 9: ì„ ìˆ˜ ì—°ì†ì„±\n",
    "            is_second_half[i],               # 10: í›„ë°˜ì „\n",
    "            # [V6 Phase 1] ìƒˆë¡œìš´ ê³µê°„ í”¼ì²˜ (11-15)\n",
    "            dist_to_goal[i],                 # 11: ê³¨ëŒ€ ê±°ë¦¬\n",
    "            angle_sin[i],                    # 12: ê³¨ëŒ€ ë°©í–¥ sin\n",
    "            angle_cos[i],                    # 13: ê³¨ëŒ€ ë°©í–¥ cos\n",
    "            dist_to_sideline[i],             # 14: í„°ì¹˜ë¼ì¸ ê±°ë¦¬\n",
    "            dist_to_endline[i],              # 15: ê³¨ë¼ì¸ ê±°ë¦¬\n",
    "        ]\n",
    "        \n",
    "        # Combine: 16 Scalar + 3 Zone + 16 Type + 9 Result = 44ì°¨ì›\n",
    "        feat_vec = np.concatenate([scalars, zone_onehot[i], types_onehot[i], results_onehot[i]])\n",
    "        \n",
    "        # (1) Start Node\n",
    "        features.append(feat_vec)\n",
    "        \n",
    "        # (2) End Node (ë§ˆì§€ë§‰ ì œì™¸)\n",
    "        if i < n - 1:\n",
    "            # End ì¢Œí‘œ ê³„ì‚°\n",
    "            ex_real = ex[i] * 105.0\n",
    "            ey_real = ey[i] * 68.0\n",
    "            \n",
    "            # End ìœ„ì¹˜ì—ì„œì˜ ê³µê°„ í”¼ì²˜ ìž¬ê³„ì‚°\n",
    "            end_dist_to_goal = np.sqrt((ex_real - GOAL_X)**2 + (ey_real - GOAL_Y)**2) / 105.0\n",
    "            end_dx_to_goal = GOAL_X - ex_real\n",
    "            end_dy_to_goal = GOAL_Y - ey_real\n",
    "            end_angle = np.arctan2(end_dy_to_goal, end_dx_to_goal)\n",
    "            end_dist_to_sideline = min(ey_real, 68.0 - ey_real) / 68.0\n",
    "            end_dist_to_endline = min(ex_real, 105.0 - ex_real) / 105.0\n",
    "            \n",
    "            end_zone = get_zone(ex[i])\n",
    "            end_zone_onehot = np.zeros(3, dtype=np.float32)\n",
    "            end_zone_onehot[end_zone] = 1.0\n",
    "            \n",
    "            scalars_end = scalars.copy()\n",
    "            scalars_end[0] = ex[i]  # End X\n",
    "            scalars_end[1] = ey[i]  # End Y\n",
    "            scalars_end[2] = cumsum_dx[i]  # ëˆ„ì  íë¦„ ì—…ë°ì´íŠ¸\n",
    "            scalars_end[3] = cumsum_dy[i]\n",
    "            # End ìœ„ì¹˜ì˜ ê³µê°„ í”¼ì²˜\n",
    "            scalars_end[11] = end_dist_to_goal\n",
    "            scalars_end[12] = np.sin(end_angle)\n",
    "            scalars_end[13] = np.cos(end_angle)\n",
    "            scalars_end[14] = end_dist_to_sideline\n",
    "            scalars_end[15] = end_dist_to_endline\n",
    "            \n",
    "            feat_vec_end = np.concatenate([scalars_end, end_zone_onehot, types_onehot[i], results_onehot[i]])\n",
    "            features.append(feat_vec_end)\n",
    "            \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "# ìž…ë ¥ ì°¨ì› ìžë™ ê³„ì‚°\n",
    "# Scalar(16) + Zone(3) + Type(16) + Result(9) = 44\n",
    "dummy_group = train_df.iloc[:5].copy()\n",
    "dummy_feat = make_features(dummy_group)\n",
    "INPUT_DIM = dummy_feat.shape[1]\n",
    "\n",
    "print(f\"âœ… [V6 Phase 1] Spatial Features Added!\")\n",
    "print(f\"   Input Dimension: {INPUT_DIM}\")\n",
    "print(f\"   - Base Scalars: 11\")\n",
    "print(f\"   - Spatial Features: 5 (goal_dist, angle_sin/cos, sideline_dist, endline_dist)\")\n",
    "print(f\"   - Field Zones: 3 (defensive/midfield/attacking)\")\n",
    "print(f\"   - Event Types: 16\")\n",
    "print(f\"   - Results: 9\")\n",
    "print(f\"   Total: 16 + 3 + 16 + 9 = {INPUT_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d92c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoccerDataset(Dataset):\n",
    "    def __init__(self, df, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.episodes = []\n",
    "        self.targets = []\n",
    "        self.team_ids = []\n",
    "        self.episode_ids = []\n",
    "        \n",
    "        # ìˆœì„œ ì„žìž„ ë°©ì§€\n",
    "        grouped = df.groupby(ID_COL, sort=False)\n",
    "        \n",
    "        for name, group in tqdm(grouped, desc=f\"Dataset ({mode})\"):\n",
    "            if mode == 'train' and len(group) < 2: continue\n",
    "            \n",
    "            seq = make_features(group)\n",
    "\n",
    "            team_id = group.iloc[0]['team_id']\n",
    "            \n",
    "            if mode == 'train':\n",
    "                last = group.iloc[-1]\n",
    "                self.targets.append([last['end_x']/105.0, last['end_y']/68.0])\n",
    "                self.episodes.append(seq)\n",
    "                self.team_ids.append(team_id)\n",
    "            else:\n",
    "                self.episodes.append(seq)\n",
    "                self.team_ids.append(team_id)\n",
    "                self.episode_ids.append(str(name))\n",
    "\n",
    "    def __len__(self): return len(self.episodes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = torch.FloatTensor(self.episodes[idx])\n",
    "        if len(seq) > MAX_SEQ_LEN: seq = seq[-MAX_SEQ_LEN:]\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            # Train ë°˜í™˜: (seq, target, team_id)\n",
    "            return seq, torch.FloatTensor(self.targets[idx]), self.team_ids[idx]\n",
    "        else:\n",
    "            # Test ë°˜í™˜: (seq, team_id, episode_id)\n",
    "            return seq, self.team_ids[idx], self.episode_ids[idx]\n",
    "\n",
    "# [V8 ìˆ˜ì •] collate_fn: lengths ì •ë³´ ì¶”ê°€\n",
    "def collate_fn(batch):\n",
    "    seqs = [b[0] for b in batch]\n",
    "    lens = torch.LongTensor([len(s) for s in seqs])\n",
    "    padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    mask = torch.arange(padded.size(1))[None, :] >= lens[:, None]\n",
    "    \n",
    "    # ë°°ì¹˜ ì²« ë²ˆì§¸ ìš”ì†Œë¡œ ëª¨ë“œë¥¼ íŒë‹¨\n",
    "    elem = batch[0]\n",
    "    \n",
    "    # Test Mode í™•ì¸ (3ë²ˆì§¸ ìš”ì†Œê°€ ë¬¸ìžì—´ IDì¸ ê²½ìš°)\n",
    "    if isinstance(elem[2], str):\n",
    "        # Test: b[1]ì´ Team ID\n",
    "        team_ids = torch.LongTensor([b[1] for b in batch])\n",
    "        episode_ids = [b[2] for b in batch]\n",
    "        return padded, mask, lens, team_ids, episode_ids\n",
    "        \n",
    "    else:\n",
    "        # Train Mode: b[1]ì€ Target(ì¢Œí‘œ), b[2]ê°€ Team ID\n",
    "        targets = torch.stack([b[1] for b in batch])\n",
    "        team_ids_train = torch.LongTensor([b[2] for b in batch]) \n",
    "        return padded, targets, mask, lens, team_ids_train\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "full_dataset = SoccerDataset(train_df, mode='train')\n",
    "test_dataset = SoccerDataset(test_df, mode='test')\n",
    "\n",
    "# ì—í”¼ì†Œë“œ ê¸¸ì´ ë¶„í¬ ë¶„ì„\n",
    "episode_lengths = [len(ep) for ep in full_dataset.episodes]\n",
    "print(f\"\\nðŸ“Š [V8] ì—í”¼ì†Œë“œ ê¸¸ì´ ë¶„í¬ ë¶„ì„\")\n",
    "print(f\"   ì´ ì—í”¼ì†Œë“œ: {len(episode_lengths):,}ê°œ\")\n",
    "print(f\"   í‰ê·  ê¸¸ì´: {np.mean(episode_lengths):.2f}\")\n",
    "print(f\"   ì¤‘ì•™ê°’: {np.median(episode_lengths):.2f}\")\n",
    "print(f\"   ìµœì†Œ/ìµœëŒ€: {min(episode_lengths)} / {max(episode_lengths)}\")\n",
    "print(f\"   í‘œì¤€íŽ¸ì°¨: {np.std(episode_lengths):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# [V8] Bi-LSTM + Attention ëª¨ë¸ ì •ì˜\n",
    "# ======================================================\n",
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Bi-LSTM + Attention ëª¨ë¸\n",
    "    - ìˆœì°¨ì  ì •ë³´ ë³´ì¡´\n",
    "    - Attentionìœ¼ë¡œ ì¤‘ìš”í•œ ì•¡ì…˜ ê°•ì¡°\n",
    "    - ë‹¨ìˆœí•˜ê³  íš¨ìœ¨ì \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Attention Mechanism\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(lstm_output_dim, lstm_output_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(lstm_output_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Final Prediction Layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(lstm_output_dim, lstm_output_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_output_dim // 2, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask=None, lengths=None, return_attention=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, input_dim)\n",
    "            mask: (batch, seq_len) - True for padding\n",
    "            lengths: (batch,) - ì‹¤ì œ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "            return_attention: bool - Attention weights ë°˜í™˜ ì—¬ë¶€\n",
    "        \n",
    "        Returns:\n",
    "            prediction: (batch, 2) - ì˜ˆì¸¡ ì¢Œí‘œ\n",
    "            attn_weights: (batch, seq_len) - Attention weights (return_attention=Trueì¼ ë•Œë§Œ)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Pack sequence for efficient LSTM processing\n",
    "        if lengths is not None:\n",
    "            lengths_cpu = lengths.cpu()\n",
    "            packed_x = pack_padded_sequence(x, lengths_cpu, batch_first=True, enforce_sorted=False)\n",
    "            packed_output, (hidden, cell) = self.lstm(packed_x)\n",
    "            lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=seq_len)\n",
    "        else:\n",
    "            lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # Attention Weights ê³„ì‚°\n",
    "        attn_weights = self.attention(lstm_out)  # (batch, seq_len, 1)\n",
    "        \n",
    "        # Mask ì ìš© (íŒ¨ë”©ëœ ë¶€ë¶„ì€ -infë¡œ)\n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(mask.unsqueeze(-1), float('-inf'))\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)  # (batch, seq_len, 1)\n",
    "        \n",
    "        # Weighted Sum\n",
    "        context = torch.sum(lstm_out * attn_weights, dim=1)  # (batch, hidden*2)\n",
    "        \n",
    "        # Final Prediction\n",
    "        prediction = self.fc(context)\n",
    "        \n",
    "        if return_attention:\n",
    "            return prediction, attn_weights.squeeze(-1)  # (batch, 2), (batch, seq_len)\n",
    "        else:\n",
    "            return prediction\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = LSTMPredictor(INPUT_DIM, HIDDEN_DIM, LSTM_LAYERS, DROPOUT, BIDIRECTIONAL).to(DEVICE)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… [V8] Bi-LSTM + Attention ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ðŸ“Š ëª¨ë¸ êµ¬ì¡°:\")\n",
    "print(f\"   - Input Dim: {INPUT_DIM}\")\n",
    "print(f\"   - Hidden Dim: {HIDDEN_DIM}\")\n",
    "print(f\"   - Layers: {LSTM_LAYERS}\")\n",
    "print(f\"   - Bidirectional: {BIDIRECTIONAL}\")\n",
    "print(f\"   - Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63608a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# [V8] Bi-LSTM + Attention í•™ìŠµ\n",
    "# ======================================================\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "SEEDS = [42, 2024, 777]\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ðŸš€ [V8] Bi-LSTM + Attention í•™ìŠµ ì‹œìž‘\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"ì „ëžµ: ë‹¨ì¼ LSTM ëª¨ë¸ë¡œ ëª¨ë“  ê¸¸ì´ ì²˜ë¦¬\")\n",
    "print(f\"  - Bi-LSTM: ì–‘ë°©í–¥ìœ¼ë¡œ ìˆœì°¨ ì •ë³´ í¬ì°©\")\n",
    "print(f\"  - Attention: ì¤‘ìš”í•œ ì•¡ì…˜ì— ì§‘ì¤‘\")\n",
    "print(f\"  - ë‹¨ìˆœí•˜ê³  íš¨ìœ¨ì ì¸ êµ¬ì¡°\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ížˆìŠ¤í† ë¦¬ ì €ìž¥ìš©\n",
    "all_histories = []\n",
    "\n",
    "for i, seed in enumerate(SEEDS):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ“¦ [ëª¨ë¸ {i+1}/3] Seed {seed} í•™ìŠµ ì‹œìž‘\")\n",
    "    print(f\"{'='*70}\")\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ë¶„í• \n",
    "    train_idx, val_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=seed)\n",
    "    train_loader = DataLoader(torch.utils.data.Subset(full_dataset, train_idx), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(torch.utils.data.Subset(full_dataset, val_idx), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    model = LSTMPredictor(INPUT_DIM, HIDDEN_DIM, LSTM_LAYERS, DROPOUT, BIDIRECTIONAL).to(DEVICE)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR_BASE, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS_BASE - WARMUP_EPOCHS, eta_min=1e-6)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    \n",
    "    best_dist = float('inf')\n",
    "    history = {'train_loss': [], 'val_dist': [], 'lr': []}\n",
    "    \n",
    "    for epoch in range(EPOCHS_BASE):\n",
    "        # Warmup\n",
    "        if epoch < WARMUP_EPOCHS:\n",
    "            lr = LR_BASE * (epoch + 1) / WARMUP_EPOCHS\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for seqs, targets, mask, lens, _ in train_loader:\n",
    "            seqs, targets, mask, lens = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(seqs, mask, lens)\n",
    "            loss = criterion(pred, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        dists = []\n",
    "        with torch.no_grad():\n",
    "            for seqs, targets, mask, lens, _ in val_loader:\n",
    "                seqs, targets, mask, lens = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n",
    "                pred = model(seqs, mask, lens)\n",
    "                p_real = pred.cpu().numpy() * np.array([105.0, 68.0])\n",
    "                t_real = targets.cpu().numpy() * np.array([105.0, 68.0])\n",
    "                dists.extend(np.sqrt(np.sum((p_real - t_real)**2, axis=1)))\n",
    "        \n",
    "        avg_dist = np.mean(dists)\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        if epoch >= WARMUP_EPOCHS:\n",
    "            scheduler.step()\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_dist'].append(avg_dist)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0 or epoch < 5:\n",
    "            print(f\"  [Epoch {epoch+1:2d}/{EPOCHS_BASE}] \"\n",
    "                  f\"Loss: {avg_train_loss:.4f} | \"\n",
    "                  f\"Val Dist: {avg_dist:.4f}m | \"\n",
    "                  f\"LR: {current_lr:.6f}\")\n",
    "        \n",
    "        if avg_dist < best_dist:\n",
    "            best_dist = avg_dist\n",
    "            torch.save(model.state_dict(), f'v8_model_{i}.pth')\n",
    "    \n",
    "    history['best_dist'] = best_dist\n",
    "    history['seed'] = seed\n",
    "    all_histories.append(history)\n",
    "    \n",
    "    print(f\"  âœ… Model {i+1} ì™„ë£Œ. Best Val Dist: {best_dist:.4f}m\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ… ì „ì²´ í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"ðŸ“Š í‰ê·  Best Distance: {np.mean([h['best_dist'] for h in all_histories]):.4f}m\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5911a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# [V8] Bi-LSTM + Attention ì¶”ë¡ \n",
    "# ======================================================\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ðŸ”® [V8] Bi-LSTM + Attention ì¶”ë¡  ì‹œìž‘\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (3ê°œ ì•™ìƒë¸”)\n",
    "models = []\n",
    "for i in range(3):\n",
    "    model = LSTMPredictor(INPUT_DIM, HIDDEN_DIM, LSTM_LAYERS, DROPOUT, BIDIRECTIONAL).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(f'v8_model_{i}.pth'))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: 3ê°œ ì•™ìƒë¸”\")\n",
    "\n",
    "# ì•™ìƒë¸” ê°€ì¤‘ì¹˜\n",
    "weights = [0.25, 0.25, 0.5]\n",
    "\n",
    "# ì¶”ë¡ \n",
    "results = []\n",
    "print(f\"\\nðŸ”„ ì¶”ë¡  ì¤‘...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seqs, mask, lens, team_ids, episode_ids in tqdm(test_loader, desc=\"Inference\"):\n",
    "        seqs, mask, lens = seqs.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n",
    "        \n",
    "        # 3ê°œ ëª¨ë¸ ì˜ˆì¸¡\n",
    "        m1 = models[0](seqs, mask, lens).cpu().numpy()\n",
    "        m2 = models[1](seqs, mask, lens).cpu().numpy()\n",
    "        m3 = models[2](seqs, mask, lens).cpu().numpy()\n",
    "        \n",
    "        # ì•™ìƒë¸”\n",
    "        avg_pred = (m1 * weights[0]) + (m2 * weights[1]) + (m3 * weights[2])\n",
    "        \n",
    "        # ì¢Œí‘œ ë³€í™˜ ë° ì €ìž¥\n",
    "        for i, eid in enumerate(episode_ids):\n",
    "            px = avg_pred[i, 0] * 105.0\n",
    "            py = avg_pred[i, 1] * 68.0\n",
    "            results.append({'game_episode': eid, 'pred_x': px, 'pred_y': py})\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "pred_df = pd.DataFrame(results)\n",
    "\n",
    "SUBMISSION_PATH = \"open_track1/sample_submission.csv\"\n",
    "if os.path.exists(SUBMISSION_PATH):\n",
    "    sub = pd.read_csv(SUBMISSION_PATH)\n",
    "else:\n",
    "    sub = pd.read_csv(TEST_META_PATH)\n",
    "    col_map = {'episode_id': 'game_episode'}\n",
    "    sub = sub.rename(columns=col_map)\n",
    "    sub = sub[['game_episode']]\n",
    "\n",
    "print(f\"\\nðŸ“„ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "print(f\"   Submission Shape: {sub.shape}, Prediction Shape: {pred_df.shape}\")\n",
    "\n",
    "final_sub = pd.merge(sub[['game_episode']], pred_df, on='game_episode', how='left')\n",
    "final_sub = final_sub.rename(columns={'pred_x': 'end_x', 'pred_y': 'end_y'})\n",
    "\n",
    "nan_count = final_sub.isnull().sum().sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"   âš ï¸ ê²½ê³ : {nan_count}ê°œì˜ ê²°ì¸¡ì¹˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    final_sub = final_sub.fillna(50.0)\n",
    "\n",
    "filename = \"v8_lstm_submit.csv\"\n",
    "final_sub.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ… [V8] Bi-LSTM + Attention ì¶”ë¡  ì™„ë£Œ!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"ðŸ“ ì œì¶œ íŒŒì¼: {filename}\")\n",
    "print(f\"ðŸ“Š ëª¨ë¸ íŠ¹ì§•:\")\n",
    "print(f\"   - Bi-LSTM: ì–‘ë°©í–¥ ì‹œí€€ìŠ¤ ì²˜ë¦¬\")\n",
    "print(f\"   - Attention: ì¤‘ìš” ì•¡ì…˜ ê°•ì¡°\")\n",
    "print(f\"   - 3-Seed ì•™ìƒë¸”: ì•ˆì •ì  ì˜ˆì¸¡\")\n",
    "print(f\"{'='*70}\")\n",
    "print(final_sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ai7ldiiy9o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# [V8] Bi-LSTM + Attention í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "# ======================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('[V8] Bi-LSTM + Attention í•™ìŠµ ê³¡ì„ ', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "labels = [f'Model {i+1} (Seed {h[\"seed\"]})' for i, h in enumerate(all_histories)]\n",
    "\n",
    "# 1. Training Loss\n",
    "ax1 = axes[0, 0]\n",
    "for i, history in enumerate(all_histories):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax1.plot(epochs, history['train_loss'], label=labels[i], color=colors[i], linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Huber Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Validation Distance\n",
    "ax2 = axes[0, 1]\n",
    "for i, history in enumerate(all_histories):\n",
    "    epochs = range(1, len(history['val_dist']) + 1)\n",
    "    ax2.plot(epochs, history['val_dist'], label=labels[i], color=colors[i], linewidth=2)\n",
    "    best_idx = np.argmin(history['val_dist'])\n",
    "    best_val = history['val_dist'][best_idx]\n",
    "    ax2.scatter(best_idx + 1, best_val, color=colors[i], s=100, zorder=5, marker='*')\n",
    "    ax2.annotate(f'{best_val:.2f}m', xy=(best_idx + 1, best_val), \n",
    "                 xytext=(5, 5), textcoords='offset points', fontsize=9, color=colors[i])\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Distance (m)')\n",
    "ax2.set_title('Validation Distance')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Learning Rate Schedule\n",
    "ax3 = axes[1, 0]\n",
    "history = all_histories[0]\n",
    "epochs = range(1, len(history['lr']) + 1)\n",
    "ax3.plot(epochs, history['lr'], color='purple', linewidth=2)\n",
    "ax3.axvline(x=WARMUP_EPOCHS, color='red', linestyle='--', label=f'Warmup End (Epoch {WARMUP_EPOCHS})')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Learning Rate')\n",
    "ax3.set_title('Learning Rate Schedule (Warmup + Cosine Annealing)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "# 4. Best Distances ë¹„êµ\n",
    "ax4 = axes[1, 1]\n",
    "best_dists = [h['best_dist'] for h in all_histories]\n",
    "seeds = [h['seed'] for h in all_histories]\n",
    "bars = ax4.bar(range(len(best_dists)), best_dists, color=colors)\n",
    "ax4.set_xticks(range(len(best_dists)))\n",
    "ax4.set_xticklabels([f'Model {i+1}\\n(Seed {s})' for i, s in enumerate(seeds)])\n",
    "ax4.set_ylabel('Best Validation Distance (m)')\n",
    "ax4.set_title('Best Performance per Model')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (bar, dist) in enumerate(zip(bars, best_dists)):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{dist:.4f}m',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('v8_training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\ní•™ìŠµ ê³¡ì„  ì €ìž¥ ì™„ë£Œ: v8_training_history.png\")\n",
    "print(f\"\\nìµœì¢… ê²°ê³¼:\")\n",
    "print(f\"{'='*60}\")\n",
    "for i, h in enumerate(all_histories):\n",
    "    print(f\"  Model {i+1} (Seed {h['seed']:4d}): {h['best_dist']:.4f}m\")\n",
    "print(f\"  í‰ê· : {np.mean([h['best_dist'] for h in all_histories]):.4f}m\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5dee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# [V8] Attention ë¶„ì„: ëª¨ë¸ì´ ì–´ë–¤ ì•¡ì…˜ì— ì§‘ì¤‘í•˜ëŠ”ê°€?\n",
    "# ======================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ðŸ” [V8] Attention ë¶„ì„ ì‹œìž‘\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ\n",
    "analysis_model = LSTMPredictor(INPUT_DIM, HIDDEN_DIM, LSTM_LAYERS, DROPOUT, BIDIRECTIONAL).to(DEVICE)\n",
    "analysis_model.load_state_dict(torch.load('v8_model_0.pth'))\n",
    "analysis_model.eval()\n",
    "\n",
    "# Validation ë°ì´í„° ì¤€ë¹„ (Seed 42 ì‚¬ìš©)\n",
    "seed_everything(42)\n",
    "train_idx, val_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n",
    "val_subset = torch.utils.data.Subset(full_dataset, val_idx)\n",
    "\n",
    "print(f\"âœ… ë¶„ì„ ë°ì´í„°: {len(val_subset):,}ê°œ ì—í”¼ì†Œë“œ\")\n",
    "\n",
    "# Attention ì¶”ì¶œ í•¨ìˆ˜\n",
    "def extract_attention_weights(model, dataset_subset, num_samples=500):\n",
    "    \"\"\"\n",
    "    ëžœë¤ ìƒ˜í”Œì—ì„œ Attention weights ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    attention_data = []\n",
    "    \n",
    "    # ëžœë¤ ìƒ˜í”Œë§\n",
    "    sample_indices = np.random.choice(len(dataset_subset), min(num_samples, len(dataset_subset)), replace=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(sample_indices, desc=\"Extracting Attention\"):\n",
    "            seq, target, team_id = dataset_subset[idx]\n",
    "            \n",
    "            # ì‹œí€€ìŠ¤ ê¸¸ì´ ì œí•œ\n",
    "            if len(seq) > MAX_SEQ_LEN:\n",
    "                seq = seq[-MAX_SEQ_LEN:]\n",
    "            \n",
    "            seq = seq.unsqueeze(0).to(DEVICE)  # (1, seq_len, features)\n",
    "            length = torch.LongTensor([len(seq[0])]).to(DEVICE)\n",
    "            mask = torch.zeros(1, len(seq[0]), dtype=torch.bool).to(DEVICE)\n",
    "            \n",
    "            # Attention weights ì¶”ì¶œ\n",
    "            pred, attn_weights = model(seq, mask, length, return_attention=True)\n",
    "            \n",
    "            attn_weights = attn_weights.cpu().numpy()[0]  # (seq_len,)\n",
    "            features = seq.cpu().numpy()[0]  # (seq_len, 44)\n",
    "            \n",
    "            attention_data.append({\n",
    "                'attention': attn_weights,\n",
    "                'features': features,\n",
    "                'seq_length': len(attn_weights)\n",
    "            })\n",
    "    \n",
    "    return attention_data\n",
    "\n",
    "# Attention weights ì¶”ì¶œ\n",
    "print(f\"\\nðŸ“Š Attention Weights ì¶”ì¶œ ì¤‘...\")\n",
    "attention_data = extract_attention_weights(analysis_model, val_subset, num_samples=1000)\n",
    "\n",
    "print(f\"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(attention_data)}ê°œ ìƒ˜í”Œ\")\n",
    "print(f\"   í‰ê·  ì‹œí€€ìŠ¤ ê¸¸ì´: {np.mean([d['seq_length'] for d in attention_data]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# ì•¡ì…˜ íƒ€ìž…ë³„ Attention ë¶„ì„\n",
    "# ======================================================\n",
    "\n",
    "# Feature ì¸ë±ìŠ¤ ì •ì˜ (make_features í•¨ìˆ˜ ê¸°ì¤€)\n",
    "# Scalar(16) + Zone(3) + Type(16) + Result(9) = 44\n",
    "ZONE_START = 16\n",
    "TYPE_START = 19  # 16 + 3\n",
    "RESULT_START = 35  # 16 + 3 + 16\n",
    "\n",
    "# ê° íƒ€ìž…ë³„ í‰ê·  Attention ê³„ì‚°\n",
    "type_attention_scores = {t: [] for t in TOP_TYPES}\n",
    "type_attention_scores['Other'] = []\n",
    "\n",
    "result_attention_scores = {r: [] for r in ALL_RESULTS}\n",
    "result_attention_scores['Other'] = []\n",
    "\n",
    "print(f\"\\nðŸ“Š ì•¡ì…˜ íƒ€ìž…ë³„ Attention ë¶„ì„...\")\n",
    "\n",
    "for sample in tqdm(attention_data, desc=\"Analyzing\"):\n",
    "    features = sample['features']  # (seq_len, 44)\n",
    "    attention = sample['attention']  # (seq_len,)\n",
    "    \n",
    "    for i in range(len(attention)):\n",
    "        # íƒ€ìž… ì¶”ì¶œ (one-hotì—ì„œ)\n",
    "        type_onehot = features[i, TYPE_START:TYPE_START+16]\n",
    "        type_idx = np.argmax(type_onehot)\n",
    "        \n",
    "        if type_idx < len(TOP_TYPES):\n",
    "            type_name = TOP_TYPES[type_idx]\n",
    "        else:\n",
    "            type_name = 'Other'\n",
    "        \n",
    "        type_attention_scores[type_name].append(attention[i])\n",
    "        \n",
    "        # ê²°ê³¼ ì¶”ì¶œ\n",
    "        result_onehot = features[i, RESULT_START:RESULT_START+9]\n",
    "        result_idx = np.argmax(result_onehot)\n",
    "        \n",
    "        if result_idx < len(ALL_RESULTS):\n",
    "            result_name = ALL_RESULTS[result_idx]\n",
    "        else:\n",
    "            result_name = 'Other'\n",
    "        \n",
    "        result_attention_scores[result_name].append(attention[i])\n",
    "\n",
    "# í‰ê·  ê³„ì‚°\n",
    "type_avg_attention = {k: np.mean(v) if v else 0 for k, v in type_attention_scores.items()}\n",
    "result_avg_attention = {k: np.mean(v) if v else 0 for k, v in result_attention_scores.items()}\n",
    "\n",
    "# ì •ë ¬ (ë†’ì€ ìˆœ)\n",
    "sorted_types = sorted(type_avg_attention.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_results = sorted(result_avg_attention.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ðŸŽ¯ ì•¡ì…˜ íƒ€ìž…ë³„ í‰ê·  Attention (ìƒìœ„ 10ê°œ)\")\n",
    "print(f\"{'='*70}\")\n",
    "for i, (type_name, avg_attn) in enumerate(sorted_types[:10], 1):\n",
    "    count = len(type_attention_scores[type_name])\n",
    "    print(f\"  {i:2d}. {type_name:20s}: {avg_attn:.6f} (ìƒ˜í”Œ ìˆ˜: {count:,})\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ðŸŽ¯ ê²°ê³¼ë³„ í‰ê·  Attention (ìƒìœ„ 8ê°œ)\")\n",
    "print(f\"{'='*70}\")\n",
    "for i, (result_name, avg_attn) in enumerate(sorted_results[:8], 1):\n",
    "    count = len(result_attention_scores[result_name])\n",
    "    print(f\"  {i:2d}. {result_name:20s}: {avg_attn:.6f} (ìƒ˜í”Œ ìˆ˜: {count:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Attention ì‹œê°í™”\n",
    "# ======================================================\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. ì•¡ì…˜ íƒ€ìž…ë³„ Attention (ìƒìœ„ 12ê°œ)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "top_types = sorted_types[:12]\n",
    "type_names = [t[0] for t in top_types]\n",
    "type_scores = [t[1] for t in top_types]\n",
    "\n",
    "bars = ax1.barh(type_names, type_scores, color='steelblue', edgecolor='black', linewidth=0.8)\n",
    "ax1.set_xlabel('í‰ê·  Attention Weight', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('[V8] ì•¡ì…˜ íƒ€ìž…ë³„ í‰ê·  Attention - ëª¨ë¸ì´ ì§‘ì¤‘í•˜ëŠ” ì•¡ì…˜', fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# ê°’ í‘œì‹œ\n",
    "for i, (bar, score) in enumerate(zip(bars, type_scores)):\n",
    "    count = len(type_attention_scores[type_names[i]])\n",
    "    ax1.text(score + 0.0001, i, f'{score:.5f} (n={count:,})', \n",
    "             va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 2. ê²°ê³¼ë³„ Attention (ìƒìœ„ 8ê°œ)\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "top_results = sorted_results[:8]\n",
    "result_names = [r[0] for r in top_results]\n",
    "result_scores = [r[1] for r in top_results]\n",
    "\n",
    "bars2 = ax2.bar(range(len(result_names)), result_scores, color='coral', edgecolor='black', linewidth=0.8)\n",
    "ax2.set_xticks(range(len(result_names)))\n",
    "ax2.set_xticklabels(result_names, rotation=45, ha='right', fontsize=9)\n",
    "ax2.set_ylabel('í‰ê·  Attention Weight', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('ê²°ê³¼ë³„ í‰ê·  Attention', fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (bar, score) in enumerate(zip(bars2, result_scores)):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(i, height + 0.0001, f'{score:.5f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "# 3. ì‹œí€€ìŠ¤ ìœ„ì¹˜ë³„ Attention íŒ¨í„´ (ížˆíŠ¸ë§µ)\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "# ì‹œí€€ìŠ¤ë¥¼ 10ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í‰ê·  attention ê³„ì‚°\n",
    "position_attention = []\n",
    "for sample in attention_data:\n",
    "    attention = sample['attention']\n",
    "    seq_len = len(attention)\n",
    "    \n",
    "    # ì •ê·œí™”ëœ ìœ„ì¹˜ (0~1)\n",
    "    normalized_positions = np.linspace(0, 1, seq_len)\n",
    "    \n",
    "    # 10ê°œ êµ¬ê°„ìœ¼ë¡œ binning\n",
    "    bins = np.linspace(0, 1, 11)\n",
    "    bin_indices = np.digitize(normalized_positions, bins) - 1\n",
    "    \n",
    "    for pos_idx in range(10):\n",
    "        mask = bin_indices == pos_idx\n",
    "        if mask.sum() > 0:\n",
    "            position_attention.append({\n",
    "                'position': pos_idx,\n",
    "                'attention': attention[mask].mean()\n",
    "            })\n",
    "\n",
    "# êµ¬ê°„ë³„ í‰ê· \n",
    "position_groups = {}\n",
    "for item in position_attention:\n",
    "    pos = item['position']\n",
    "    if pos not in position_groups:\n",
    "        position_groups[pos] = []\n",
    "    position_groups[pos].append(item['attention'])\n",
    "\n",
    "position_avg = [np.mean(position_groups.get(i, [0])) for i in range(10)]\n",
    "position_labels = ['ì‹œìž‘\\n0-10%', '10-20%', '20-30%', '30-40%', '40-50%', \n",
    "                   '50-60%', '60-70%', '70-80%', '80-90%', 'ë\\n90-100%']\n",
    "\n",
    "bars3 = ax3.bar(range(10), position_avg, color='mediumseagreen', edgecolor='black', linewidth=0.8)\n",
    "ax3.set_xticks(range(10))\n",
    "ax3.set_xticklabels(position_labels, rotation=0, fontsize=9)\n",
    "ax3.set_ylabel('í‰ê·  Attention Weight', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('ì‹œí€€ìŠ¤ ìœ„ì¹˜ë³„ í‰ê·  Attention', fontsize=12, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (bar, score) in enumerate(zip(bars3, position_avg)):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(i, height + 0.0001, f'{score:.5f}', ha='center', va='bottom', fontsize=8, fontweight='bold', rotation=0)\n",
    "\n",
    "# 4. Attention ë¶„í¬ ížˆìŠ¤í† ê·¸ëž¨\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "\n",
    "all_attention_weights = []\n",
    "for sample in attention_data:\n",
    "    all_attention_weights.extend(sample['attention'])\n",
    "\n",
    "ax4.hist(all_attention_weights, bins=100, color='purple', alpha=0.7, edgecolor='black')\n",
    "ax4.set_xlabel('Attention Weight', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('ë¹ˆë„', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('ì „ì²´ Attention Weight ë¶„í¬', fontsize=14, fontweight='bold')\n",
    "ax4.axvline(np.mean(all_attention_weights), color='red', linestyle='--', linewidth=2, label=f'í‰ê· : {np.mean(all_attention_weights):.5f}')\n",
    "ax4.axvline(np.median(all_attention_weights), color='orange', linestyle='--', linewidth=2, label=f'ì¤‘ì•™ê°’: {np.median(all_attention_weights):.5f}')\n",
    "ax4.legend(fontsize=11)\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('[V8] Bi-LSTM + Attention ë¶„ì„ - ëª¨ë¸ì´ ì§‘ì¤‘í•˜ëŠ” ì•¡ì…˜ íŒ¨í„´', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.savefig('v8_attention_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ… Attention ë¶„ì„ ì™„ë£Œ!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"ðŸ“Š ì£¼ìš” ë°œê²¬:\")\n",
    "print(f\"   - ê°€ìž¥ ë†’ì€ Attention: {sorted_types[0][0]} ({sorted_types[0][1]:.5f})\")\n",
    "print(f\"   - ì „ì²´ í‰ê·  Attention: {np.mean(all_attention_weights):.5f}\")\n",
    "print(f\"   - Attention í‘œì¤€íŽ¸ì°¨: {np.std(all_attention_weights):.5f}\")\n",
    "print(f\"   - ì‹œí€€ìŠ¤ ëë¶€ë¶„ Attention: {position_avg[-1]:.5f}\")\n",
    "print(f\"   - ì‹œí€€ìŠ¤ ì‹œìž‘ë¶€ë¶„ Attention: {position_avg[0]:.5f}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"ðŸ’¾ ì €ìž¥ ì™„ë£Œ: v8_attention_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41712c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daycon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
