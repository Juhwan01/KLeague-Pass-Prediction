{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d00b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "[V11-Improved] MDN ê°œì„  ë²„ì „\n",
      "  ê°œì„ ì‚¬í•­:\n",
      "  1. Gaussians: 3 â†’ 2 (ë‹¨ìˆœí™”)\n",
      "  2. LR: 5e-4 â†’ 1e-3 (V10 ìˆ˜ì¤€)\n",
      "  3. Sigma ë²”ìœ„ ì œí•œ: [0.01, 0.3]\n",
      "  4. Hybrid Loss: NLL + 0.3*MSE\n",
      "  5. ì˜ˆì¸¡ ì „ëµ: ê°€ì¤‘í‰ê·  (mean)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# ======================================================\n",
    "# [V11-Improved] MDN ê°œì„  ë²„ì „\n",
    "# ======================================================\n",
    "BATCH_SIZE = 64\n",
    "LR_BASE = 1e-3  # âœ… 5e-4 â†’ 1e-3\n",
    "WARMUP_EPOCHS = 3  # âœ… 5 â†’ 3\n",
    "EPOCHS_BASE = 50\n",
    "DROPOUT = 0.2\n",
    "MAX_SEQ_LEN = 30\n",
    "GRAD_CLIP = 1.0\n",
    "\n",
    "HIDDEN_DIM = 256\n",
    "LSTM_LAYERS = 3\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "# MDN ê°œì„  íŒŒë¼ë¯¸í„°\n",
    "NUM_GAUSSIANS = 2  # âœ… 3 â†’ 2 (ì£¼ìš”/ëŒ€ì•ˆ)\n",
    "MIN_SIGMA = 0.01   # âœ… ë” í° í•˜í•œ\n",
    "MAX_SIGMA = 0.3    # âœ… ë¶„ì‚° ìƒí•œ ì¶”ê°€\n",
    "HYBRID_LOSS_WEIGHT = 0.3  # âœ… Hybrid Loss ê°€ì¤‘ì¹˜\n",
    "\n",
    "print(f\"[V11-Improved] MDN ê°œì„  ë²„ì „\")\n",
    "print(f\"  ê°œì„ ì‚¬í•­:\")\n",
    "print(f\"  1. Gaussians: 3 â†’ 2 (ë‹¨ìˆœí™”)\")\n",
    "print(f\"  2. LR: 5e-4 â†’ 1e-3 (V10 ìˆ˜ì¤€)\")\n",
    "print(f\"  3. Sigma ë²”ìœ„ ì œí•œ: [0.01, 0.3]\")\n",
    "print(f\"  4. Hybrid Loss: NLL + 0.3*MSE\")\n",
    "print(f\"  5. ì˜ˆì¸¡ ì „ëµ: ê°€ì¤‘í‰ê·  (mean)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de82db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train Loaded: (356721, 15)\n",
      "â„¹ï¸ Reading 2414 test files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Test CSVs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2414/2414 [00:04<00:00, 527.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test Data Merged: (53110, 15)\n",
      "Data Ready. ID Column: game_episode\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¡œë“œ (V11ê³¼ ë™ì¼)\n",
    "BASE_DIR = \"./open_track1\"\n",
    "if not os.path.exists(BASE_DIR): BASE_DIR = \".\"\n",
    "\n",
    "TRAIN_PATH = os.path.join(BASE_DIR, \"train.csv\")\n",
    "TEST_META_PATH = os.path.join(BASE_DIR, \"test.csv\")\n",
    "MATCH_PATH = os.path.join(BASE_DIR, \"match_info.csv\")\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "print(f\"âœ… Train Loaded: {train_df.shape}\")\n",
    "\n",
    "if os.path.exists(TEST_META_PATH):\n",
    "    test_meta = pd.read_csv(TEST_META_PATH)\n",
    "    print(f\"â„¹ï¸ Reading {len(test_meta)} test files...\")\n",
    "    test_dfs = []\n",
    "    for _, row in tqdm(test_meta.iterrows(), total=len(test_meta), desc=\"Loading Test CSVs\"):\n",
    "        rel_path = row['path']\n",
    "        paths_to_try = [\n",
    "            rel_path,\n",
    "            os.path.join(BASE_DIR, rel_path.lstrip(\"./\")),\n",
    "            os.path.join(BASE_DIR, \"test\", str(row['game_id']), os.path.basename(rel_path))\n",
    "        ]\n",
    "        for p in paths_to_try:\n",
    "            if os.path.exists(p):\n",
    "                test_dfs.append(pd.read_csv(p))\n",
    "                break\n",
    "    if test_dfs:\n",
    "        test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "        print(f\"âœ… Test Data Merged: {test_df.shape}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"test.csv not found\")\n",
    "\n",
    "if os.path.exists(MATCH_PATH):\n",
    "    match_info = pd.read_csv(MATCH_PATH)\n",
    "    match_subset = match_info[['game_id', 'home_team_id', 'venue']]\n",
    "    train_df = pd.merge(train_df, match_subset, on='game_id', how='left')\n",
    "    test_df = pd.merge(test_df, match_subset, on='game_id', how='left')\n",
    "\n",
    "def preprocess(df):\n",
    "    if 'home_team_id' in df.columns:\n",
    "        df['is_home'] = (df['team_id'] == df['home_team_id']).astype(float)\n",
    "    else:\n",
    "        df['is_home'] = 0.5\n",
    "    if 'end_x' not in df.columns:\n",
    "        df['end_x'] = 0.0\n",
    "        df['end_y'] = 0.0\n",
    "    else:\n",
    "        df['end_x'] = df['end_x'].fillna(0.0)\n",
    "        df['end_y'] = df['end_y'].fillna(0.0)\n",
    "    return df\n",
    "\n",
    "train_df = preprocess(train_df)\n",
    "test_df = preprocess(test_df)\n",
    "\n",
    "ID_COL = 'game_episode' if 'game_episode' in train_df.columns else 'episode_id'\n",
    "print(f\"Data Ready. ID Column: {ID_COL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Input Dimension: 44\n"
     ]
    }
   ],
   "source": [
    "# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (V11ê³¼ ë™ì¼)\n",
    "TOP_TYPES = ['Pass', 'Carry', 'Recovery', 'Interception', 'Duel', 'Tackle', \n",
    "             'Throw-In', 'Clearance', 'Intervention', 'Block', 'Pass_Freekick', \n",
    "             'Cross', 'Goal Kick', 'Error', 'Shot']\n",
    "ALL_RESULTS = ['Successful', 'Unsuccessful', 'On Target', 'Yellow_Card', \n",
    "               'Blocked', 'Keeper Rush-Out', 'Low Quality Shot', 'Off Target']\n",
    "\n",
    "def make_features(group):\n",
    "    n = len(group)\n",
    "    sx = group['start_x'].values / 105.0\n",
    "    sy = group['start_y'].values / 68.0\n",
    "    ex = group['end_x'].values / 105.0\n",
    "    ey = group['end_y'].values / 68.0\n",
    "    is_home = group['is_home'].values\n",
    "    \n",
    "    if 'time_seconds' in group.columns:\n",
    "        times = group['time_seconds'].values\n",
    "        dt = np.zeros(n, dtype=np.float32)\n",
    "        dt[1:] = times[1:] - times[:-1]\n",
    "        dt = np.maximum(dt, 0.1)\n",
    "    else:\n",
    "        dt = np.ones(n, dtype=np.float32)\n",
    "\n",
    "    dx = ex - sx\n",
    "    dy = ey - sy\n",
    "    dist_meter = np.sqrt((dx*105)**2 + (dy*68)**2)\n",
    "    cumsum_dx = np.cumsum(dx) / 105.0\n",
    "    cumsum_dy = np.cumsum(dy) / 68.0\n",
    "    lag_dist_m = np.roll(dist_meter, 1); lag_dist_m[0] = 0\n",
    "    lag_cumsum_dx = np.roll(cumsum_dx, 1); lag_cumsum_dx[0] = 0\n",
    "    lag_cumsum_dy = np.roll(cumsum_dy, 1); lag_cumsum_dy[0] = 0\n",
    "    lag_dt = np.roll(dt, 1); lag_dt[0] = 1.0\n",
    "    lag_speed = lag_dist_m / np.maximum(lag_dt, 0.1)\n",
    "    \n",
    "    if 'player_id' in group.columns:\n",
    "        p_ids = group['player_id'].values\n",
    "        is_same = np.zeros(n, dtype=np.float32)\n",
    "        is_same[1:] = (p_ids[1:] == p_ids[:-1]).astype(np.float32)\n",
    "    else:\n",
    "        is_same = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "    progress = np.arange(n) / max(n-1, 1)\n",
    "    is_second_half = (group['period_id'].values > 1).astype(np.float32) if 'period_id' in group.columns else np.zeros(n)\n",
    "    \n",
    "    GOAL_X, GOAL_Y = 105.0, 34.0\n",
    "    sx_real, sy_real = sx * 105.0, sy * 68.0\n",
    "    dist_to_goal = np.sqrt((sx_real - GOAL_X)**2 + (sy_real - GOAL_Y)**2) / 105.0\n",
    "    angle_to_goal = np.arctan2(GOAL_Y - sy_real, GOAL_X - sx_real)\n",
    "    angle_sin, angle_cos = np.sin(angle_to_goal), np.cos(angle_to_goal)\n",
    "    dist_to_sideline = np.minimum(sy_real, 68.0 - sy_real) / 68.0\n",
    "    dist_to_endline = np.minimum(sx_real, 105.0 - sx_real) / 105.0\n",
    "    \n",
    "    def get_zone(x_norm):\n",
    "        if x_norm < 35.0/105.0: return 0\n",
    "        elif x_norm < 70.0/105.0: return 1\n",
    "        else: return 2\n",
    "    \n",
    "    zones = np.array([get_zone(x) for x in sx])\n",
    "    zone_onehot = np.zeros((n, 3), dtype=np.float32)\n",
    "    for i, z in enumerate(zones): zone_onehot[i, z] = 1.0\n",
    "    \n",
    "    types_onehot = np.zeros((n, len(TOP_TYPES) + 1), dtype=np.float32)\n",
    "    for i, t in enumerate(group['type_name'].values):\n",
    "        types_onehot[i, TOP_TYPES.index(t) if t in TOP_TYPES else -1] = 1.0\n",
    "    \n",
    "    results_onehot = np.zeros((n, len(ALL_RESULTS) + 1), dtype=np.float32)\n",
    "    for i, r in enumerate(group['result_name'].values):\n",
    "        results_onehot[i, ALL_RESULTS.index(r) if r in ALL_RESULTS else -1] = 1.0\n",
    "\n",
    "    features = []\n",
    "    for i in range(n):\n",
    "        scalars = [sx[i], sy[i], lag_cumsum_dx[i], lag_cumsum_dy[i], lag_dist_m[i]/100.0,\n",
    "                   lag_speed[i]/10.0, dt[i]/10.0, progress[i], is_home[i], is_same[i],\n",
    "                   is_second_half[i], dist_to_goal[i], angle_sin[i], angle_cos[i],\n",
    "                   dist_to_sideline[i], dist_to_endline[i]]\n",
    "        feat_vec = np.concatenate([scalars, zone_onehot[i], types_onehot[i], results_onehot[i]])\n",
    "        features.append(feat_vec)\n",
    "        \n",
    "        if i < n - 1:\n",
    "            ex_real, ey_real = ex[i] * 105.0, ey[i] * 68.0\n",
    "            end_dist_to_goal = np.sqrt((ex_real - GOAL_X)**2 + (ey_real - GOAL_Y)**2) / 105.0\n",
    "            end_angle = np.arctan2(GOAL_Y - ey_real, GOAL_X - ex_real)\n",
    "            scalars_end = scalars.copy()\n",
    "            scalars_end[0:2] = [ex[i], ey[i]]\n",
    "            scalars_end[2:4] = [cumsum_dx[i], cumsum_dy[i]]\n",
    "            scalars_end[11:16] = [end_dist_to_goal, np.sin(end_angle), np.cos(end_angle),\n",
    "                                   min(ey_real, 68.0 - ey_real) / 68.0,\n",
    "                                   min(ex_real, 105.0 - ex_real) / 105.0]\n",
    "            end_zone_onehot = np.zeros(3, dtype=np.float32)\n",
    "            end_zone_onehot[get_zone(ex[i])] = 1.0\n",
    "            feat_vec_end = np.concatenate([scalars_end, end_zone_onehot, types_onehot[i], results_onehot[i]])\n",
    "            features.append(feat_vec_end)\n",
    "            \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "dummy_group = train_df.iloc[:5].copy()\n",
    "INPUT_DIM = make_features(dummy_group).shape[1]\n",
    "print(f\"âœ… Input Dimension: {INPUT_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15435/15435 [00:17<00:00, 898.34it/s]\n",
      "Dataset (test): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2414/2414 [00:02<00:00, 864.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset: 15428 train, 2414 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ (V11ê³¼ ë™ì¼)\n",
    "class SoccerDataset(Dataset):\n",
    "    def __init__(self, df, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.episodes, self.targets, self.team_ids, self.episode_ids = [], [], [], []\n",
    "        for name, group in tqdm(df.groupby(ID_COL, sort=False), desc=f\"Dataset ({mode})\"):\n",
    "            if mode == 'train' and len(group) < 2: continue\n",
    "            seq = make_features(group)\n",
    "            team_id = group.iloc[0]['team_id']\n",
    "            if mode == 'train':\n",
    "                last = group.iloc[-1]\n",
    "                self.targets.append([last['end_x']/105.0, last['end_y']/68.0])\n",
    "                self.episodes.append(seq)\n",
    "                self.team_ids.append(team_id)\n",
    "            else:\n",
    "                self.episodes.append(seq)\n",
    "                self.team_ids.append(team_id)\n",
    "                self.episode_ids.append(str(name))\n",
    "\n",
    "    def __len__(self): return len(self.episodes)\n",
    "    def __getitem__(self, idx):\n",
    "        seq = torch.FloatTensor(self.episodes[idx])\n",
    "        if len(seq) > MAX_SEQ_LEN: seq = seq[-MAX_SEQ_LEN:]\n",
    "        if self.mode == 'train':\n",
    "            return seq, torch.FloatTensor(self.targets[idx]), self.team_ids[idx]\n",
    "        return seq, self.team_ids[idx], self.episode_ids[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seqs = [b[0] for b in batch]\n",
    "    lens = torch.LongTensor([len(s) for s in seqs])\n",
    "    padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    mask = torch.arange(padded.size(1))[None, :] >= lens[:, None]\n",
    "    if isinstance(batch[0][2], str):\n",
    "        return padded, mask, lens, torch.LongTensor([b[1] for b in batch]), [b[2] for b in batch]\n",
    "    return padded, torch.stack([b[1] for b in batch]), mask, lens, torch.LongTensor([b[2] for b in batch])\n",
    "\n",
    "full_dataset = SoccerDataset(train_df, mode='train')\n",
    "test_dataset = SoccerDataset(test_df, mode='test')\n",
    "print(f\"âœ… Dataset: {len(full_dataset)} train, {len(test_dataset)} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "âœ… [V11-Improved] MDN ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ\n",
      "======================================================================\n",
      "ì£¼ìš” ê°œì„ ì‚¬í•­:\n",
      "  1. Gaussians: 2ê°œ (ë‹¨ìˆœí™”)\n",
      "  2. Mu: Sigmoidë¡œ 0~1 ë³´ì¥\n",
      "  3. Sigma: [0.01, 0.3] ì œí•œ\n",
      "  4. Loss: Hybrid (NLL + 0.3*MSE)\n",
      "  5. ì˜ˆì¸¡: mean (ê°€ì¤‘í‰ê· )\n",
      "  6. LR: 1e-3 (V10 ìˆ˜ì¤€)\n",
      "  Parameters: 4,351,949\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# [V11-Improved] MDN ëª¨ë¸ (ê°œì„ )\n",
    "# ======================================================\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.goal_attn = nn.Sequential(nn.Linear(3, 16), nn.ReLU(), nn.Linear(16, 1))\n",
    "        self.zone_attn = nn.Sequential(nn.Linear(3, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "        self.pos_attn = nn.Sequential(nn.Linear(2, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "        self.fusion = nn.Linear(3, 1)\n",
    "    def forward(self, x):\n",
    "        return self.fusion(torch.cat([self.pos_attn(x[..., 0:2]), \n",
    "                                       self.goal_attn(x[..., 11:14]), \n",
    "                                       self.zone_attn(x[..., 16:19])], dim=-1))\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 100, hidden_dim) * 0.02)\n",
    "        self.temporal_attn = nn.Sequential(nn.Linear(hidden_dim, hidden_dim // 2), \n",
    "                                           nn.Tanh(), nn.Dropout(0.1), \n",
    "                                           nn.Linear(hidden_dim // 2, 1))\n",
    "    def forward(self, lstm_out):\n",
    "        return self.temporal_attn(lstm_out + self.pos_encoding[:, :lstm_out.size(1), :])\n",
    "\n",
    "class SpatialTemporalFusion(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.spatial_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        self.temporal_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        self.combine = nn.Sequential(nn.Linear(2, 8), nn.ReLU(), nn.Linear(8, 1), nn.Sigmoid())\n",
    "    def forward(self, s, t):\n",
    "        return (torch.sigmoid(self.spatial_weight) * s + \n",
    "                torch.sigmoid(self.temporal_weight) * t) * self.combine(torch.cat([s, t], -1))\n",
    "\n",
    "class ImprovedMDNPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    [V11-Improved] ê°œì„ ëœ MDN\n",
    "    - Gaussian ìˆ˜ ê°ì†Œ (2ê°œ)\n",
    "    - Sigma ë²”ìœ„ ì œí•œ\n",
    "    - ì´ˆê¸°í™” ê°œì„ \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout, num_gaussians=2, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.num_gaussians = num_gaussians\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                           dropout=dropout if num_layers > 1 else 0, bidirectional=bidirectional)\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        \n",
    "        self.spatial_attn = SpatialAttention(lstm_output_dim)\n",
    "        self.temporal_attn = TemporalAttention(lstm_output_dim)\n",
    "        self.fusion = SpatialTemporalFusion(lstm_output_dim)\n",
    "        \n",
    "        # MDN Heads (ë” ê°•ë ¥í•œ ì´ˆê¸°í™”)\n",
    "        self.pi_head = nn.Sequential(\n",
    "            nn.Linear(lstm_output_dim, lstm_output_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_output_dim // 2, num_gaussians)\n",
    "        )\n",
    "        self.mu_head = nn.Sequential(\n",
    "            nn.Linear(lstm_output_dim, lstm_output_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_output_dim // 2, num_gaussians * 2)\n",
    "        )\n",
    "        self.sigma_head = nn.Sequential(\n",
    "            nn.Linear(lstm_output_dim, lstm_output_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_output_dim // 2, num_gaussians * 2)\n",
    "        )\n",
    "        \n",
    "        # âœ… Mu ì´ˆê¸°í™”: ì¤‘ì•™ ê·¼ì²˜ bias\n",
    "        nn.init.constant_(self.mu_head[-1].bias, 0.5)\n",
    "        nn.init.xavier_uniform_(self.mu_head[-1].weight, gain=0.1)\n",
    "\n",
    "    def forward(self, x, mask=None, lengths=None):\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        \n",
    "        # LSTM + Attention\n",
    "        if lengths is not None:\n",
    "            packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            lstm_out, _ = self.lstm(packed)\n",
    "            lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True, total_length=seq_len)\n",
    "        else:\n",
    "            lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        fused_attn = self.fusion(self.spatial_attn(x), self.temporal_attn(lstm_out))\n",
    "        if mask is not None:\n",
    "            fused_attn = fused_attn.masked_fill(mask.unsqueeze(-1), float('-inf'))\n",
    "        final_attn = torch.softmax(fused_attn, dim=1)\n",
    "        context = torch.sum(lstm_out * final_attn, dim=1)\n",
    "        \n",
    "        # MDN ì¶œë ¥\n",
    "        pi = torch.softmax(self.pi_head(context), dim=1)\n",
    "        mu = torch.sigmoid(self.mu_head(context)).view(batch_size, self.num_gaussians, 2)  # âœ… Sigmoidë¡œ 0~1 ê°•ì œ\n",
    "        \n",
    "        # âœ… Sigma ë²”ìœ„ ì œí•œ: [MIN_SIGMA, MAX_SIGMA]\n",
    "        sigma_raw = self.sigma_head(context).view(batch_size, self.num_gaussians, 2)\n",
    "        sigma = torch.sigmoid(sigma_raw) * (MAX_SIGMA - MIN_SIGMA) + MIN_SIGMA\n",
    "        \n",
    "        return pi, mu, sigma\n",
    "\n",
    "\n",
    "def hybrid_mdn_loss(pi, mu, sigma, target, mse_weight=HYBRID_LOSS_WEIGHT):\n",
    "    \"\"\"\n",
    "    âœ… Hybrid Loss: NLL + MSE\n",
    "    - NLL: í™•ë¥  ë¶„í¬ í•™ìŠµ\n",
    "    - MSE: ì§ì ‘ì ì¸ ì¢Œí‘œ í•™ìŠµ (ì•ˆì •ì„±)\n",
    "    \"\"\"\n",
    "    batch_size = target.size(0)\n",
    "    target_expanded = target.unsqueeze(1).expand_as(mu)\n",
    "    \n",
    "    # NLL Loss\n",
    "    diff = target_expanded - mu\n",
    "    log_prob_components = (\n",
    "        -0.5 * math.log(2 * math.pi) - torch.log(sigma) - 0.5 * (diff / sigma) ** 2\n",
    "    )\n",
    "    log_prob = log_prob_components.sum(dim=2)\n",
    "    weighted_log_prob = log_prob + torch.log(pi + 1e-8)\n",
    "    nll_loss = -torch.logsumexp(weighted_log_prob, dim=1).mean()\n",
    "    \n",
    "    # âœ… MSE Loss (ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡)\n",
    "    pred_mean = (pi.unsqueeze(-1) * mu).sum(dim=1)\n",
    "    mse_loss = nn.functional.mse_loss(pred_mean, target)\n",
    "    \n",
    "    # Hybrid\n",
    "    total_loss = nll_loss + mse_weight * mse_loss\n",
    "    \n",
    "    return total_loss, nll_loss, mse_loss\n",
    "\n",
    "\n",
    "def mdn_predict_improved(pi, mu, sigma, strategy='mean'):\n",
    "    \"\"\"\n",
    "    âœ… ê°œì„ ëœ ì˜ˆì¸¡: ê¸°ë³¸ ì „ëµì„ meanìœ¼ë¡œ ë³€ê²½\n",
    "    - mean: ë” ì•ˆì •ì ì´ê³  ëª¨ë“  ì˜µì…˜ ê³ ë ¤\n",
    "    \"\"\"\n",
    "    if strategy == 'mode':\n",
    "        max_idx = torch.argmax(pi, dim=1)\n",
    "        pred = mu[torch.arange(len(mu)), max_idx]\n",
    "    elif strategy == 'mean':\n",
    "        pred = (pi.unsqueeze(-1) * mu).sum(dim=1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "    \n",
    "    return torch.clamp(pred, 0.0, 1.0)\n",
    "\n",
    "\n",
    "model = ImprovedMDNPredictor(INPUT_DIM, HIDDEN_DIM, LSTM_LAYERS, DROPOUT, NUM_GAUSSIANS, BIDIRECTIONAL).to(DEVICE)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âœ… [V11-Improved] MDN ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ì£¼ìš” ê°œì„ ì‚¬í•­:\")\n",
    "print(f\"  1. Gaussians: 2ê°œ (ë‹¨ìˆœí™”)\")\n",
    "print(f\"  2. Mu: Sigmoidë¡œ 0~1 ë³´ì¥\")\n",
    "print(f\"  3. Sigma: [{MIN_SIGMA}, {MAX_SIGMA}] ì œí•œ\")\n",
    "print(f\"  4. Loss: Hybrid (NLL + {HYBRID_LOSS_WEIGHT}*MSE)\")\n",
    "print(f\"  5. ì˜ˆì¸¡: mean (ê°€ì¤‘í‰ê· )\")\n",
    "print(f\"  6. LR: 1e-3 (V10 ìˆ˜ì¤€)\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": "# ======================================================\n# ğŸ” Bayesian Optimization + ìµœì  íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ\n# ======================================================\nimport optuna\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\"*70)\nprint(\"ğŸ” Step 1: Bayesian Optimizationìœ¼ë¡œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì°¾ê¸°\")\nprint(\"=\"*70)\n\n# ======================================================\n# OPTUNA OBJECTIVE FUNCTION\n# ======================================================\ndef objective(trial):\n    \"\"\"í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\"\"\"\n    # 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§\n    num_gaussians = trial.suggest_int(\"num_gaussians\", 2, 5)\n    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [128, 256, 512])\n    lstm_layers = trial.suggest_int(\"lstm_layers\", 2, 4)\n    dropout = trial.suggest_float(\"dropout\", 0.1, 0.4)\n    lr = trial.suggest_float(\"lr\", 5e-4, 5e-3, log=True)\n    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n    min_sigma = trial.suggest_float(\"min_sigma\", 0.005, 0.02)\n    max_sigma = trial.suggest_float(\"max_sigma\", 0.2, 0.5)\n    hybrid_loss_weight = trial.suggest_float(\"hybrid_loss_weight\", 0.1, 0.5)\n    \n    # 2. ë°ì´í„° ë¡œë”\n    seed_everything(42)\n    train_idx, val_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n    train_loader = DataLoader(torch.utils.data.Subset(full_dataset, train_idx), \n                              batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(torch.utils.data.Subset(full_dataset, val_idx), \n                           batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n    \n    # 3. ëª¨ë¸ ìƒì„±\n    model_trial = ImprovedMDNPredictor(\n        INPUT_DIM, hidden_dim, lstm_layers, dropout, num_gaussians, BIDIRECTIONAL\n    ).to(DEVICE)\n    \n    optimizer = optim.AdamW(model_trial.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n    \n    # 4. ë¹ ë¥¸ í•™ìŠµ (20 epochs)\n    OPTUNA_EPOCHS = 20\n    best_dist = float('inf')\n    patience, patience_counter = 5, 0\n    \n    for epoch in range(OPTUNA_EPOCHS):\n        model_trial.train()\n        for seqs, targets, mask, lens, _ in train_loader:\n            seqs, targets, mask, lens = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n            optimizer.zero_grad()\n            pi, mu, sigma = model_trial(seqs, mask, lens)\n            sigma = torch.clamp(sigma, min_sigma, max_sigma)\n            \n            # Loss\n            target_expanded = targets.unsqueeze(1).expand_as(mu)\n            diff = target_expanded - mu\n            log_prob_components = -0.5 * math.log(2 * math.pi) - torch.log(sigma) - 0.5 * (diff / sigma) ** 2\n            log_prob = log_prob_components.sum(dim=2)\n            weighted_log_prob = log_prob + torch.log(pi + 1e-8)\n            nll_loss = -torch.logsumexp(weighted_log_prob, dim=1).mean()\n            pred_mean = (pi.unsqueeze(-1) * mu).sum(dim=1)\n            mse_loss = nn.functional.mse_loss(pred_mean, targets)\n            loss = nll_loss + hybrid_loss_weight * mse_loss\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model_trial.parameters(), GRAD_CLIP)\n            optimizer.step()\n        \n        # Validation\n        model_trial.eval()\n        dists = []\n        with torch.no_grad():\n            for seqs, targets, mask, lens, _ in val_loader:\n                seqs, targets, mask, lens = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n                pi, mu, sigma = model_trial(seqs, mask, lens)\n                pred = mdn_predict_improved(pi, mu, sigma, strategy='mean')\n                p_real = pred.cpu().numpy() * np.array([105.0, 68.0])\n                t_real = targets.cpu().numpy() * np.array([105.0, 68.0])\n                dists.extend(np.sqrt(np.sum((p_real - t_real)**2, axis=1)))\n        \n        avg_dist = np.mean(dists)\n        scheduler.step()\n        \n        # Early stopping\n        if avg_dist < best_dist:\n            best_dist = avg_dist\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                break\n        \n        # Optuna pruning\n        trial.report(avg_dist, epoch)\n        if trial.should_prune():\n            raise optuna.TrialPruned()\n    \n    return best_dist\n\n\n# ======================================================\n# OPTUNA ì‹¤í–‰\n# ======================================================\nprint(\"\\nğŸ“Š íƒìƒ‰ ê³µê°„:\")\nprint(\"  - num_gaussians: 2~5, hidden_dim: [128,256,512], lstm_layers: 2~4\")\nprint(\"  - dropout: 0.1~0.4, lr: 5e-4~5e-3, batch_size: [32,64,128]\")\nprint(\"  - min_sigma: 0.005~0.02, max_sigma: 0.2~0.5, hybrid_weight: 0.1~0.5\")\nprint(\"\\nâ±ï¸  ì˜ˆìƒ ì‹œê°„: 2-3ì‹œê°„ (30 trials x 5ë¶„)\")\nprint(\"=\"*70 + \"\\n\")\n\nstudy = optuna.create_study(\n    direction=\"minimize\",\n    pruner=optuna.pruners.MedianPruner(n_startup_trials=5),\n    sampler=optuna.samplers.TPESampler(seed=42)\n)\n\nstudy.optimize(objective, n_trials=30, show_progress_bar=True)\n\n# ======================================================\n# ê²°ê³¼ ì¶œë ¥\n# ======================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… Optuna ìµœì í™” ì™„ë£Œ!\")\nprint(\"=\"*70)\nprint(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥: {study.best_value:.4f}m\")\nprint(f\"\\nğŸ“Œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\nfor key, value in study.best_params.items():\n    print(f\"  - {key}: {value}\")\n\n# ì €ì¥\nwith open('best_hyperparameters.json', 'w', encoding='utf-8') as f:\n    json.dump({'best_value': study.best_value, 'best_params': study.best_params, \n               'n_trials': len(study.trials)}, f, indent=2, ensure_ascii=False)\n\n# ======================================================\n# Step 2: ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ\n# ======================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸš€ Step 2: ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ (50 epochs)\")\nprint(\"=\"*70)\n\nbest_params = study.best_params\nSEEDS = [42, 2024, 777]\nPRED_STRATEGY = 'mean'\nall_histories = []\n\nfor i, seed in enumerate(SEEDS):\n    print(f\"\\n{'='*70}\")\n    print(f\"ğŸ“¦ [ëª¨ë¸ {i+1}/3] Seed {seed}\")\n    print(f\"{'='*70}\")\n    seed_everything(seed)\n    \n    train_idx, val_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=seed)\n    train_loader = DataLoader(torch.utils.data.Subset(full_dataset, train_idx), \n                              batch_size=best_params['batch_size'], shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(torch.utils.data.Subset(full_dataset, val_idx), \n                           batch_size=best_params['batch_size'], shuffle=False, collate_fn=collate_fn)\n    \n    # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„±\n    model = ImprovedMDNPredictor(\n        INPUT_DIM, best_params['hidden_dim'], best_params['lstm_layers'], \n        best_params['dropout'], best_params['num_gaussians'], BIDIRECTIONAL\n    ).to(DEVICE)\n    \n    optimizer = optim.AdamW(model.parameters(), lr=best_params['lr'], weight_decay=1e-4)\n    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS_BASE - WARMUP_EPOCHS, eta_min=1e-6)\n    \n    best_dist = float('inf')\n    history = {'train_loss': [], 'train_nll': [], 'train_mse': [], 'val_dist': [], 'lr': []}\n    \n    for epoch in range(EPOCHS_BASE):\n        # Warmup\n        if epoch < WARMUP_EPOCHS:\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = best_params['lr'] * (epoch + 1) / WARMUP_EPOCHS\n        \n        # Training\n        model.train()\n        train_loss, train_nll, train_mse = 0.0, 0.0, 0.0\n        for seqs, targets, mask, lens, _ in train_loader:\n            seqs, targets, mask, lens = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n            optimizer.zero_grad()\n            pi, mu, sigma = model(seqs, mask, lens)\n            sigma = torch.clamp(sigma, best_params['min_sigma'], best_params['max_sigma'])\n            \n            # Loss\n            target_expanded = targets.unsqueeze(1).expand_as(mu)\n            diff = target_expanded - mu\n            log_prob_components = -0.5 * math.log(2 * math.pi) - torch.log(sigma) - 0.5 * (diff / sigma) ** 2\n            log_prob = log_prob_components.sum(dim=2)\n            weighted_log_prob = log_prob + torch.log(pi + 1e-8)\n            nll_loss = -torch.logsumexp(weighted_log_prob, dim=1).mean()\n            pred_mean = (pi.unsqueeze(-1) * mu).sum(dim=1)\n            mse_loss = nn.functional.mse_loss(pred_mean, targets)\n            loss = nll_loss + best_params['hybrid_loss_weight'] * mse_loss\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n            optimizer.step()\n            train_loss += loss.item()\n            train_nll += nll_loss.item()\n            train_mse += mse_loss.item()\n        \n        # Validation\n        model.eval()\n        dists = []\n        with torch.no_grad():\n            for seqs, targets, mask, lens, _ in val_loader:\n                seqs, targets, mask, lens = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n                pi, mu, sigma = model(seqs, mask, lens)\n                pred = mdn_predict_improved(pi, mu, sigma, strategy=PRED_STRATEGY)\n                p_real = pred.cpu().numpy() * np.array([105.0, 68.0])\n                t_real = targets.cpu().numpy() * np.array([105.0, 68.0])\n                dists.extend(np.sqrt(np.sum((p_real - t_real)**2, axis=1)))\n        \n        avg_dist = np.mean(dists)\n        avg_loss = train_loss / len(train_loader)\n        avg_nll = train_nll / len(train_loader)\n        avg_mse = train_mse / len(train_loader)\n        \n        if epoch >= WARMUP_EPOCHS:\n            scheduler.step()\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        history['train_loss'].append(avg_loss)\n        history['train_nll'].append(avg_nll)\n        history['train_mse'].append(avg_mse)\n        history['val_dist'].append(avg_dist)\n        history['lr'].append(current_lr)\n        \n        if (epoch + 1) % 5 == 0 or epoch < 5:\n            print(f\"  [Epoch {epoch+1:2d}/{EPOCHS_BASE}] \"\n                  f\"Loss: {avg_loss:.4f} (NLL: {avg_nll:.4f}, MSE: {avg_mse:.4f}) | \"\n                  f\"Val: {avg_dist:.4f}m | LR: {current_lr:.6f}\")\n        \n        if avg_dist < best_dist:\n            best_dist = avg_dist\n            torch.save(model.state_dict(), f'optimized_mdn_{i}.pth')\n    \n    history['best_dist'] = best_dist\n    history['seed'] = seed\n    all_histories.append(history)\n    print(f\"  âœ… Best: {best_dist:.4f}m\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"âœ… ìµœì¢… í•™ìŠµ ì™„ë£Œ!\")\nprint(f\"{'='*70}\")\nprint(f\"í‰ê·  Best: {np.mean([h['best_dist'] for h in all_histories]):.4f}m\")\nprint(f\"ìµœê³  ì„±ëŠ¥: {min([h['best_dist'] for h in all_histories]):.4f}m\")\nprint(f\"{'='*70}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference",
   "metadata": {},
   "outputs": [],
   "source": "# ======================================================\n# ì¶”ë¡  (ìµœì í™”ëœ ëª¨ë¸ ì‚¬ìš©)\n# ======================================================\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n\nmodels = []\nfor i in range(3):\n    model = ImprovedMDNPredictor(\n        INPUT_DIM, \n        best_params['hidden_dim'], \n        best_params['lstm_layers'], \n        best_params['dropout'], \n        best_params['num_gaussians'], \n        BIDIRECTIONAL\n    ).to(DEVICE)\n    model.load_state_dict(torch.load(f'optimized_mdn_{i}.pth'))  # âœ… ìµœì í™”ëœ ëª¨ë¸ ë¡œë“œ\n    model.eval()\n    models.append(model)\n\nresults = []\nwith torch.no_grad():\n    for seqs, mask, lens, team_ids, episode_ids in tqdm(test_loader, desc=\"Inference\"):\n        seqs, mask, lens = seqs.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n        preds = []\n        for model in models:\n            pi, mu, sigma = model(seqs, mask, lens)\n            pred = mdn_predict_improved(pi, mu, sigma, strategy='mean')\n            preds.append(pred.cpu().numpy())\n        avg_pred = np.mean(preds, axis=0)\n        for i, eid in enumerate(episode_ids):\n            results.append({'game_episode': eid, 'pred_x': avg_pred[i, 0] * 105.0, \n                          'pred_y': avg_pred[i, 1] * 68.0})\n\npred_df = pd.DataFrame(results)\nSUBMISSION_PATH = \"open_track1/sample_submission.csv\"\nif os.path.exists(SUBMISSION_PATH):\n    sub = pd.read_csv(SUBMISSION_PATH)\nelse:\n    sub = pd.read_csv(TEST_META_PATH).rename(columns={'episode_id': 'game_episode'})[['game_episode']]\n\nfinal_sub = pd.merge(sub[['game_episode']], pred_df, on='game_episode', how='left')\nfinal_sub = final_sub.rename(columns={'pred_x': 'end_x', 'pred_y': 'end_y'})\nif final_sub.isnull().sum().sum() > 0:\n    final_sub = final_sub.fillna(50.0)\n\nfilename = \"optimized_submission.csv\"\nfinal_sub.to_csv(filename, index=False)\n\nprint(f\"\\nâœ… ì œì¶œ íŒŒì¼: {filename}\")\nprint(f\"ğŸ“Œ ì‚¬ìš©ëœ ìµœì  íŒŒë¼ë¯¸í„°:\")\nfor key, value in best_params.items():\n    print(f\"  - {key}: {value}\")\nprint(final_sub.head())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('[V11-Improved] MDN í•™ìŠµ ê³¡ì„ ', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "# 1. Hybrid Loss\n",
    "ax1 = axes[0, 0]\n",
    "for i, h in enumerate(all_histories):\n",
    "    epochs = range(1, len(h['train_loss']) + 1)\n",
    "    ax1.plot(epochs, h['train_loss'], label=f\"Model {i+1}\", color=colors[i], linewidth=2)\n",
    "ax1.set_title('Training Loss (Hybrid: NLL + MSE)')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Validation Distance\n",
    "ax2 = axes[0, 1]\n",
    "for i, h in enumerate(all_histories):\n",
    "    epochs = range(1, len(h['val_dist']) + 1)\n",
    "    ax2.plot(epochs, h['val_dist'], label=f\"Model {i+1}\", color=colors[i], linewidth=2)\n",
    "    best_idx = np.argmin(h['val_dist'])\n",
    "    ax2.scatter(best_idx + 1, h['val_dist'][best_idx], color=colors[i], s=100, marker='*')\n",
    "ax2.set_title(f'Validation Distance ({PRED_STRATEGY})')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Distance (m)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. NLL vs MSE\n",
    "ax3 = axes[1, 0]\n",
    "h = all_histories[0]\n",
    "epochs = range(1, len(h['train_nll']) + 1)\n",
    "ax3.plot(epochs, h['train_nll'], label='NLL', color='red', linewidth=2)\n",
    "ax3_twin = ax3.twinx()\n",
    "ax3_twin.plot(epochs, h['train_mse'], label='MSE', color='blue', linewidth=2)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('NLL', color='red')\n",
    "ax3_twin.set_ylabel('MSE', color='blue')\n",
    "ax3.set_title('Loss Components (Model 1)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Best Distances\n",
    "ax4 = axes[1, 1]\n",
    "best_dists = [h['best_dist'] for h in all_histories]\n",
    "bars = ax4.bar(range(len(best_dists)), best_dists, color=colors)\n",
    "ax4.set_xticks(range(len(best_dists)))\n",
    "ax4.set_xticklabels([f'Model {i+1}' for i in range(len(best_dists))])\n",
    "ax4.set_ylabel('Best Distance (m)')\n",
    "ax4.set_title('Best Performance')\n",
    "for bar, dist in zip(bars, best_dists):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height(), \n",
    "             f'{dist:.4f}m', ha='center', va='bottom', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('v11_improved_mdn_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nìµœì¢… ê²°ê³¼:\")\n",
    "for i, h in enumerate(all_histories):\n",
    "    print(f\"  Model {i+1}: {h['best_dist']:.4f}m\")\n",
    "print(f\"  í‰ê· : {np.mean(best_dists):.4f}m\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "b62x9e115f4",
   "source": "# ======================================================\n# ğŸ” Bayesian Optimizationìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n# ======================================================\nimport optuna\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\"*70)\nprint(\"ğŸ” Bayesian Optimization í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘\")\nprint(\"=\"*70)\n\n# Optuna Objective Function\ndef objective(trial):\n    \"\"\"\n    Optunaê°€ ìµœì í™”í•  ëª©ì  í•¨ìˆ˜\n    Trialë§ˆë‹¤ ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  validation ì„±ëŠ¥ ë°˜í™˜\n    \"\"\"\n    \n    # ======================================================\n    # 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§\n    # ======================================================\n    num_gaussians = trial.suggest_int(\"num_gaussians\", 2, 5)\n    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [128, 256, 512])\n    lstm_layers = trial.suggest_int(\"lstm_layers\", 2, 4)\n    dropout = trial.suggest_float(\"dropout\", 0.1, 0.4)\n    lr = trial.suggest_float(\"lr\", 5e-4, 5e-3, log=True)\n    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n    min_sigma = trial.suggest_float(\"min_sigma\", 0.005, 0.02)\n    max_sigma = trial.suggest_float(\"max_sigma\", 0.2, 0.5)\n    hybrid_loss_weight = trial.suggest_float(\"hybrid_loss_weight\", 0.1, 0.5)\n    \n    # ======================================================\n    # 2. ëª¨ë¸ ìƒì„± (ìƒ˜í”Œë§ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©)\n    # ======================================================\n    seed_everything(42)  # ê³ ì • seedë¡œ ê³µì •í•œ ë¹„êµ\n    \n    # Train/Val split\n    train_idx, val_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n    train_loader = DataLoader(torch.utils.data.Subset(full_dataset, train_idx), \n                              batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(torch.utils.data.Subset(full_dataset, val_idx), \n                           batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n    \n    # ëª¨ë¸ ìƒì„±\n    model_trial = ImprovedMDNPredictor(\n        INPUT_DIM, hidden_dim, lstm_layers, dropout, \n        num_gaussians, BIDIRECTIONAL\n    ).to(DEVICE)\n    \n    optimizer = optim.AdamW(model_trial.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n    \n    # ======================================================\n    # 3. í•™ìŠµ (ë¹ ë¥¸ í‰ê°€ë¥¼ ìœ„í•´ epochs ì¤„ì„)\n    # ======================================================\n    OPTUNA_EPOCHS = 20  # Trialë§ˆë‹¤ 20 epochsë§Œ (ë¹ ë¥¸ í‰ê°€)\n    best_dist = float('inf')\n    patience = 5\n    patience_counter = 0\n    \n    for epoch in range(OPTUNA_EPOCHS):\n        # Training\n        model_trial.train()\n        train_loss = 0.0\n        for seqs, targets, mask, lens, _ in train_loader:\n            seqs, targets, mask, lens = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n            optimizer.zero_grad()\n            pi, mu, sigma = model_trial(seqs, mask, lens)\n            \n            # Sigma ë²”ìœ„ ì œí•œ ì ìš©\n            sigma = torch.clamp(sigma, min_sigma, max_sigma)\n            \n            # Loss ê³„ì‚°\n            target_expanded = targets.unsqueeze(1).expand_as(mu)\n            diff = target_expanded - mu\n            log_prob_components = (\n                -0.5 * math.log(2 * math.pi) - torch.log(sigma) - 0.5 * (diff / sigma) ** 2\n            )\n            log_prob = log_prob_components.sum(dim=2)\n            weighted_log_prob = log_prob + torch.log(pi + 1e-8)\n            nll_loss = -torch.logsumexp(weighted_log_prob, dim=1).mean()\n            \n            pred_mean = (pi.unsqueeze(-1) * mu).sum(dim=1)\n            mse_loss = nn.functional.mse_loss(pred_mean, targets)\n            \n            loss = nll_loss + hybrid_loss_weight * mse_loss\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model_trial.parameters(), GRAD_CLIP)\n            optimizer.step()\n            train_loss += loss.item()\n        \n        # Validation\n        model_trial.eval()\n        dists = []\n        with torch.no_grad():\n            for seqs, targets, mask, lens, _ in val_loader:\n                seqs, targets, mask, lens = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n                pi, mu, sigma = model_trial(seqs, mask, lens)\n                pred = mdn_predict_improved(pi, mu, sigma, strategy='mean')\n                p_real = pred.cpu().numpy() * np.array([105.0, 68.0])\n                t_real = targets.cpu().numpy() * np.array([105.0, 68.0])\n                dists.extend(np.sqrt(np.sum((p_real - t_real)**2, axis=1)))\n        \n        avg_dist = np.mean(dists)\n        scheduler.step()\n        \n        # Early stopping\n        if avg_dist < best_dist:\n            best_dist = avg_dist\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                break\n        \n        # Optuna pruning (ì„±ëŠ¥ ì•ˆ ì¢‹ì€ trial ì¡°ê¸° ì¢…ë£Œ)\n        trial.report(avg_dist, epoch)\n        if trial.should_prune():\n            raise optuna.TrialPruned()\n    \n    return best_dist\n\n\n# ======================================================\n# Optuna Study ì‹¤í–‰\n# ======================================================\nprint(\"\\nğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„:\")\nprint(\"  - num_gaussians: 2 ~ 5\")\nprint(\"  - hidden_dim: [128, 256, 512]\")\nprint(\"  - lstm_layers: 2 ~ 4\")\nprint(\"  - dropout: 0.1 ~ 0.4\")\nprint(\"  - learning_rate: 5e-4 ~ 5e-3\")\nprint(\"  - batch_size: [32, 64, 128]\")\nprint(\"  - min_sigma: 0.005 ~ 0.02\")\nprint(\"  - max_sigma: 0.2 ~ 0.5\")\nprint(\"  - hybrid_loss_weight: 0.1 ~ 0.5\")\nprint(\"\\nâ±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 2-3ì‹œê°„ (30 trials)\")\nprint(\"=\"*70)\n\n# Study ìƒì„± ë° ì‹¤í–‰\nstudy = optuna.create_study(\n    direction=\"minimize\",  # validation distance ìµœì†Œí™”\n    pruner=optuna.pruners.MedianPruner(n_startup_trials=5),  # ì„±ëŠ¥ ì•ˆ ì¢‹ì€ trial ì¡°ê¸° ì¢…ë£Œ\n    sampler=optuna.samplers.TPESampler(seed=42)  # Bayesian Optimization\n)\n\n# ìµœì í™” ì‹¤í–‰ (30 trials)\nstudy.optimize(objective, n_trials=30, show_progress_bar=True)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0j3lhbdxg43e",
   "source": "# ======================================================\n# ğŸ“Š ìµœì í™” ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”\n# ======================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… ìµœì í™” ì™„ë£Œ!\")\nprint(\"=\"*70)\n\nprint(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥: {study.best_value:.4f}m\")\nprint(f\"\\nğŸ“Œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\nfor key, value in study.best_params.items():\n    print(f\"  - {key}: {value}\")\n\nprint(f\"\\nğŸ“ˆ Trial í†µê³„:\")\nprint(f\"  - ì™„ë£Œëœ trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")\nprint(f\"  - Pruned trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\")\n\n# ìƒìœ„ 5ê°œ trial\nprint(f\"\\nğŸ” ìƒìœ„ 5ê°œ Trials:\")\nsorted_trials = sorted(study.trials, key=lambda t: t.value if t.value is not None else float('inf'))\nfor i, trial in enumerate(sorted_trials[:5]):\n    print(f\"  {i+1}. Trial {trial.number}: {trial.value:.4f}m\")\n\n# ======================================================\n# ì‹œê°í™”\n# ======================================================\nfig = plt.figure(figsize=(15, 5))\n\n# 1. Optimization History\nax1 = plt.subplot(1, 3, 1)\nvalues = [t.value for t in study.trials if t.value is not None]\nax1.plot(values, marker='o', linestyle='-', alpha=0.6)\nax1.axhline(study.best_value, color='r', linestyle='--', label=f'Best: {study.best_value:.4f}m')\nax1.set_xlabel('Trial')\nax1.set_ylabel('Validation Distance (m)')\nax1.set_title('Optimization History')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 2. Parameter Importances\ntry:\n    importances = optuna.importance.get_param_importances(study)\n    ax2 = plt.subplot(1, 3, 2)\n    params = list(importances.keys())[:8]  # ìƒìœ„ 8ê°œ\n    imp_values = [importances[p] for p in params]\n    ax2.barh(params, imp_values)\n    ax2.set_xlabel('Importance')\n    ax2.set_title('Parameter Importances')\n    ax2.grid(True, alpha=0.3, axis='x')\nexcept Exception as e:\n    print(f\"Parameter importance ê³„ì‚° ì‹¤íŒ¨: {e}\")\n\n# 3. Best vs Average per epoch\nax3 = plt.subplot(1, 3, 3)\nall_intermediate = []\nfor trial in study.trials:\n    if trial.state == optuna.trial.TrialState.COMPLETE:\n        for step, value in trial.intermediate_values.items():\n            all_intermediate.append((step, value))\n\nif all_intermediate:\n    from collections import defaultdict\n    epoch_values = defaultdict(list)\n    for step, value in all_intermediate:\n        epoch_values[step].append(value)\n    \n    epochs = sorted(epoch_values.keys())\n    avg_values = [np.mean(epoch_values[e]) for e in epochs]\n    min_values = [np.min(epoch_values[e]) for e in epochs]\n    \n    ax3.plot(epochs, avg_values, label='Average', marker='o')\n    ax3.plot(epochs, min_values, label='Best', marker='s')\n    ax3.set_xlabel('Epoch')\n    ax3.set_ylabel('Validation Distance (m)')\n    ax3.set_title('Learning Curves')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('optuna_optimization_results.png', dpi=150)\nplt.show()\n\nprint(f\"\\nğŸ’¾ ì‹œê°í™” ì €ì¥: optuna_optimization_results.png\")\n\n# ======================================================\n# ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥\n# ======================================================\nwith open('best_hyperparameters.json', 'w', encoding='utf-8') as f:\n    json.dump({\n        'best_value': study.best_value,\n        'best_params': study.best_params,\n        'n_trials': len(study.trials)\n    }, f, indent=2, ensure_ascii=False)\n\nprint(f\"ğŸ’¾ ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥: best_hyperparameters.json\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0hplit2z3rdt",
   "source": "# ======================================================\n# ğŸš€ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ\n# ======================================================\n\nprint(\"=\"*70)\nprint(\"ğŸš€ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ\")\nprint(\"=\"*70)\n\n# ìµœì  íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°\nbest_params = study.best_params\nprint(f\"\\nğŸ“Œ ì‚¬ìš©í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\nfor key, value in best_params.items():\n    print(f\"  - {key}: {value}\")\n\n# íŒŒë¼ë¯¸í„° ì ìš©\nNUM_GAUSSIANS_OPT = best_params['num_gaussians']\nHIDDEN_DIM_OPT = best_params['hidden_dim']\nLSTM_LAYERS_OPT = best_params['lstm_layers']\nDROPOUT_OPT = best_params['dropout']\nLR_OPT = best_params['lr']\nBATCH_SIZE_OPT = best_params['batch_size']\nMIN_SIGMA_OPT = best_params['min_sigma']\nMAX_SIGMA_OPT = best_params['max_sigma']\nHYBRID_LOSS_WEIGHT_OPT = best_params['hybrid_loss_weight']\n\n# 3ê°œ seedë¡œ ì•™ìƒë¸” í•™ìŠµ\nSEEDS_OPT = [42, 2024, 777]\nEPOCHS_OPT = 50\n\nall_histories_opt = []\n\nfor i, seed in enumerate(SEEDS_OPT):\n    print(f\"\\n{'='*70}\")\n    print(f\"ğŸ“¦ [ìµœì  ëª¨ë¸ {i+1}/3] Seed {seed}\")\n    print(f\"{'='*70}\")\n    seed_everything(seed)\n    \n    train_idx, val_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=seed)\n    train_loader = DataLoader(torch.utils.data.Subset(full_dataset, train_idx), \n                              batch_size=BATCH_SIZE_OPT, shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(torch.utils.data.Subset(full_dataset, val_idx), \n                           batch_size=BATCH_SIZE_OPT, shuffle=False, collate_fn=collate_fn)\n    \n    # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„±\n    model_opt = ImprovedMDNPredictor(\n        INPUT_DIM, HIDDEN_DIM_OPT, LSTM_LAYERS_OPT, DROPOUT_OPT, \n        NUM_GAUSSIANS_OPT, BIDIRECTIONAL\n    ).to(DEVICE)\n    \n    optimizer = optim.AdamW(model_opt.parameters(), lr=LR_OPT, weight_decay=1e-4)\n    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS_OPT - WARMUP_EPOCHS, eta_min=1e-6)\n    \n    best_dist = float('inf')\n    history = {'train_loss': [], 'val_dist': [], 'lr': []}\n    \n    for epoch in range(EPOCHS_OPT):\n        # Warmup\n        if epoch < WARMUP_EPOCHS:\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = LR_OPT * (epoch + 1) / WARMUP_EPOCHS\n        \n        # Training\n        model_opt.train()\n        train_loss = 0.0\n        for seqs, targets, mask, lens, _ in train_loader:\n            seqs, targets, mask, lens = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n            optimizer.zero_grad()\n            pi, mu, sigma = model_opt(seqs, mask, lens)\n            \n            # ìµœì  sigma ë²”ìœ„ ì ìš©\n            sigma = torch.clamp(sigma, MIN_SIGMA_OPT, MAX_SIGMA_OPT)\n            \n            # ìµœì  loss weight ì ìš©\n            target_expanded = targets.unsqueeze(1).expand_as(mu)\n            diff = target_expanded - mu\n            log_prob_components = (\n                -0.5 * math.log(2 * math.pi) - torch.log(sigma) - 0.5 * (diff / sigma) ** 2\n            )\n            log_prob = log_prob_components.sum(dim=2)\n            weighted_log_prob = log_prob + torch.log(pi + 1e-8)\n            nll_loss = -torch.logsumexp(weighted_log_prob, dim=1).mean()\n            \n            pred_mean = (pi.unsqueeze(-1) * mu).sum(dim=1)\n            mse_loss = nn.functional.mse_loss(pred_mean, targets)\n            \n            loss = nll_loss + HYBRID_LOSS_WEIGHT_OPT * mse_loss\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model_opt.parameters(), GRAD_CLIP)\n            optimizer.step()\n            train_loss += loss.item()\n        \n        # Validation\n        model_opt.eval()\n        dists = []\n        with torch.no_grad():\n            for seqs, targets, mask, lens, _ in val_loader:\n                seqs, targets, mask, lens = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n                pi, mu, sigma = model_opt(seqs, mask, lens)\n                pred = mdn_predict_improved(pi, mu, sigma, strategy='mean')\n                p_real = pred.cpu().numpy() * np.array([105.0, 68.0])\n                t_real = targets.cpu().numpy() * np.array([105.0, 68.0])\n                dists.extend(np.sqrt(np.sum((p_real - t_real)**2, axis=1)))\n        \n        avg_dist = np.mean(dists)\n        avg_loss = train_loss / len(train_loader)\n        \n        if epoch >= WARMUP_EPOCHS:\n            scheduler.step()\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        history['train_loss'].append(avg_loss)\n        history['val_dist'].append(avg_dist)\n        history['lr'].append(current_lr)\n        \n        if (epoch + 1) % 5 == 0 or epoch < 5:\n            print(f\"  [Epoch {epoch+1:2d}/{EPOCHS_OPT}] \"\n                  f\"Loss: {avg_loss:.4f} | Val: {avg_dist:.4f}m | LR: {current_lr:.6f}\")\n        \n        if avg_dist < best_dist:\n            best_dist = avg_dist\n            torch.save(model_opt.state_dict(), f'optimized_mdn_{i}.pth')\n    \n    history['best_dist'] = best_dist\n    history['seed'] = seed\n    all_histories_opt.append(history)\n    print(f\"  âœ… Best: {best_dist:.4f}m\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"âœ… ìµœì í™”ëœ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\nprint(f\"{'='*70}\")\nprint(f\"í‰ê·  Best: {np.mean([h['best_dist'] for h in all_histories_opt]):.4f}m\")\nprint(f\"ìµœê³  ì„±ëŠ¥: {min([h['best_dist'] for h in all_histories_opt]):.4f}m\")\nprint(f\"ê¸°ì¡´ V11 ëŒ€ë¹„ ê°œì„ : {14.46 - np.mean([h['best_dist'] for h in all_histories_opt]):.4f}m\")\nprint(f\"{'='*70}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daycon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}