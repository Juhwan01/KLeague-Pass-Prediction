{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d00b8",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\nimport random\nimport os\nimport math\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nSEED = 42\nseed_everything(SEED)\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {DEVICE}\")\n\n# ======================================================\n# [V7] ê¸¸ì´ë³„ ì•™ìƒë¸” ì „ëµ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n# ======================================================\nBATCH_SIZE = 64\nLR_BASE = 1e-3\nWARMUP_EPOCHS = 3\nEPOCHS_BASE = 40\nDROPOUT = 0.2\nMAX_SEQ_LEN = 30\nGRAD_CLIP = 1.0\n\n# Transformer íŒŒë¼ë¯¸í„°\nD_MODEL = 128\nNHEAD = 4\nNUM_LAYERS = 3\n\n# LSTM íŒŒë¼ë¯¸í„°\nHIDDEN_DIM = 256       # LSTM hidden dimension\nLSTM_LAYERS = 2        # LSTM ë ˆì´ì–´ ìˆ˜\nBIDIRECTIONAL = True   # ì–‘ë°©í–¥ LSTM\n\n# ê¸¸ì´ë³„ ì„ê³„ê°’ (ì´ë¡ ì— ê¸°ë°˜)\nSHORT_THRESHOLD = 15   # ì§§ì€ ì—í”¼ì†Œë“œ: <= 15\nLONG_THRESHOLD = 40    # ê¸´ ì—í”¼ì†Œë“œ: > 40\n\nprint(f\"[V7] ê¸¸ì´ë³„ ì•™ìƒë¸” ì „ëµ\")\nprint(f\"  - ì§§ì€ ì—í”¼ì†Œë“œ (<= {SHORT_THRESHOLD}): LSTMë§Œ ì‚¬ìš©\")\nprint(f\"  - ì¤‘ê°„ ì—í”¼ì†Œë“œ ({SHORT_THRESHOLD} ~ {LONG_THRESHOLD}): LSTM 60% + Transformer 40%\")\nprint(f\"  - ê¸´ ì—í”¼ì†Œë“œ (> {LONG_THRESHOLD}): Transformerë§Œ ì‚¬ìš©\")"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70de82db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train Loaded: (356721, 15)\n",
      "â„¹ï¸ Reading 2414 test files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Test CSVs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2414/2414 [00:06<00:00, 367.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test Data Merged: (53110, 15)\n",
      "Data Ready. ID Column: game_episode\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# 1. ê²½ë¡œ ì„¤ì • ë° Train ë¡œë“œ\n",
    "# ======================================================\n",
    "BASE_DIR = \"./open_track1\" # ë°ì´í„°ê°€ ìˆëŠ” í´ë”ëª… (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)\n",
    "if not os.path.exists(BASE_DIR): BASE_DIR = \".\" # í´ë” ì—†ìœ¼ë©´ í˜„ì¬ ê²½ë¡œ\n",
    "\n",
    "TRAIN_PATH = os.path.join(BASE_DIR, \"train.csv\")\n",
    "TEST_META_PATH = os.path.join(BASE_DIR, \"test.csv\") # ì‚¬ìš©ìë‹˜ì´ ì˜¬ë¦¬ì‹  íŒŒì¼ëª…\n",
    "MATCH_PATH = os.path.join(BASE_DIR, \"match_info.csv\")\n",
    "\n",
    "# 1. Train ë¡œë“œ\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "print(f\"âœ… Train Loaded: {train_df.shape}\")\n",
    "\n",
    "# 2. Test í†µí•© ë¡œë“œ (í•µì‹¬ ìˆ˜ì • ì‚¬í•­)\n",
    "if os.path.exists(TEST_META_PATH):\n",
    "    test_meta = pd.read_csv(TEST_META_PATH)\n",
    "    print(f\"â„¹ï¸ Reading {len(test_meta)} test files...\")\n",
    "    \n",
    "    test_dfs = []\n",
    "    # tqdmìœ¼ë¡œ ì§„í–‰ìƒí™© í™•ì¸\n",
    "    for _, row in tqdm(test_meta.iterrows(), total=len(test_meta), desc=\"Loading Test CSVs\"):\n",
    "        rel_path = row['path']\n",
    "        \n",
    "        # ê²½ë¡œ ë³´ì •: 1) ê·¸ëŒ€ë¡œ, 2) í´ë” ë¶™ì—¬ì„œ, 3) íŒŒì¼ëª…ë§Œìœ¼ë¡œ ì°¾ê¸°\n",
    "        paths_to_try = [\n",
    "            rel_path,\n",
    "            os.path.join(BASE_DIR, rel_path.lstrip(\"./\")),\n",
    "            os.path.join(BASE_DIR, \"test\", str(row['game_id']), os.path.basename(rel_path))\n",
    "        ]\n",
    "        \n",
    "        for p in paths_to_try:\n",
    "            if os.path.exists(p):\n",
    "                test_dfs.append(pd.read_csv(p))\n",
    "                break\n",
    "        \n",
    "    if test_dfs:\n",
    "        test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "        print(f\"âœ… Test Data Merged: {test_df.shape}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Test íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"test.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 3. Match Info ë³‘í•©\n",
    "if os.path.exists(MATCH_PATH):\n",
    "    match_info = pd.read_csv(MATCH_PATH)\n",
    "    match_subset = match_info[['game_id', 'home_team_id', 'venue']]\n",
    "    train_df = pd.merge(train_df, match_subset, on='game_id', how='left')\n",
    "    test_df = pd.merge(test_df, match_subset, on='game_id', how='left')\n",
    "\n",
    "# 4. ì „ì²˜ë¦¬ (ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°)\n",
    "def preprocess(df):\n",
    "    if 'home_team_id' in df.columns:\n",
    "        df['is_home'] = (df['team_id'] == df['home_team_id']).astype(float)\n",
    "    else:\n",
    "        df['is_home'] = 0.5\n",
    "        \n",
    "    # V2 ë¡œì§ ì‘ë™ì„ ìœ„í•´ end_xê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€\n",
    "    if 'end_x' not in df.columns:\n",
    "        df['end_x'] = 0.0\n",
    "        df['end_y'] = 0.0\n",
    "    else:\n",
    "        df['end_x'] = df['end_x'].fillna(0.0)\n",
    "        df['end_y'] = df['end_y'].fillna(0.0)\n",
    "    return df\n",
    "\n",
    "train_df = preprocess(train_df)\n",
    "test_df = preprocess(test_df)\n",
    "\n",
    "ID_COL = 'game_episode' if 'game_episode' in train_df.columns else 'episode_id'\n",
    "print(f\"Data Ready. ID Column: {ID_COL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5662aa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… [V6 Phase 1] Spatial Features Added!\n",
      "   Input Dimension: 44\n",
      "   - Base Scalars: 11\n",
      "   - Spatial Features: 5 (goal_dist, angle_sin/cos, sideline_dist, endline_dist)\n",
      "   - Field Zones: 3 (defensive/midfield/attacking)\n",
      "   - Event Types: 16\n",
      "   - Results: 9\n",
      "   Total: 16 + 3 + 16 + 9 = 44\n"
     ]
    }
   ],
   "source": [
    "# 1. ë²”ì£¼í˜• ë³€ìˆ˜ ì •ì˜ (Train ë°ì´í„° ë¶„ì„ ê¸°ë°˜)\n",
    "TOP_TYPES = ['Pass', 'Carry', 'Recovery', 'Interception', 'Duel', 'Tackle', \n",
    "             'Throw-In', 'Clearance', 'Intervention', 'Block', 'Pass_Freekick', \n",
    "             'Cross', 'Goal Kick', 'Error', 'Shot']\n",
    "ALL_RESULTS = ['Successful', 'Unsuccessful', 'On Target', 'Yellow_Card', \n",
    "               'Blocked', 'Keeper Rush-Out', 'Low Quality Shot', 'Off Target']\n",
    "\n",
    "def make_features(group):\n",
    "    n = len(group)\n",
    "    \n",
    "    # --- 1. ê¸°ë³¸ ì¢Œí‘œ & Context ---\n",
    "    sx = group['start_x'].values / 105.0\n",
    "    sy = group['start_y'].values / 68.0\n",
    "    ex = group['end_x'].values / 105.0\n",
    "    ey = group['end_y'].values / 68.0\n",
    "    is_home = group['is_home'].values\n",
    "    \n",
    "    # --- 2. ì‹œê°„ & ì†ë„ ---\n",
    "    if 'time_seconds' in group.columns:\n",
    "        times = group['time_seconds'].values\n",
    "        dt = np.zeros(n, dtype=np.float32)\n",
    "        dt[1:] = times[1:] - times[:-1]\n",
    "        dt = np.maximum(dt, 0.1)\n",
    "    else:\n",
    "        dt = np.ones(n, dtype=np.float32)\n",
    "\n",
    "    # --- 3. ì´ë™ëŸ‰ & ê±°ë¦¬ ê³„ì‚° ---\n",
    "    dx = ex - sx\n",
    "    dy = ey - sy\n",
    "    dist_meter = np.sqrt((dx*105)**2 + (dy*68)**2)\n",
    "    \n",
    "    # --- 4. í†µê³„ëŸ‰ (Lagged Features) ---\n",
    "    cumsum_dx = np.cumsum(dx) / 105.0\n",
    "    cumsum_dy = np.cumsum(dy) / 68.0\n",
    "    \n",
    "    lag_dist_m = np.roll(dist_meter, 1); lag_dist_m[0] = 0\n",
    "    lag_cumsum_dx = np.roll(cumsum_dx, 1); lag_cumsum_dx[0] = 0\n",
    "    lag_cumsum_dy = np.roll(cumsum_dy, 1); lag_cumsum_dy[0] = 0\n",
    "    \n",
    "    lag_dt = np.roll(dt, 1); lag_dt[0] = 1.0\n",
    "    lag_speed = lag_dist_m / np.maximum(lag_dt, 0.1)\n",
    "    \n",
    "    # --- 5. ì„ ìˆ˜ ì •ë³´ ---\n",
    "    if 'player_id' in group.columns:\n",
    "        p_ids = group['player_id'].values\n",
    "        is_same = np.zeros(n, dtype=np.float32)\n",
    "        is_same[1:] = (p_ids[1:] == p_ids[:-1]).astype(np.float32)\n",
    "    else:\n",
    "        is_same = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "    # --- 6. ì§„í–‰ë¥  & ê¸°íƒ€ ---\n",
    "    progress = np.arange(n) / max(n-1, 1)\n",
    "    is_second_half = (group['period_id'].values > 1).astype(np.float32) if 'period_id' in group.columns else np.zeros(n)\n",
    "    \n",
    "    # ======================================================\n",
    "    # [V6 Phase 1] ê³µê°„ í”¼ì²˜ ì¶”ê°€\n",
    "    # ======================================================\n",
    "    # 6-1. ê³¨ëŒ€ê¹Œì§€ ê±°ë¦¬ (ìƒëŒ€ ê³¨ëŒ€ = ì˜¤ë¥¸ìª½)\n",
    "    GOAL_X = 105.0\n",
    "    GOAL_Y = 34.0  # ê³¨ëŒ€ ì¤‘ì•™\n",
    "    \n",
    "    sx_real = sx * 105.0\n",
    "    sy_real = sy * 68.0\n",
    "    dist_to_goal = np.sqrt((sx_real - GOAL_X)**2 + (sy_real - GOAL_Y)**2) / 105.0  # ì •ê·œí™”\n",
    "    \n",
    "    # 6-2. ê³¨ëŒ€ ë°©í–¥ ê°ë„ (sin/cos)\n",
    "    dx_to_goal = GOAL_X - sx_real\n",
    "    dy_to_goal = GOAL_Y - sy_real\n",
    "    angle_to_goal = np.arctan2(dy_to_goal, dx_to_goal)\n",
    "    angle_sin = np.sin(angle_to_goal)\n",
    "    angle_cos = np.cos(angle_to_goal)\n",
    "    \n",
    "    # 6-3. ê²½ê³„ì„ ê¹Œì§€ ê±°ë¦¬\n",
    "    dist_to_sideline = np.minimum(sy_real, 68.0 - sy_real) / 68.0  # ìœ„/ì•„ë˜ í„°ì¹˜ë¼ì¸\n",
    "    dist_to_endline = np.minimum(sx_real, 105.0 - sx_real) / 105.0  # ì¢Œ/ìš° ê³¨ë¼ì¸\n",
    "    \n",
    "    # 6-4. í•„ë“œ êµ¬ì—­ (3êµ¬ì—­: ìˆ˜ë¹„/ì¤‘ì•™/ê³µê²©)\n",
    "    def get_zone(x_norm):\n",
    "        if x_norm < 35.0/105.0:\n",
    "            return 0  # ìˆ˜ë¹„ êµ¬ì—­\n",
    "        elif x_norm < 70.0/105.0:\n",
    "            return 1  # ì¤‘ì•™ êµ¬ì—­\n",
    "        else:\n",
    "            return 2  # ê³µê²© êµ¬ì—­\n",
    "    \n",
    "    zones = np.array([get_zone(x) for x in sx])\n",
    "    zone_onehot = np.zeros((n, 3), dtype=np.float32)\n",
    "    for i, z in enumerate(zones):\n",
    "        zone_onehot[i, z] = 1.0\n",
    "    \n",
    "    # --- 7. ë²”ì£¼í˜• ë³€ìˆ˜ One-hot Encoding ---\n",
    "    types_onehot = np.zeros((n, len(TOP_TYPES) + 1), dtype=np.float32)\n",
    "    curr_types = group['type_name'].values\n",
    "    for i, t in enumerate(curr_types):\n",
    "        if t in TOP_TYPES:\n",
    "            idx = TOP_TYPES.index(t)\n",
    "            types_onehot[i, idx] = 1.0\n",
    "        else:\n",
    "            types_onehot[i, -1] = 1.0\n",
    "\n",
    "    results_onehot = np.zeros((n, len(ALL_RESULTS) + 1), dtype=np.float32)\n",
    "    curr_results = group['result_name'].values\n",
    "    for i, r in enumerate(curr_results):\n",
    "        if r in ALL_RESULTS:\n",
    "            idx = ALL_RESULTS.index(r)\n",
    "            results_onehot[i, idx] = 1.0\n",
    "        else:\n",
    "            results_onehot[i, -1] = 1.0\n",
    "\n",
    "    features = []\n",
    "    \n",
    "    # --- 8. ì‹œí€€ìŠ¤ ê²°í•© ---\n",
    "    for i in range(n):\n",
    "        # ìŠ¤ì¹¼ë¼ í”¼ì²˜ (11ê°œ â†’ 16ê°œ)\n",
    "        scalars = [\n",
    "            sx[i], sy[i],                    # 0-1: ìœ„ì¹˜\n",
    "            lag_cumsum_dx[i], lag_cumsum_dy[i],  # 2-3: ëˆ„ì  íë¦„\n",
    "            lag_dist_m[i] / 100.0,           # 4: ì§ì „ ê±°ë¦¬\n",
    "            lag_speed[i] / 10.0,             # 5: ì§ì „ ì†ë„\n",
    "            dt[i] / 10.0,                    # 6: ì‹œê°„ ê²½ê³¼\n",
    "            progress[i],                     # 7: ì§„í–‰ë¥ \n",
    "            is_home[i],                      # 8: í™ˆ ì—¬ë¶€\n",
    "            is_same[i],                      # 9: ì„ ìˆ˜ ì—°ì†ì„±\n",
    "            is_second_half[i],               # 10: í›„ë°˜ì „\n",
    "            # [V6 Phase 1] ìƒˆë¡œìš´ ê³µê°„ í”¼ì²˜ (11-15)\n",
    "            dist_to_goal[i],                 # 11: ê³¨ëŒ€ ê±°ë¦¬\n",
    "            angle_sin[i],                    # 12: ê³¨ëŒ€ ë°©í–¥ sin\n",
    "            angle_cos[i],                    # 13: ê³¨ëŒ€ ë°©í–¥ cos\n",
    "            dist_to_sideline[i],             # 14: í„°ì¹˜ë¼ì¸ ê±°ë¦¬\n",
    "            dist_to_endline[i],              # 15: ê³¨ë¼ì¸ ê±°ë¦¬\n",
    "        ]\n",
    "        \n",
    "        # Combine: 16 Scalar + 3 Zone + 16 Type + 9 Result = 44ì°¨ì›\n",
    "        feat_vec = np.concatenate([scalars, zone_onehot[i], types_onehot[i], results_onehot[i]])\n",
    "        \n",
    "        # (1) Start Node\n",
    "        features.append(feat_vec)\n",
    "        \n",
    "        # (2) End Node (ë§ˆì§€ë§‰ ì œì™¸)\n",
    "        if i < n - 1:\n",
    "            # End ì¢Œí‘œ ê³„ì‚°\n",
    "            ex_real = ex[i] * 105.0\n",
    "            ey_real = ey[i] * 68.0\n",
    "            \n",
    "            # End ìœ„ì¹˜ì—ì„œì˜ ê³µê°„ í”¼ì²˜ ì¬ê³„ì‚°\n",
    "            end_dist_to_goal = np.sqrt((ex_real - GOAL_X)**2 + (ey_real - GOAL_Y)**2) / 105.0\n",
    "            end_dx_to_goal = GOAL_X - ex_real\n",
    "            end_dy_to_goal = GOAL_Y - ey_real\n",
    "            end_angle = np.arctan2(end_dy_to_goal, end_dx_to_goal)\n",
    "            end_dist_to_sideline = min(ey_real, 68.0 - ey_real) / 68.0\n",
    "            end_dist_to_endline = min(ex_real, 105.0 - ex_real) / 105.0\n",
    "            \n",
    "            end_zone = get_zone(ex[i])\n",
    "            end_zone_onehot = np.zeros(3, dtype=np.float32)\n",
    "            end_zone_onehot[end_zone] = 1.0\n",
    "            \n",
    "            scalars_end = scalars.copy()\n",
    "            scalars_end[0] = ex[i]  # End X\n",
    "            scalars_end[1] = ey[i]  # End Y\n",
    "            scalars_end[2] = cumsum_dx[i]  # ëˆ„ì  íë¦„ ì—…ë°ì´íŠ¸\n",
    "            scalars_end[3] = cumsum_dy[i]\n",
    "            # End ìœ„ì¹˜ì˜ ê³µê°„ í”¼ì²˜\n",
    "            scalars_end[11] = end_dist_to_goal\n",
    "            scalars_end[12] = np.sin(end_angle)\n",
    "            scalars_end[13] = np.cos(end_angle)\n",
    "            scalars_end[14] = end_dist_to_sideline\n",
    "            scalars_end[15] = end_dist_to_endline\n",
    "            \n",
    "            feat_vec_end = np.concatenate([scalars_end, end_zone_onehot, types_onehot[i], results_onehot[i]])\n",
    "            features.append(feat_vec_end)\n",
    "            \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "# ì…ë ¥ ì°¨ì› ìë™ ê³„ì‚°\n",
    "# Scalar(16) + Zone(3) + Type(16) + Result(9) = 44\n",
    "dummy_group = train_df.iloc[:5].copy()\n",
    "dummy_feat = make_features(dummy_group)\n",
    "INPUT_DIM = dummy_feat.shape[1]\n",
    "\n",
    "print(f\"âœ… [V6 Phase 1] Spatial Features Added!\")\n",
    "print(f\"   Input Dimension: {INPUT_DIM}\")\n",
    "print(f\"   - Base Scalars: 11\")\n",
    "print(f\"   - Spatial Features: 5 (goal_dist, angle_sin/cos, sideline_dist, endline_dist)\")\n",
    "print(f\"   - Field Zones: 3 (defensive/midfield/attacking)\")\n",
    "print(f\"   - Event Types: 16\")\n",
    "print(f\"   - Results: 9\")\n",
    "print(f\"   Total: 16 + 3 + 16 + 9 = {INPUT_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d92c70",
   "metadata": {},
   "outputs": [],
   "source": "class SoccerDataset(Dataset):\n    def __init__(self, df, mode='train'):\n        self.mode = mode\n        self.episodes = []\n        self.targets = []\n        self.team_ids = []\n        self.episode_ids = []\n        \n        # ìˆœì„œ ì„ì„ ë°©ì§€\n        grouped = df.groupby(ID_COL, sort=False)\n        \n        for name, group in tqdm(grouped, desc=f\"Dataset ({mode})\"):\n            if mode == 'train' and len(group) < 2: continue\n            \n            seq = make_features(group)\n\n            team_id = group.iloc[0]['team_id']\n            \n            if mode == 'train':\n                last = group.iloc[-1]\n                self.targets.append([last['end_x']/105.0, last['end_y']/68.0])\n                self.episodes.append(seq)\n                self.team_ids.append(team_id)\n            else:\n                self.episodes.append(seq)\n                self.team_ids.append(team_id)\n                self.episode_ids.append(str(name))\n\n    def __len__(self): return len(self.episodes)\n    \n    def __getitem__(self, idx):\n        seq = torch.FloatTensor(self.episodes[idx])\n        if len(seq) > MAX_SEQ_LEN: seq = seq[-MAX_SEQ_LEN:]\n        \n        if self.mode == 'train':\n            # Train ë°˜í™˜: (seq, target, team_id)\n            return seq, torch.FloatTensor(self.targets[idx]), self.team_ids[idx]\n        else:\n            # Test ë°˜í™˜: (seq, team_id, episode_id)\n            return seq, self.team_ids[idx], self.episode_ids[idx]\n\n# [V7 ìˆ˜ì •] collate_fn: lengths ì •ë³´ ì¶”ê°€\ndef collate_fn(batch):\n    seqs = [b[0] for b in batch]\n    lens = torch.LongTensor([len(s) for s in seqs])\n    padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n    mask = torch.arange(padded.size(1))[None, :] >= lens[:, None]\n    \n    # ë°°ì¹˜ ì²« ë²ˆì§¸ ìš”ì†Œë¡œ ëª¨ë“œë¥¼ íŒë‹¨\n    elem = batch[0]\n    \n    # Test Mode í™•ì¸ (3ë²ˆì§¸ ìš”ì†Œê°€ ë¬¸ìì—´ IDì¸ ê²½ìš°)\n    if isinstance(elem[2], str):\n        # Test: b[1]ì´ Team ID\n        team_ids = torch.LongTensor([b[1] for b in batch])\n        episode_ids = [b[2] for b in batch]\n        return padded, mask, lens, team_ids, episode_ids\n        \n    else:\n        # Train Mode: b[1]ì€ Target(ì¢Œí‘œ), b[2]ê°€ Team ID\n        targets = torch.stack([b[1] for b in batch])\n        team_ids_train = torch.LongTensor([b[2] for b in batch]) \n        return padded, targets, mask, lens, team_ids_train\n\n# ë°ì´í„°ì…‹ ìƒì„±\nfull_dataset = SoccerDataset(train_df, mode='train')\ntest_dataset = SoccerDataset(test_df, mode='test')\n\n# ì—í”¼ì†Œë“œ ê¸¸ì´ ë¶„í¬ ë¶„ì„\nepisode_lengths = [len(ep) for ep in full_dataset.episodes]\nprint(f\"\\nğŸ“Š [V7] ì—í”¼ì†Œë“œ ê¸¸ì´ ë¶„í¬ ë¶„ì„\")\nprint(f\"   ì´ ì—í”¼ì†Œë“œ: {len(episode_lengths):,}ê°œ\")\nprint(f\"   í‰ê·  ê¸¸ì´: {np.mean(episode_lengths):.2f}\")\nprint(f\"   ì¤‘ì•™ê°’: {np.median(episode_lengths):.2f}\")\nprint(f\"   ì§§ì€ ì—í”¼ì†Œë“œ (<= {SHORT_THRESHOLD}): {sum(1 for l in episode_lengths if l <= SHORT_THRESHOLD):,}ê°œ ({100*sum(1 for l in episode_lengths if l <= SHORT_THRESHOLD)/len(episode_lengths):.1f}%)\")\nprint(f\"   ì¤‘ê°„ ì—í”¼ì†Œë“œ ({SHORT_THRESHOLD} ~ {LONG_THRESHOLD}): {sum(1 for l in episode_lengths if SHORT_THRESHOLD < l <= LONG_THRESHOLD):,}ê°œ ({100*sum(1 for l in episode_lengths if SHORT_THRESHOLD < l <= LONG_THRESHOLD)/len(episode_lengths):.1f}%)\")\nprint(f\"   ê¸´ ì—í”¼ì†Œë“œ (> {LONG_THRESHOLD}): {sum(1 for l in episode_lengths if l > LONG_THRESHOLD):,}ê°œ ({100*sum(1 for l in episode_lengths if l > LONG_THRESHOLD)/len(episode_lengths):.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca77e0",
   "metadata": {},
   "outputs": [],
   "source": "# ======================================================\n# [V7] ê¸¸ì´ë³„ ì•™ìƒë¸”: LSTM + Transformer ëª¨ë¸ ì •ì˜\n# ======================================================\n\n# ===== 1. Sinusoidal Positional Encoding =====\nclass SinusoidalPositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=100):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1), :]\n\n# ===== 2. Transformer ëª¨ë¸ (ê¸´ ì—í”¼ì†Œë“œìš©) =====\nclass TransformerPredictor(nn.Module):\n    def __init__(self, input_dim, d_model, nhead, num_layers, dropout, max_seq_len=MAX_SEQ_LEN):\n        super().__init__()\n        self.embedding = nn.Linear(input_dim, d_model)\n        self.pos_encoder = SinusoidalPositionalEncoding(d_model, max_len=max_seq_len)\n        self.dropout = nn.Dropout(dropout)\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=d_model*4,\n            dropout=dropout,\n            activation='gelu',\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        self.fc = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model // 2, 2)\n        )\n\n    def forward(self, x, mask=None):\n        x = self.embedding(x)\n        x = self.pos_encoder(x)\n        x = self.dropout(x)\n        output = self.transformer(x, src_key_padding_mask=mask)\n\n        # Valid Token Mean Pooling\n        valid_mask = (~mask).unsqueeze(-1).float()\n        mean_out = (output * valid_mask).sum(dim=1) / valid_mask.sum(dim=1).clamp(min=1e-9)\n        return self.fc(mean_out)\n\n# ===== 3. LSTM ëª¨ë¸ (ì§§ì€ ì—í”¼ì†Œë“œìš©) =====\nclass LSTMPredictor(nn.Module):\n    \"\"\"\n    Bi-LSTM + Attention ëª¨ë¸\n    - ì§§ì€ ì‹œí€€ìŠ¤ì— ìµœì í™”\n    - ìˆœì°¨ì  ì •ë³´ ë³´ì¡´\n    - Transformerë³´ë‹¤ ì ì€ íŒŒë¼ë¯¸í„°\n    \"\"\"\n    def __init__(self, input_dim, hidden_dim, num_layers, dropout, bidirectional=True):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n        \n        # LSTM Layer\n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=bidirectional\n        )\n        \n        # Attention Mechanism\n        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n        self.attention = nn.Sequential(\n            nn.Linear(lstm_output_dim, lstm_output_dim // 2),\n            nn.Tanh(),\n            nn.Linear(lstm_output_dim // 2, 1)\n        )\n        \n        # Final Prediction Layer\n        self.fc = nn.Sequential(\n            nn.Linear(lstm_output_dim, lstm_output_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(lstm_output_dim // 2, 2)\n        )\n    \n    def forward(self, x, mask=None, lengths=None):\n        \"\"\"\n        Args:\n            x: (batch, seq_len, input_dim)\n            mask: (batch, seq_len) - True for padding\n            lengths: (batch,) - ì‹¤ì œ ì‹œí€€ìŠ¤ ê¸¸ì´\n        \"\"\"\n        batch_size = x.size(0)\n        seq_len = x.size(1)\n        \n        # Pack sequence for efficient LSTM processing\n        if lengths is not None:\n            # lengthsë¥¼ CPUë¡œ ì˜®ê¸°ê³  sort\n            lengths_cpu = lengths.cpu()\n            packed_x = pack_padded_sequence(x, lengths_cpu, batch_first=True, enforce_sorted=False)\n            packed_output, (hidden, cell) = self.lstm(packed_x)\n            # [FIX] total_lengthë¥¼ ì§€ì •í•˜ì—¬ ì›ë˜ ê¸¸ì´ë¡œ ë³µì›\n            lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=seq_len)\n        else:\n            lstm_out, (hidden, cell) = self.lstm(x)\n        \n        # Attention Weights ê³„ì‚°\n        # lstm_out: (batch, seq_len, hidden*2)\n        attn_weights = self.attention(lstm_out)  # (batch, seq_len, 1)\n        \n        # Mask ì ìš© (íŒ¨ë”©ëœ ë¶€ë¶„ì€ -infë¡œ)\n        if mask is not None:\n            attn_weights = attn_weights.masked_fill(mask.unsqueeze(-1), float('-inf'))\n        \n        attn_weights = torch.softmax(attn_weights, dim=1)  # (batch, seq_len, 1)\n        \n        # Weighted Sum\n        context = torch.sum(lstm_out * attn_weights, dim=1)  # (batch, hidden*2)\n        \n        # Final Prediction\n        return self.fc(context)\n\n# ===== 4. ëª¨ë¸ ì´ˆê¸°í™” =====\ntransformer_model = TransformerPredictor(INPUT_DIM, D_MODEL, NHEAD, NUM_LAYERS, DROPOUT).to(DEVICE)\nlstm_model = LSTMPredictor(INPUT_DIM, HIDDEN_DIM, LSTM_LAYERS, DROPOUT, BIDIRECTIONAL).to(DEVICE)\n\nprint(\"=\" * 60)\nprint(\"âœ… [V7] ê¸¸ì´ë³„ ì•™ìƒë¸” ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ\")\nprint(\"=\" * 60)\nprint(f\"ğŸ“Š Transformer ëª¨ë¸ (ê¸´ ì—í”¼ì†Œë“œ > {LONG_THRESHOLD})\")\nprint(f\"   - Input Dim: {INPUT_DIM}\")\nprint(f\"   - D_Model: {D_MODEL}\")\nprint(f\"   - Heads: {NHEAD}\")\nprint(f\"   - Layers: {NUM_LAYERS}\")\nprint(f\"   - Parameters: {sum(p.numel() for p in transformer_model.parameters()):,}\")\nprint()\nprint(f\"ğŸ“Š LSTM ëª¨ë¸ (ì§§ì€ ì—í”¼ì†Œë“œ <= {SHORT_THRESHOLD})\")\nprint(f\"   - Input Dim: {INPUT_DIM}\")\nprint(f\"   - Hidden Dim: {HIDDEN_DIM}\")\nprint(f\"   - Layers: {LSTM_LAYERS}\")\nprint(f\"   - Bidirectional: {BIDIRECTIONAL}\")\nprint(f\"   - Parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63608a68",
   "metadata": {},
   "outputs": [],
   "source": "# ======================================================\n# [V7] ê¸¸ì´ë³„ ì•™ìƒë¸”: LSTM + Transformer í•™ìŠµ\n# ======================================================\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nSEEDS = [42, 2024, 777]\nprint(f\"\\n{'='*70}\")\nprint(f\"ğŸš€ [V7] ê¸¸ì´ë³„ ì•™ìƒë¸” í•™ìŠµ ì‹œì‘\")\nprint(f\"{'='*70}\")\nprint(f\"ì „ëµ: LSTMê³¼ Transformerë¥¼ ê°ê° í•™ìŠµí•˜ì—¬ ì¶”ë¡  ì‹œ ê¸¸ì´ë³„ë¡œ ì„ íƒ\")\nprint(f\"  - LSTM: ì§§ì€ ì‹œí€€ìŠ¤ì— ìµœì í™” (íŒŒë¼ë¯¸í„° íš¨ìœ¨ì )\")\nprint(f\"  - Transformer: ê¸´ ì‹œí€€ìŠ¤ì— ìµœì í™” (long-range dependency)\")\nprint(f\"{'='*70}\\n\")\n\n# íˆìŠ¤í† ë¦¬ ì €ì¥ìš©\nlstm_histories = []\ntransformer_histories = []\n\nfor i, seed in enumerate(SEEDS):\n    print(f\"\\n{'='*70}\")\n    print(f\"ğŸ“¦ [ëª¨ë¸ ì„¸íŠ¸ {i+1}/3] Seed {seed} í•™ìŠµ ì‹œì‘\")\n    print(f\"{'='*70}\")\n    seed_everything(seed)\n    \n    # ë°ì´í„°ì…‹ ë¶„í• \n    train_idx, val_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=seed)\n    train_loader = DataLoader(torch.utils.data.Subset(full_dataset, train_idx), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(torch.utils.data.Subset(full_dataset, val_idx), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n    # ===== 1. LSTM ëª¨ë¸ í•™ìŠµ =====\n    print(f\"\\nğŸ”¹ [1/2] LSTM ëª¨ë¸ í•™ìŠµ (ì§§ì€ ì—í”¼ì†Œë“œ íŠ¹í™”)\")\n    print(f\"   ëª©í‘œ: ì§§ì€ ì‹œí€€ìŠ¤ì—ì„œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±\")\n    \n    lstm = LSTMPredictor(INPUT_DIM, HIDDEN_DIM, LSTM_LAYERS, DROPOUT, BIDIRECTIONAL).to(DEVICE)\n    lstm_optimizer = optim.AdamW(lstm.parameters(), lr=LR_BASE, weight_decay=1e-4)\n    lstm_scheduler = CosineAnnealingLR(lstm_optimizer, T_max=EPOCHS_BASE - WARMUP_EPOCHS, eta_min=1e-6)\n    criterion = nn.HuberLoss(delta=1.0)\n    \n    best_lstm_dist = float('inf')\n    lstm_history = {'train_loss': [], 'val_dist': [], 'lr': []}\n    \n    for epoch in range(EPOCHS_BASE):\n        # Warmup\n        if epoch < WARMUP_EPOCHS:\n            lr = LR_BASE * (epoch + 1) / WARMUP_EPOCHS\n            for param_group in lstm_optimizer.param_groups:\n                param_group['lr'] = lr\n        \n        # Training\n        lstm.train()\n        train_loss = 0.0\n        for seqs, targets, mask, lens, _ in train_loader:\n            seqs, targets, mask, lens = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n            \n            lstm_optimizer.zero_grad()\n            pred = lstm(seqs, mask, lens)\n            loss = criterion(pred, targets)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(lstm.parameters(), GRAD_CLIP)\n            lstm_optimizer.step()\n            train_loss += loss.item()\n        \n        # Validation\n        lstm.eval()\n        dists = []\n        with torch.no_grad():\n            for seqs, targets, mask, lens, _ in val_loader:\n                seqs, targets, mask, lens = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n                pred = lstm(seqs, mask, lens)\n                p_real = pred.cpu().numpy() * np.array([105.0, 68.0])\n                t_real = targets.cpu().numpy() * np.array([105.0, 68.0])\n                dists.extend(np.sqrt(np.sum((p_real - t_real)**2, axis=1)))\n        \n        avg_dist = np.mean(dists)\n        avg_train_loss = train_loss / len(train_loader)\n        \n        if epoch >= WARMUP_EPOCHS:\n            lstm_scheduler.step()\n        \n        current_lr = lstm_optimizer.param_groups[0]['lr']\n        lstm_history['train_loss'].append(avg_train_loss)\n        lstm_history['val_dist'].append(avg_dist)\n        lstm_history['lr'].append(current_lr)\n        \n        if (epoch + 1) % 10 == 0 or epoch < 3:\n            print(f\"   [Epoch {epoch+1:2d}/{EPOCHS_BASE}] Loss: {avg_train_loss:.4f} | Val Dist: {avg_dist:.4f}m | LR: {current_lr:.6f}\")\n        \n        if avg_dist < best_lstm_dist:\n            best_lstm_dist = avg_dist\n            torch.save(lstm.state_dict(), f'v7_lstm_{i}.pth')\n    \n    lstm_history['best_dist'] = best_lstm_dist\n    lstm_history['seed'] = seed\n    lstm_histories.append(lstm_history)\n    print(f\"   âœ… LSTM ì™„ë£Œ. Best Val Dist: {best_lstm_dist:.4f}m\")\n    \n    # ===== 2. Transformer ëª¨ë¸ í•™ìŠµ =====\n    print(f\"\\nğŸ”¹ [2/2] Transformer ëª¨ë¸ í•™ìŠµ (ê¸´ ì—í”¼ì†Œë“œ íŠ¹í™”)\")\n    print(f\"   ëª©í‘œ: ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±\")\n    \n    transformer = TransformerPredictor(INPUT_DIM, D_MODEL, NHEAD, NUM_LAYERS, DROPOUT).to(DEVICE)\n    transformer_optimizer = optim.AdamW(transformer.parameters(), lr=LR_BASE, weight_decay=1e-4)\n    transformer_scheduler = CosineAnnealingLR(transformer_optimizer, T_max=EPOCHS_BASE - WARMUP_EPOCHS, eta_min=1e-6)\n    \n    best_transformer_dist = float('inf')\n    transformer_history = {'train_loss': [], 'val_dist': [], 'lr': []}\n    \n    for epoch in range(EPOCHS_BASE):\n        # Warmup\n        if epoch < WARMUP_EPOCHS:\n            lr = LR_BASE * (epoch + 1) / WARMUP_EPOCHS\n            for param_group in transformer_optimizer.param_groups:\n                param_group['lr'] = lr\n        \n        # Training\n        transformer.train()\n        train_loss = 0.0\n        for seqs, targets, mask, lens, _ in train_loader:\n            seqs, targets, mask = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE)\n            \n            transformer_optimizer.zero_grad()\n            pred = transformer(seqs, mask)\n            loss = criterion(pred, targets)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(transformer.parameters(), GRAD_CLIP)\n            transformer_optimizer.step()\n            train_loss += loss.item()\n        \n        # Validation\n        transformer.eval()\n        dists = []\n        with torch.no_grad():\n            for seqs, targets, mask, lens, _ in val_loader:\n                seqs, targets, mask = seqs.to(DEVICE), targets.to(DEVICE), mask.to(DEVICE)\n                pred = transformer(seqs, mask)\n                p_real = pred.cpu().numpy() * np.array([105.0, 68.0])\n                t_real = targets.cpu().numpy() * np.array([105.0, 68.0])\n                dists.extend(np.sqrt(np.sum((p_real - t_real)**2, axis=1)))\n        \n        avg_dist = np.mean(dists)\n        avg_train_loss = train_loss / len(train_loader)\n        \n        if epoch >= WARMUP_EPOCHS:\n            transformer_scheduler.step()\n        \n        current_lr = transformer_optimizer.param_groups[0]['lr']\n        transformer_history['train_loss'].append(avg_train_loss)\n        transformer_history['val_dist'].append(avg_dist)\n        transformer_history['lr'].append(current_lr)\n        \n        if (epoch + 1) % 10 == 0 or epoch < 3:\n            print(f\"   [Epoch {epoch+1:2d}/{EPOCHS_BASE}] Loss: {avg_train_loss:.4f} | Val Dist: {avg_dist:.4f}m | LR: {current_lr:.6f}\")\n        \n        if avg_dist < best_transformer_dist:\n            best_transformer_dist = avg_dist\n            torch.save(transformer.state_dict(), f'v7_transformer_{i}.pth')\n    \n    transformer_history['best_dist'] = best_transformer_dist\n    transformer_history['seed'] = seed\n    transformer_histories.append(transformer_history)\n    print(f\"   âœ… Transformer ì™„ë£Œ. Best Val Dist: {best_transformer_dist:.4f}m\")\n    \n    print(f\"\\nğŸ“Š [ëª¨ë¸ ì„¸íŠ¸ {i+1}] ìš”ì•½\")\n    print(f\"   LSTM Best:        {best_lstm_dist:.4f}m\")\n    print(f\"   Transformer Best: {best_transformer_dist:.4f}m\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"âœ… ì „ì²´ í•™ìŠµ ì™„ë£Œ!\")\nprint(f\"{'='*70}\")\nprint(f\"ğŸ“Š LSTM í‰ê·  Best Distance:        {np.mean([h['best_dist'] for h in lstm_histories]):.4f}m\")\nprint(f\"ğŸ“Š Transformer í‰ê·  Best Distance: {np.mean([h['best_dist'] for h in transformer_histories]):.4f}m\")\nprint(f\"{'='*70}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5911a718",
   "metadata": {},
   "outputs": [],
   "source": "# ======================================================\n# [V7] ê¸¸ì´ë³„ ë™ì  ì•™ìƒë¸” ì¶”ë¡ \n# ======================================================\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\nprint(f\"\\n{'='*70}\")\nprint(f\"ğŸ”® [V7] ê¸¸ì´ë³„ ë™ì  ì•™ìƒë¸” ì¶”ë¡  ì‹œì‘\")\nprint(f\"{'='*70}\")\n\n# ===== 1. ëª¨ë¸ ë¡œë“œ (ê° 3ê°œì”©) =====\nlstm_models = []\ntransformer_models = []\n\nfor i in range(3):\n    # LSTM ëª¨ë¸\n    lstm = LSTMPredictor(INPUT_DIM, HIDDEN_DIM, LSTM_LAYERS, DROPOUT, BIDIRECTIONAL).to(DEVICE)\n    lstm.load_state_dict(torch.load(f'v7_lstm_{i}.pth'))\n    lstm.eval()\n    lstm_models.append(lstm)\n    \n    # Transformer ëª¨ë¸\n    transformer = TransformerPredictor(INPUT_DIM, D_MODEL, NHEAD, NUM_LAYERS, DROPOUT).to(DEVICE)\n    transformer.load_state_dict(torch.load(f'v7_transformer_{i}.pth'))\n    transformer.eval()\n    transformer_models.append(transformer)\n\nprint(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\nprint(f\"   - LSTM ëª¨ë¸: 3ê°œ\")\nprint(f\"   - Transformer ëª¨ë¸: 3ê°œ\")\n\n# ===== 2. ê¸¸ì´ë³„ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ =====\nseed_weights = [0.25, 0.25, 0.5]  # 3ê°œ ëª¨ë¸ ì•™ìƒë¸” ê°€ì¤‘ì¹˜\n\ndef length_based_ensemble(lstm_pred, transformer_pred, length):\n    \"\"\"\n    ê¸¸ì´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì ìš©\n    \n    Args:\n        lstm_pred: LSTM ì˜ˆì¸¡ (batch, 2)\n        transformer_pred: Transformer ì˜ˆì¸¡ (batch, 2)\n        length: ì—í”¼ì†Œë“œ ê¸¸ì´ (ìŠ¤ì¹¼ë¼)\n    \n    Returns:\n        ì•™ìƒë¸”ëœ ì˜ˆì¸¡ (batch, 2)\n    \"\"\"\n    if length <= SHORT_THRESHOLD:\n        # ì§§ì€ ì—í”¼ì†Œë“œ: LSTM 100%\n        return lstm_pred\n    elif length <= LONG_THRESHOLD:\n        # ì¤‘ê°„ ì—í”¼ì†Œë“œ: LSTM 60% + Transformer 40%\n        return 0.6 * lstm_pred + 0.4 * transformer_pred\n    else:\n        # ê¸´ ì—í”¼ì†Œë“œ: Transformer 100%\n        return transformer_pred\n\n# ===== 3. ì¶”ë¡  =====\nresults = []\nlength_distribution = {'short': 0, 'medium': 0, 'long': 0}\n\nprint(f\"\\nğŸ”„ ì¶”ë¡  ì¤‘...\")\nwith torch.no_grad():\n    for seqs, mask, lens, team_ids, episode_ids in tqdm(test_loader, desc=\"Inference\"):\n        seqs, mask, lens = seqs.to(DEVICE), mask.to(DEVICE), lens.to(DEVICE)\n        batch_size = seqs.size(0)\n        \n        # ê° ë°°ì¹˜ ë‚´ ìƒ˜í”Œë³„ë¡œ ì²˜ë¦¬\n        for b in range(batch_size):\n            seq = seqs[b:b+1]\n            m = mask[b:b+1]\n            l = lens[b:b+1]\n            length = lens[b].item()\n            eid = episode_ids[b]\n            \n            # ê¸¸ì´ ë¶„í¬ ê¸°ë¡\n            if length <= SHORT_THRESHOLD:\n                length_distribution['short'] += 1\n            elif length <= LONG_THRESHOLD:\n                length_distribution['medium'] += 1\n            else:\n                length_distribution['long'] += 1\n            \n            # 3ê°œ ëª¨ë¸ì—ì„œ ê°ê° ì˜ˆì¸¡\n            lstm_preds = []\n            transformer_preds = []\n            \n            for j in range(3):\n                lstm_out = lstm_models[j](seq, m, l).cpu().numpy()\n                transformer_out = transformer_models[j](seq, m).cpu().numpy()\n                lstm_preds.append(lstm_out)\n                transformer_preds.append(transformer_out)\n            \n            # Seed ì•™ìƒë¸” (3ê°œ ëª¨ë¸)\n            lstm_ensemble = (lstm_preds[0] * seed_weights[0] + \n                           lstm_preds[1] * seed_weights[1] + \n                           lstm_preds[2] * seed_weights[2])\n            transformer_ensemble = (transformer_preds[0] * seed_weights[0] + \n                                  transformer_preds[1] * seed_weights[1] + \n                                  transformer_preds[2] * seed_weights[2])\n            \n            # ê¸¸ì´ë³„ ì•™ìƒë¸”\n            final_pred = length_based_ensemble(lstm_ensemble, transformer_ensemble, length)\n            \n            # ì¢Œí‘œ ë³€í™˜\n            px = final_pred[0, 0] * 105.0\n            py = final_pred[0, 1] * 68.0\n            \n            results.append({'game_episode': eid, 'pred_x': px, 'pred_y': py})\n\n# ===== 4. ê²°ê³¼ í†µê³„ =====\ntotal = sum(length_distribution.values())\nprint(f\"\\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸¸ì´ ë¶„í¬:\")\nprint(f\"   ì§§ì€ ì—í”¼ì†Œë“œ (<= {SHORT_THRESHOLD}):  {length_distribution['short']:5d}ê°œ ({100*length_distribution['short']/total:.1f}%) â†’ LSTM 100%\")\nprint(f\"   ì¤‘ê°„ ì—í”¼ì†Œë“œ ({SHORT_THRESHOLD}~{LONG_THRESHOLD}): {length_distribution['medium']:5d}ê°œ ({100*length_distribution['medium']/total:.1f}%) â†’ LSTM 60% + Transformer 40%\")\nprint(f\"   ê¸´ ì—í”¼ì†Œë“œ (> {LONG_THRESHOLD}):   {length_distribution['long']:5d}ê°œ ({100*length_distribution['long']/total:.1f}%) â†’ Transformer 100%\")\n\n# ===== 5. ì œì¶œ íŒŒì¼ ìƒì„± =====\npred_df = pd.DataFrame(results)\n\nSUBMISSION_PATH = \"open_track1/sample_submission.csv\"\nif os.path.exists(SUBMISSION_PATH):\n    sub = pd.read_csv(SUBMISSION_PATH)\nelse:\n    sub = pd.read_csv(TEST_META_PATH)\n    col_map = {'episode_id': 'game_episode'}\n    sub = sub.rename(columns=col_map)\n    sub = sub[['game_episode']]\n\nprint(f\"\\nğŸ“„ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\nprint(f\"   Submission Shape: {sub.shape}, Prediction Shape: {pred_df.shape}\")\n\nfinal_sub = pd.merge(sub[['game_episode']], pred_df, on='game_episode', how='left')\nfinal_sub = final_sub.rename(columns={'pred_x': 'end_x', 'pred_y': 'end_y'})\n\nnan_count = final_sub.isnull().sum().sum()\nif nan_count > 0:\n    print(f\"   âš ï¸ ê²½ê³ : {nan_count}ê°œì˜ ê²°ì¸¡ì¹˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\")\n    final_sub = final_sub.fillna(50.0)\n\nfilename = \"v7_length_ensemble_submit.csv\"\nfinal_sub.to_csv(filename, index=False)\n\nprint(f\"\\n{'='*70}\")\nprint(f\"âœ… [V7] ê¸¸ì´ë³„ ë™ì  ì•™ìƒë¸” ì¶”ë¡  ì™„ë£Œ!\")\nprint(f\"{'='*70}\")\nprint(f\"ğŸ“ ì œì¶œ íŒŒì¼: {filename}\")\nprint(f\"ğŸ“Š ì „ëµ ìš”ì•½:\")\nprint(f\"   - ì§§ì€ ì—í”¼ì†Œë“œ: LSTMì˜ ìˆœì°¨ì  ì •ë³´ ì²˜ë¦¬ ëŠ¥ë ¥ í™œìš©\")\nprint(f\"   - ì¤‘ê°„ ì—í”¼ì†Œë“œ: ë‘ ëª¨ë¸ì˜ ê°•ì ì„ ê· í˜•ìˆê²Œ ê²°í•©\")\nprint(f\"   - ê¸´ ì—í”¼ì†Œë“œ: Transformerì˜ long-range dependency í¬ì°© ëŠ¥ë ¥ í™œìš©\")\nprint(f\"{'='*70}\")\nprint(final_sub.head())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ai7ldiiy9o",
   "metadata": {},
   "outputs": [],
   "source": "# ======================================================\n# [V7] ê¸¸ì´ë³„ ì•™ìƒë¸” í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n# ======================================================\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\n\n# í•œê¸€ í°íŠ¸ ì„¤ì •\nplt.rcParams['font.family'] = 'Malgun Gothic'  # Windows\n# plt.rcParams['font.family'] = 'AppleGothic'  # Mac\n# plt.rcParams['font.family'] = 'NanumGothic'  # Linux\nplt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€\n\nfig = plt.figure(figsize=(18, 12))\ngs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n\nfig.suptitle('[V7] ê¸¸ì´ë³„ ì•™ìƒë¸”: LSTM + Transformer í•™ìŠµ ê³¡ì„ ', fontsize=18, fontweight='bold')\n\ncolors_lstm = ['#1f77b4', '#ff7f0e', '#2ca02c']\ncolors_transformer = ['#d62728', '#9467bd', '#8c564b']\n\n# ===== 1. LSTM Training Loss =====\nax1 = fig.add_subplot(gs[0, 0])\nfor i, history in enumerate(lstm_histories):\n    epochs = range(1, len(history['train_loss']) + 1)\n    ax1.plot(epochs, history['train_loss'], label=f'Seed {history[\"seed\"]}', \n             color=colors_lstm[i], linewidth=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Huber Loss')\nax1.set_title('LSTM Training Loss')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# ===== 2. LSTM Validation Distance =====\nax2 = fig.add_subplot(gs[0, 1])\nfor i, history in enumerate(lstm_histories):\n    epochs = range(1, len(history['val_dist']) + 1)\n    ax2.plot(epochs, history['val_dist'], label=f'Seed {history[\"seed\"]}', \n             color=colors_lstm[i], linewidth=2)\n    best_idx = np.argmin(history['val_dist'])\n    best_val = history['val_dist'][best_idx]\n    ax2.scatter(best_idx + 1, best_val, color=colors_lstm[i], s=100, zorder=5, marker='*')\n    ax2.annotate(f'{best_val:.2f}m', xy=(best_idx + 1, best_val), \n                 xytext=(5, 5), textcoords='offset points', fontsize=9, color=colors_lstm[i])\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Distance (m)')\nax2.set_title('LSTM Validation Distance')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# ===== 3. LSTM Best Distances =====\nax3 = fig.add_subplot(gs[0, 2])\nbest_dists_lstm = [h['best_dist'] for h in lstm_histories]\nseeds_lstm = [h['seed'] for h in lstm_histories]\nbars = ax3.bar(range(len(best_dists_lstm)), best_dists_lstm, color=colors_lstm)\nax3.set_xticks(range(len(best_dists_lstm)))\nax3.set_xticklabels([f'Seed\\n{s}' for s in seeds_lstm])\nax3.set_ylabel('Best Validation Distance (m)')\nax3.set_title('LSTM Best Performance')\nax3.grid(True, alpha=0.3, axis='y')\nfor i, (bar, dist) in enumerate(zip(bars, best_dists_lstm)):\n    height = bar.get_height()\n    ax3.text(bar.get_x() + bar.get_width()/2., height, f'{dist:.4f}m',\n             ha='center', va='bottom', fontweight='bold', fontsize=9)\n\n# ===== 4. Transformer Training Loss =====\nax4 = fig.add_subplot(gs[1, 0])\nfor i, history in enumerate(transformer_histories):\n    epochs = range(1, len(history['train_loss']) + 1)\n    ax4.plot(epochs, history['train_loss'], label=f'Seed {history[\"seed\"]}', \n             color=colors_transformer[i], linewidth=2)\nax4.set_xlabel('Epoch')\nax4.set_ylabel('Huber Loss')\nax4.set_title('Transformer Training Loss')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\n# ===== 5. Transformer Validation Distance =====\nax5 = fig.add_subplot(gs[1, 1])\nfor i, history in enumerate(transformer_histories):\n    epochs = range(1, len(history['val_dist']) + 1)\n    ax5.plot(epochs, history['val_dist'], label=f'Seed {history[\"seed\"]}', \n             color=colors_transformer[i], linewidth=2)\n    best_idx = np.argmin(history['val_dist'])\n    best_val = history['val_dist'][best_idx]\n    ax5.scatter(best_idx + 1, best_val, color=colors_transformer[i], s=100, zorder=5, marker='*')\n    ax5.annotate(f'{best_val:.2f}m', xy=(best_idx + 1, best_val), \n                 xytext=(5, 5), textcoords='offset points', fontsize=9, color=colors_transformer[i])\nax5.set_xlabel('Epoch')\nax5.set_ylabel('Distance (m)')\nax5.set_title('Transformer Validation Distance')\nax5.legend()\nax5.grid(True, alpha=0.3)\n\n# ===== 6. Transformer Best Distances =====\nax6 = fig.add_subplot(gs[1, 2])\nbest_dists_transformer = [h['best_dist'] for h in transformer_histories]\nseeds_transformer = [h['seed'] for h in transformer_histories]\nbars = ax6.bar(range(len(best_dists_transformer)), best_dists_transformer, color=colors_transformer)\nax6.set_xticks(range(len(best_dists_transformer)))\nax6.set_xticklabels([f'Seed\\n{s}' for s in seeds_transformer])\nax6.set_ylabel('Best Validation Distance (m)')\nax6.set_title('Transformer Best Performance')\nax6.grid(True, alpha=0.3, axis='y')\nfor i, (bar, dist) in enumerate(zip(bars, best_dists_transformer)):\n    height = bar.get_height()\n    ax6.text(bar.get_x() + bar.get_width()/2., height, f'{dist:.4f}m',\n             ha='center', va='bottom', fontweight='bold', fontsize=9)\n\n# ===== 7. Learning Rate Schedule (ê³µí†µ) =====\nax7 = fig.add_subplot(gs[2, 0])\nhistory = lstm_histories[0]  # LRì€ ëª¨ë‘ ë™ì¼\nepochs = range(1, len(history['lr']) + 1)\nax7.plot(epochs, history['lr'], color='purple', linewidth=2)\nax7.axvline(x=WARMUP_EPOCHS, color='red', linestyle='--', label=f'Warmup End (Epoch {WARMUP_EPOCHS})')\nax7.set_xlabel('Epoch')\nax7.set_ylabel('Learning Rate')\nax7.set_title('Learning Rate Schedule (ê³µí†µ)')\nax7.legend()\nax7.grid(True, alpha=0.3)\nax7.set_yscale('log')\n\n# ===== 8. LSTM vs Transformer ë¹„êµ =====\nax8 = fig.add_subplot(gs[2, 1])\navg_lstm = np.mean(best_dists_lstm)\navg_transformer = np.mean(best_dists_transformer)\nmodels = ['LSTM\\n(ì§§ì€ ì—í”¼ì†Œë“œ)', 'Transformer\\n(ê¸´ ì—í”¼ì†Œë“œ)']\navgs = [avg_lstm, avg_transformer]\ncolors_comp = ['#1f77b4', '#d62728']\nbars = ax8.bar(models, avgs, color=colors_comp, alpha=0.7, edgecolor='black', linewidth=2)\nax8.set_ylabel('í‰ê·  Best Distance (m)')\nax8.set_title('LSTM vs Transformer ì„±ëŠ¥ ë¹„êµ')\nax8.grid(True, alpha=0.3, axis='y')\nfor bar, avg in zip(bars, avgs):\n    height = bar.get_height()\n    ax8.text(bar.get_x() + bar.get_width()/2., height, f'{avg:.4f}m',\n             ha='center', va='bottom', fontweight='bold', fontsize=12)\n\n# ===== 9. ì „ëµ ìš”ì•½ í…ìŠ¤íŠ¸ =====\nax9 = fig.add_subplot(gs[2, 2])\nax9.axis('off')\nsummary_text = f\"\"\"[V7] ê¸¸ì´ë³„ ì•™ìƒë¸” ì „ëµ\n\nëª¨ë¸ ì„±ëŠ¥:\n  LSTM í‰ê· : {avg_lstm:.4f}m\n  Transformer í‰ê· : {avg_transformer:.4f}m\n\nì¶”ë¡  ì „ëµ:\n  ì§§ì€ (<= {SHORT_THRESHOLD}): LSTM 100%\n  ì¤‘ê°„ ({SHORT_THRESHOLD}~{LONG_THRESHOLD}): 0.6*LSTM + 0.4*Transformer\n  ê¸´ (> {LONG_THRESHOLD}): Transformer 100%\n\nì´ë¡ ì  ê·¼ê±°:\n  LSTM: ì§§ì€ ì‹œí€€ìŠ¤ì— íš¨ìœ¨ì \n        ìˆœì°¨ ì •ë³´ ë³´ì¡´\n        ì ì€ íŒŒë¼ë¯¸í„°\n  \n  Transformer: ê¸´ ì‹œí€€ìŠ¤ì— íš¨ìœ¨ì \n               Long-range dependency\n               ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥\n\"\"\"\n# [FIX] ëª…ì‹œì ìœ¼ë¡œ í°íŠ¸ ì§€ì • (ì œëª©ì²˜ëŸ¼!)\nax9.text(0.05, 0.5, summary_text, fontsize=10, verticalalignment='center',\n         fontfamily='Malgun Gothic',  # â† ì´ê²Œ í•µì‹¬!\n         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n\nplt.savefig('v7_training_history.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\ní•™ìŠµ ê³¡ì„  ì €ì¥ ì™„ë£Œ: v7_training_history.png\")\nprint(f\"\\nìµœì¢… ê²°ê³¼:\")\nprint(f\"{'='*60}\")\nprint(f\"LSTM ëª¨ë¸:\")\nfor i, h in enumerate(lstm_histories):\n    print(f\"  Seed {h['seed']:4d}: {h['best_dist']:.4f}m\")\nprint(f\"  í‰ê· : {avg_lstm:.4f}m\")\nprint(f\"\\nTransformer ëª¨ë¸:\")\nfor i, h in enumerate(transformer_histories):\n    print(f\"  Seed {h['seed']:4d}: {h['best_dist']:.4f}m\")\nprint(f\"  í‰ê· : {avg_transformer:.4f}m\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "55eec295",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“Š [V7] K-League íŒ¨ìŠ¤ ì˜ˆì¸¡ - ê¸¸ì´ë³„ ì•™ìƒë¸” ì „ëµ\n\n## ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´\n\nì´ë¡ ì  ë¶„ì„ì— ê¸°ë°˜í•˜ì—¬, **ì—í”¼ì†Œë“œ ê¸¸ì´ì— ë”°ë¼ ìµœì ì˜ ëª¨ë¸ì„ ë™ì ìœ¼ë¡œ ì„ íƒ**í•˜ëŠ” ì•™ìƒë¸” ì „ëµì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.\n\n### ğŸ“Œ ë¬¸ì œ ì¸ì‹\n\n- **í‰ê· ì˜ í•¨ì •**: í‰ê·  ì—í”¼ì†Œë“œ ê¸¸ì´ëŠ” 23.11ê°œì´ì§€ë§Œ, ì´ëŠ” ëŒ€í‘œê°’ì´ ì•„ë‹˜\n- **Right-skewed ë¶„í¬**: ëŒ€ë¶€ë¶„ì˜ ì—í”¼ì†Œë“œëŠ” ì¤‘ì•™ê°’(18ê°œ)ë³´ë‹¤ ì§§ìŒ\n- **Transformerì˜ í•œê³„**: ê¸´ ì‹œí€€ìŠ¤ì— ìµœì í™”ë˜ì–´ ì§§ì€ ì‹œí€€ìŠ¤ì—ì„œëŠ” ë¹„íš¨ìœ¨ì \n\n### ğŸ”¬ ì´ë¡ ì  ê·¼ê±°\n\n| ì—í”¼ì†Œë“œ ê¸¸ì´ | íŠ¹ì„± | ì í•©í•œ ëª¨ë¸ | ì´ìœ  |\n|---|---|---|---|\n| **ì§§ìŒ (â‰¤15)** | ë¹ ë¥¸ ì—­ìŠµ, ë‹¨ìˆœ íŒ¨í„´ | **LSTM** | ìˆœì°¨ ì •ë³´ ë³´ì¡´, íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  |\n| **ì¤‘ê°„ (15~40)** | ì¼ë°˜ì  ê³µê²© ì „ê°œ | **ì•™ìƒë¸”** | ë‘ ëª¨ë¸ì˜ ê°•ì  ê²°í•© |\n| **ê¸´ (>40)** | ë³µì¡í•œ ë¹Œë“œì—… | **Transformer** | Long-range dependency í¬ì°© |\n\n## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜\n\n### 1. LSTM ëª¨ë¸ (ì§§ì€ ì—í”¼ì†Œë“œ íŠ¹í™”)\n- **Bi-LSTM**: ì–‘ë°©í–¥ìœ¼ë¡œ ì‹œí€€ìŠ¤ ì •ë³´ ì²˜ë¦¬\n- **Attention**: ì¤‘ìš”í•œ ì•¡ì…˜ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬\n- **ì¥ì **: \n  - ì‘ì€ íŒŒë¼ë¯¸í„°ë¡œ íš¨ìœ¨ì  í•™ìŠµ\n  - ìˆœì°¨ì  íŒ¨í„´ ë³´ì¡´\n  - Overfitting ìœ„í—˜ ë‚®ìŒ\n\n### 2. Transformer ëª¨ë¸ (ê¸´ ì—í”¼ì†Œë“œ íŠ¹í™”)\n- **Sinusoidal PE**: ìœ„ì¹˜ ì •ë³´ ì¸ì½”ë”©\n- **GELU Activation**: ë¶€ë“œëŸ¬ìš´ ë¹„ì„ í˜•ì„±\n- **ì¥ì **:\n  - Long-range dependency í¬ì°©\n  - ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥¸ í•™ìŠµ\n  - ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥\n\n## ğŸ² ì¶”ë¡  ì „ëµ\n\n```python\nIF episode_length <= 15:\n    prediction = LSTM_model(episode)\n    weight = 1.0\n    \nELIF episode_length <= 40:\n    lstm_pred = LSTM_model(episode)\n    transformer_pred = Transformer_model(episode)\n    prediction = 0.6 * lstm_pred + 0.4 * transformer_pred\n    \nELSE:  # > 40\n    prediction = Transformer_model(episode)\n    weight = 1.0\n```\n\n## ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼\n\n1. **ëŒ€ë¶€ë¶„ì˜ ì—í”¼ì†Œë“œ(ì§§ì€ êµ¬ê°„)ì—ì„œ LSTM ìš°ì„¸** â†’ ì „ì²´ ì„±ëŠ¥ í–¥ìƒ\n2. **ê° ëª¨ë¸ì„ ê°•ì  êµ¬ê°„ì— í™œìš©** â†’ ìµœì  ì„±ëŠ¥ ë‹¬ì„±\n3. **ì´ë¡ ì— ê¸°ë°˜í•œ ì„¤ê³„** â†’ ì¼ë°˜í™” ì„±ëŠ¥ ìš°ìˆ˜\n\n## ğŸ”§ í•™ìŠµ ì„¤ì •\n\n- **Seeds**: [42, 2024, 777] (3ê°œ ì•™ìƒë¸”)\n- **Epochs**: 40 (Warmup 3 epochs)\n- **Optimizer**: AdamW (weight_decay=1e-4)\n- **Scheduler**: Cosine Annealing\n- **Loss**: Huber Loss (ì´ìƒì¹˜ì— ê°•í•¨)\n- **Gradient Clipping**: 1.0\n\n## ğŸ“š ì°¸ê³  ë¬¸í—Œ\n\nì´ êµ¬í˜„ì€ ë‹¤ìŒ ì´ë¡ ì  ì—°êµ¬ì— ê¸°ë°˜í•©ë‹ˆë‹¤:\n\n- **LSTM vs Transformer ë¹„êµ ì—°êµ¬** (Machine Learning Mastery, 2024)\n- **ìŠ¤í¬ì¸  ë¶„ì„ì—ì„œì˜ GNN ì‘ìš©** (arXiv, 2022)\n- **ì¶•êµ¬ ì´ë²¤íŠ¸ ì˜ˆì¸¡** (KDD 2022, Seq2Event)\n- **ì‹œê³„ì—´ ì˜ˆì¸¡ì—ì„œì˜ ëª¨ë¸ ì„ íƒ** (Neural Brain Works, 2025)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daycon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}